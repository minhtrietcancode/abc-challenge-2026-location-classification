{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SABZgR0aAqX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO2SJav2aAs5"
      },
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drh4865maAvl"
      },
      "outputs": [],
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"âœ“ All folds loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKGLWXInaAyW"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns # Check if we are in 'training' mode\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def build_bidirectional_gru_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU Architecture\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"âœ… Basic functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EXPERIMENT 4: 7 Directions + Adaptive Confidence Thresholds\n",
        "\n",
        "Building on Experiment 2's best result (0.4384), we add adaptive confidence-based weighting:\n",
        "- When a direction is highly confident (>0.70), boost its weight\n",
        "- When centered is very confident (>0.70), trust it heavily (it had 0.655 avg in Exp 1)\n",
        "- Dynamically adjust direction contributions based on confidence patterns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_extended_multidirectional_windows(vector_df):\n",
        "    \"\"\"\n",
        "    Create 7 types of sliding windows for extended multi-directional prediction\n",
        "    (Same as Experiment 2)\n",
        "    \"\"\"\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "    \n",
        "    results = {\n",
        "        'backward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'centered_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'backward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_past': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_future': {'sequences': [], 'labels': [], 'indices': []},\n",
        "    }\n",
        "    \n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        n = len(vectors)\n",
        "        \n",
        "        for i in range(n):\n",
        "            # 1. BACKWARD_10\n",
        "            if i >= 9:\n",
        "                window = vectors[i - 9 : i + 1]\n",
        "                results['backward_10']['sequences'].append(window)\n",
        "                results['backward_10']['labels'].append(rooms[i])\n",
        "                results['backward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # 2. CENTERED_10\n",
        "            if i >= 4 and i + 5 < n:\n",
        "                window = vectors[i - 4 : i + 6]\n",
        "                results['centered_10']['sequences'].append(window)\n",
        "                results['centered_10']['labels'].append(rooms[i])\n",
        "                results['centered_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # 3. FORWARD_10\n",
        "            if i + 9 < n:\n",
        "                window = vectors[i : i + 10]\n",
        "                results['forward_10']['sequences'].append(window)\n",
        "                results['forward_10']['labels'].append(rooms[i])\n",
        "                results['forward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # 4. BACKWARD_15\n",
        "            if i >= 14:\n",
        "                window = vectors[i - 14 : i + 1]\n",
        "                results['backward_15']['sequences'].append(window)\n",
        "                results['backward_15']['labels'].append(rooms[i])\n",
        "                results['backward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # 5. FORWARD_15\n",
        "            if i + 14 < n:\n",
        "                window = vectors[i : i + 15]\n",
        "                results['forward_15']['sequences'].append(window)\n",
        "                results['forward_15']['labels'].append(rooms[i])\n",
        "                results['forward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # 6. ASYMM_PAST\n",
        "            if i >= 11 and i + 3 < n:\n",
        "                window = vectors[i - 11 : i + 4]\n",
        "                results['asymm_past']['sequences'].append(window)\n",
        "                results['asymm_past']['labels'].append(rooms[i])\n",
        "                results['asymm_past']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # 7. ASYMM_FUTURE\n",
        "            if i >= 3 and i + 11 < n:\n",
        "                window = vectors[i - 3 : i + 12]\n",
        "                results['asymm_future']['sequences'].append(window)\n",
        "                results['asymm_future']['labels'].append(rooms[i])\n",
        "                results['asymm_future']['indices'].append((day_group['date'].iloc[0], i))\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"âœ… Extended multi-directional window function defined (7 directions)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ensemble_models(train_df, n_models=5, base_seed=42, verbose=False):\n",
        "    \"\"\"\n",
        "    Train multiple models with different seeds for ensemble\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"  Training ensemble of {n_models} models...\")\n",
        "    \n",
        "    # Prepare data\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df, max_length=50)\n",
        "    \n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "    \n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50, padding='post', dtype='float32', value=0.0)\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "    \n",
        "    # Train multiple models\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000\n",
        "        set_seeds(model_seed)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"    Model {i+1}/{n_models} (seed {model_seed})...\", end=\" \")\n",
        "        \n",
        "        model = build_bidirectional_gru_model(\n",
        "            input_shape=(50, 23),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "        \n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "        \n",
        "        model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        models.append(model)\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"âœ“\")\n",
        "    \n",
        "    return models, label_encoder\n",
        "\n",
        "print(\"âœ“ Ensemble training function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEW: Adaptive Confidence-Based Direction Weighting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_single_direction(models, sequences, max_seq_length=50):\n",
        "    \"\"\"\n",
        "    Get ensemble predictions for a single direction\n",
        "    \"\"\"\n",
        "    X_padded = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "    \n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        proba = model.predict(X_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "    \n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "    \n",
        "    return ensemble_proba\n",
        "\n",
        "def combine_directional_predictions_adaptive(direction_results, \n",
        "                                             high_conf_threshold=0.70,\n",
        "                                             very_high_conf_threshold=0.75,\n",
        "                                             centered_boost_threshold=0.68):\n",
        "    \"\"\"\n",
        "    NEW: Adaptive confidence-based direction combination\n",
        "    \n",
        "    Key innovation: Dynamically adjust direction weights based on confidence patterns\n",
        "    \n",
        "    Strategy:\n",
        "    1. If centered_10 has high confidence (>0.68), boost its weight (it had 0.655 avg in Exp 1)\n",
        "    2. If any direction has very high confidence (>0.75), boost it significantly\n",
        "    3. If all directions have low confidence (<0.60), use equal weighting (uncertain situation)\n",
        "    4. Otherwise, use standard confidence weighting\n",
        "    \n",
        "    Args:\n",
        "        direction_results: Dict with keys for all 7 directions\n",
        "        high_conf_threshold: Threshold for boosting a direction (default 0.70)\n",
        "        very_high_conf_threshold: Threshold for strong boost (default 0.75)\n",
        "        centered_boost_threshold: Lower threshold for centered (it's historically best)\n",
        "    \n",
        "    Returns:\n",
        "        combined_proba: (n_positions, n_classes) final probability matrix\n",
        "        position_map: mapping from (date, position) to array index\n",
        "    \"\"\"\n",
        "    # Build position mapping\n",
        "    all_positions = set()\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10', \n",
        "                      'backward_15', 'forward_15', \n",
        "                      'asymm_past', 'asymm_future']\n",
        "    \n",
        "    for direction in direction_names:\n",
        "        all_positions.update(direction_results[direction]['indices'])\n",
        "    \n",
        "    all_positions = sorted(all_positions)\n",
        "    position_map = {pos: idx for idx, pos in enumerate(all_positions)}\n",
        "    \n",
        "    n_classes = direction_results['backward_10']['proba'].shape[1]\n",
        "    n_positions = len(all_positions)\n",
        "    \n",
        "    combined_proba = np.zeros((n_positions, n_classes))\n",
        "    position_counts = np.zeros(n_positions)\n",
        "    \n",
        "    # Pre-compute confidences for all directions\n",
        "    direction_confidences = {}\n",
        "    for direction_name in direction_names:\n",
        "        proba = direction_results[direction_name]['proba']\n",
        "        direction_confidences[direction_name] = np.max(proba, axis=1)\n",
        "    \n",
        "    # For each position, apply adaptive weighting\n",
        "    for pos_idx, pos in enumerate(all_positions):\n",
        "        # Collect all directions that have this position\n",
        "        position_directions = {}\n",
        "        position_confs = {}\n",
        "        \n",
        "        for direction_name in direction_names:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                position_directions[direction_name] = direction_results[direction_name]['proba'][idx]\n",
        "                position_confs[direction_name] = direction_confidences[direction_name][idx]\n",
        "        \n",
        "        if not position_directions:\n",
        "            continue\n",
        "        \n",
        "        # ADAPTIVE WEIGHTING LOGIC\n",
        "        weights = {}\n",
        "        \n",
        "        # Check confidence patterns\n",
        "        max_conf = max(position_confs.values())\n",
        "        avg_conf = np.mean(list(position_confs.values()))\n",
        "        centered_conf = position_confs.get('centered_10', 0)\n",
        "        \n",
        "        # Strategy 1: Very high confidence in any direction â†’ Trust it heavily\n",
        "        if max_conf >= very_high_conf_threshold:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                if conf >= very_high_conf_threshold:\n",
        "                    weights[direction_name] = conf * 2.5  # Strong boost\n",
        "                elif conf >= high_conf_threshold:\n",
        "                    weights[direction_name] = conf * 1.2  # Moderate boost\n",
        "                else:\n",
        "                    weights[direction_name] = conf * 0.5  # Reduce low-conf directions\n",
        "        \n",
        "        # Strategy 2: Centered is confident â†’ Trust it more (it had highest avg in Exp 1)\n",
        "        elif centered_conf >= centered_boost_threshold:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                if direction_name == 'centered_10':\n",
        "                    weights[direction_name] = conf * 1.8  # Boost centered\n",
        "                else:\n",
        "                    weights[direction_name] = conf * 0.8  # Slight reduction for others\n",
        "        \n",
        "        # Strategy 3: All low confidence â†’ Equal weighting (uncertain)\n",
        "        elif avg_conf < 0.60:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                weights[direction_name] = 1.0  # Equal weight when all uncertain\n",
        "        \n",
        "        # Strategy 4: Normal case â†’ Standard confidence weighting\n",
        "        else:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                weights[direction_name] = conf  # Standard confidence weight\n",
        "        \n",
        "        # Combine predictions with adaptive weights\n",
        "        total_weight = sum(weights.values())\n",
        "        for direction_name, weight in weights.items():\n",
        "            combined_proba[pos_idx] += position_directions[direction_name] * weight\n",
        "        \n",
        "        # Normalize\n",
        "        if total_weight > 0:\n",
        "            combined_proba[pos_idx] /= total_weight\n",
        "    \n",
        "    return combined_proba, position_map\n",
        "\n",
        "print(\"âœ… Adaptive confidence-based direction combination defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_confidence_weighted_voting(predictions_proba, vote_window=5):\n",
        "    \"\"\"\n",
        "    Confidence-weighted temporal voting (same as before)\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        half_window = vote_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "        \n",
        "        window_proba = predictions_proba[start:end]\n",
        "        window_confidences = np.max(window_proba, axis=1)\n",
        "        \n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "        \n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "    \n",
        "    return voted_predictions\n",
        "\n",
        "print(\"âœ… Temporal voting function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Complete Pipeline with Adaptive Confidence Thresholds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_adaptive_multidirectional_pipeline(train_df, test_df, seed, n_ensemble=5, \n",
        "                                          vote_window=5,\n",
        "                                          high_conf_threshold=0.70,\n",
        "                                          very_high_conf_threshold=0.75,\n",
        "                                          centered_boost_threshold=0.68,\n",
        "                                          verbose=False):\n",
        "    \"\"\"\n",
        "    EXPERIMENT 4: 7 directions + Adaptive confidence-based weighting\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Train ensemble (5 models)\n",
        "    2. Create 7 directional windows\n",
        "    3. Get ensemble predictions for each direction\n",
        "    4. NEW: Adaptive confidence-based direction combination\n",
        "    5. Apply temporal voting\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n  Seed {seed}: Training ensemble...\")\n",
        "    \n",
        "    # 1. Train Ensemble\n",
        "    models, label_encoder = train_ensemble_models(\n",
        "        train_df,\n",
        "        n_models=n_ensemble,\n",
        "        base_seed=seed,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"  Creating multi-directional windows (7 directions)...\")\n",
        "    \n",
        "    # 2. Create Windows\n",
        "    test_vectors = create_beacon_count_vectors(test_df)\n",
        "    direction_windows = create_extended_multidirectional_windows(test_vectors)\n",
        "    \n",
        "    if verbose:\n",
        "        for direction_name in ['backward_10', 'centered_10', 'forward_10', \n",
        "                              'backward_15', 'forward_15', 'asymm_past', 'asymm_future']:\n",
        "            print(f\"    {direction_name}: {len(direction_windows[direction_name]['sequences'])} windows\")\n",
        "        print(\"  Getting directional predictions...\")\n",
        "    \n",
        "    # 3. Get Predictions\n",
        "    direction_results = {}\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10', \n",
        "                      'backward_15', 'forward_15', \n",
        "                      'asymm_past', 'asymm_future']\n",
        "    \n",
        "    for direction_name in direction_names:\n",
        "        if verbose:\n",
        "            print(f\"    Predicting {direction_name}...\", end=\" \")\n",
        "        \n",
        "        sequences = direction_windows[direction_name]['sequences']\n",
        "        proba = predict_single_direction(models, sequences, max_seq_length=50)\n",
        "        \n",
        "        direction_results[direction_name] = {\n",
        "            'proba': proba,\n",
        "            'indices': direction_windows[direction_name]['indices'],\n",
        "            'labels': direction_windows[direction_name]['labels']\n",
        "        }\n",
        "        \n",
        "        if verbose:\n",
        "            avg_conf = np.mean(np.max(proba, axis=1))\n",
        "            print(f\"avg confidence: {avg_conf:.3f}\")\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Combining with ADAPTIVE confidence thresholds...\")\n",
        "        print(f\"    High conf threshold: {high_conf_threshold}\")\n",
        "        print(f\"    Very high conf threshold: {very_high_conf_threshold}\")\n",
        "        print(f\"    Centered boost threshold: {centered_boost_threshold}\")\n",
        "    \n",
        "    # 4. NEW: Adaptive Combination\n",
        "    combined_proba, position_map = combine_directional_predictions_adaptive(\n",
        "        direction_results,\n",
        "        high_conf_threshold=high_conf_threshold,\n",
        "        very_high_conf_threshold=very_high_conf_threshold,\n",
        "        centered_boost_threshold=centered_boost_threshold\n",
        "    )\n",
        "    \n",
        "    # Get ground truth labels\n",
        "    y_test = []\n",
        "    for pos in sorted(position_map.keys()):\n",
        "        for direction_name in direction_names:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                y_test.append(direction_results[direction_name]['labels'][idx])\n",
        "                break\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Applying temporal voting (window={vote_window})...\")\n",
        "    \n",
        "    # 5. Temporal Voting\n",
        "    y_pred_voted_encoded = apply_confidence_weighted_voting(combined_proba, vote_window=vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "    \n",
        "    # 6. Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  âœ“ Macro F1: {macro_f1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)},\n",
        "        'thresholds': {\n",
        "            'high_conf': high_conf_threshold,\n",
        "            'very_high_conf': very_high_conf_threshold,\n",
        "            'centered_boost': centered_boost_threshold\n",
        "        }\n",
        "    }\n",
        "\n",
        "print(\"âœ… Complete adaptive multi-directional pipeline defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Experiment 4: Adaptive Confidence Thresholds\n",
        "\n",
        "## Quick Test on Fold 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QUICK TEST: Fold 1, 3 seeds\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 4: ADAPTIVE CONFIDENCE-BASED DIRECTION WEIGHTING\")\n",
        "print(\"Testing on Fold 1 with 3 seeds\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seeds = [42, 123, 456]\n",
        "train_df, test_df = train_df_1, test_df_1\n",
        "\n",
        "results_adaptive = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\nRunning seed {seed}...\")\n",
        "    result = run_adaptive_multidirectional_pipeline(\n",
        "        train_df, test_df, \n",
        "        seed=seed,\n",
        "        n_ensemble=5,\n",
        "        vote_window=5,\n",
        "        high_conf_threshold=0.70,\n",
        "        very_high_conf_threshold=0.75,\n",
        "        centered_boost_threshold=0.68,\n",
        "        verbose=True\n",
        "    )\n",
        "    results_adaptive.append(result)\n",
        "\n",
        "macro_f1_scores = [r['macro_f1'] for r in results_adaptive]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT 4 RESULTS (Fold 1, 3 seeds)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nAdaptive Confidence Weighting:\")\n",
        "print(f\"  Mean Macro F1: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "print(f\"  Individual runs:\")\n",
        "for result in results_adaptive:\n",
        "    print(f\"    Seed {result['seed']}: {result['macro_f1']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON:\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Experiment 2 (7 dir, standard weighting): 0.4896 Â± 0.0151\")\n",
        "print(f\"Experiment 4 (7 dir, adaptive weighting):  {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "print(f\"\\nChange: {np.mean(macro_f1_scores) - 0.4896:+.4f}\")\n",
        "\n",
        "if np.mean(macro_f1_scores) > 0.4896:\n",
        "    print(\"\\nâœ… Adaptive weighting helps! Continue with full 4-fold CV.\")\n",
        "else:\n",
        "    print(\"\\nâš ï¸  Adaptive weighting doesn't improve. Try different thresholds?\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Full 4-Fold Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FULL EXPERIMENT\n",
        "print(\"=\"*80)\n",
        "print(\"FULL 4-FOLD CROSS-VALIDATION - EXPERIMENT 4\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seeds = [42, 123, 456]\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    fold_results = []\n",
        "    \n",
        "    for seed in seeds:\n",
        "        print(f\"  Running seed {seed}...\", end=\" \")\n",
        "        result = run_adaptive_multidirectional_pipeline(\n",
        "            train_df, test_df, \n",
        "            seed=seed,\n",
        "            n_ensemble=5,\n",
        "            vote_window=5,\n",
        "            high_conf_threshold=0.70,\n",
        "            very_high_conf_threshold=0.75,\n",
        "            centered_boost_threshold=0.68,\n",
        "            verbose=False\n",
        "        )\n",
        "        fold_results.append(result)\n",
        "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "    \n",
        "    all_fold_results[fold_num] = fold_results\n",
        "    \n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
        "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY - EXPERIMENT 4 (ADAPTIVE CONFIDENCE)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "all_macro_f1 = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} Â± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLETE PROGRESSION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Baseline: 0.4106 Â± 0.0266\")\n",
        "print(\"Exp 1 (3 dir): 0.4273 Â± 0.0312\")\n",
        "print(\"Exp 2 (7 dir, standard): 0.4384 Â± 0.0329\")\n",
        "print(f\"Exp 4 (7 dir, adaptive): {np.mean(all_macro_f1):.4f} Â± {np.std(all_macro_f1):.4f}\")\n",
        "\n",
        "total_gain = np.mean(all_macro_f1) - 0.4106\n",
        "gain_from_exp2 = np.mean(all_macro_f1) - 0.4384\n",
        "target_gap = 0.45 - np.mean(all_macro_f1)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Total gain from baseline: {total_gain:+.4f}\")\n",
        "print(f\"Gain from Exp 2: {gain_from_exp2:+.4f}\")\n",
        "print(f\"Gap to target (0.45): {target_gap:.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if np.mean(all_macro_f1) >= 0.45:\n",
        "    print(\"\\nðŸŽ¯ðŸŽ¯ðŸŽ¯ TARGET ACHIEVED! 0.45 F1 REACHED! ðŸŽ¯ðŸŽ¯ðŸŽ¯\")\n",
        "elif gain_from_exp2 > 0.005:\n",
        "    print(f\"\\nâœ… Adaptive confidence helps! +{gain_from_exp2:.4f} improvement\")\n",
        "    if target_gap < 0.005:\n",
        "        print(\"   SO CLOSE! Try hyperparameter tuning for final push\")\n",
        "    else:\n",
        "        print(f\"   {target_gap:.4f} remaining to target\")\n",
        "else:\n",
        "    print(\"\\nðŸ“Š Adaptive confidence similar to standard weighting\")\n",
        "    print(\"   Try: Different thresholds or hyperparameter tuning\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "with open('experiment4_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"EXPERIMENT 4: ADAPTIVE CONFIDENCE-BASED DIRECTION WEIGHTING\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"Configuration:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Directions: 7 (same as Exp 2)\\n\")\n",
        "    f.write(\"Ensemble: 5 models\\n\")\n",
        "    f.write(\"Temporal voting: 5 seconds\\n\")\n",
        "    f.write(\"\\nNEW Adaptive Thresholds:\\n\")\n",
        "    f.write(\"  High confidence threshold: 0.70\\n\")\n",
        "    f.write(\"  Very high confidence threshold: 0.75\\n\")\n",
        "    f.write(\"  Centered boost threshold: 0.68\\n\\n\")\n",
        "    \n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "    \n",
        "    f.write(\"OVERALL RESULTS:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} Â± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
        "    \n",
        "    f.write(\"PROGRESSION:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Baseline: 0.4106\\n\")\n",
        "    f.write(\"Exp 2 (7 dir, standard): 0.4384\\n\")\n",
        "    f.write(f\"Exp 4 (7 dir, adaptive): {np.mean(all_macro_f1):.4f}\\n\\n\")\n",
        "    f.write(f\"Gain from baseline: {np.mean(all_macro_f1) - 0.4106:+.4f}\\n\")\n",
        "    f.write(f\"Gain from Exp 2: {np.mean(all_macro_f1) - 0.4384:+.4f}\\n\")\n",
        "    f.write(f\"Gap to target: {0.45 - np.mean(all_macro_f1):.4f}\\n\")\n",
        "    \n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\n{'='*80}\\n\")\n",
        "        f.write(f\"FOLD {fold_num}\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "        \n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "        \n",
        "        for result in fold_results:\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
        "        \n",
        "        f.write(f\"\\n  Mean: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "\n",
        "print(\"âœ… Results saved to experiment4_results.txt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
