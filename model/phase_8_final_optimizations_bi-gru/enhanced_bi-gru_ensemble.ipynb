{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Bi-GRU with Confidence-Weighted Ensemble\n",
        "\n",
        "## Key Improvements:\n",
        "1. **Multi-Model Ensemble**: Train 5 models with different seeds per fold\n",
        "2. **Confidence-Weighted Voting**: Use softmax probabilities instead of majority voting\n",
        "3. **Adaptive Temporal Window**: Adjust voting window based on confidence\n",
        "4. **Timestamp Gap Features**: Utilize temporal gaps as additional input\n",
        "5. **Two-Stage Filtering**: High-confidence predictions stabilize uncertain ones\n",
        "\n",
        "**Target**: Improve from 0.3854 â†’ 0.43-0.45 F1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Input, Concatenate\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy.stats import mode\n",
        "from scipy.special import softmax\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"âœ“ All folds loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def compute_time_gaps(df):\n",
        "    \"\"\"NEW: Compute time gaps between consecutive readings\"\"\"\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['timestamp_dt'] = pd.to_datetime(df['timestamp'])\n",
        "    df['time_gap'] = df['timestamp_dt'].diff().dt.total_seconds().fillna(0)\n",
        "    \n",
        "    # Categorize gaps\n",
        "    df['gap_category'] = pd.cut(\n",
        "        df['time_gap'],\n",
        "        bins=[-np.inf, 1, 3, 5, 10, np.inf],\n",
        "        labels=[0, 1, 2, 3, 4]  # 0: <1s, 1: 1-3s, 2: 3-5s, 3: 5-10s, 4: >10s\n",
        "    ).astype(int)\n",
        "    \n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors_with_gaps(df):\n",
        "    \"\"\"ENHANCED: Aggregates readings with time gap features\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns\n",
        "    \n",
        "    # Compute gaps first\n",
        "    df = compute_time_gaps(df)\n",
        "\n",
        "    for timestamp, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        # Get gap features (same for all readings in this timestamp)\n",
        "        time_gap = group['time_gap'].iloc[0]\n",
        "        gap_category = group['gap_category'].iloc[0]\n",
        "        \n",
        "        entry = {\n",
        "            'timestamp': timestamp,\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector,\n",
        "            'time_gap': time_gap,\n",
        "            'gap_category': gap_category\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"ENHANCED: Creates sequences with gap features\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        # Combine beacon vectors with gap features\n",
        "        sequence = []\n",
        "        for _, row in group.iterrows():\n",
        "            # Beacon counts (23-dim) + time_gap (1-dim) + gap_category (1-dim) = 25-dim\n",
        "            combined_vector = row['beacon_vector'] + [row['time_gap'], row['gap_category']]\n",
        "            sequence.append(combined_vector)\n",
        "        \n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def create_sliding_windows_by_day(vector_df, window_size=10):\n",
        "    \"\"\"ENHANCED: Creates sliding windows with gap features\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    timestamps = []  # Track timestamps for later analysis\n",
        "\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        if len(day_group) >= window_size:\n",
        "            for i in range(len(day_group) - window_size + 1):\n",
        "                window_group = day_group.iloc[i : i + window_size]\n",
        "                \n",
        "                # Combine beacon vectors with gap features\n",
        "                window = []\n",
        "                for _, row in window_group.iterrows():\n",
        "                    combined_vector = row['beacon_vector'] + [row['time_gap'], row['gap_category']]\n",
        "                    window.append(combined_vector)\n",
        "                \n",
        "                sequences.append(window)\n",
        "                labels.append(day_group.iloc[i + window_size - 1]['room'])\n",
        "                timestamps.append(day_group.iloc[i + window_size - 1]['timestamp'])\n",
        "\n",
        "    return sequences, labels, timestamps\n",
        "\n",
        "def build_bidirectional_gru_model_with_gaps(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    ENHANCED: Bidirectional GRU that accepts beacon counts + gap features\n",
        "    Input shape: (sequence_length, 25)  [23 beacons + 1 time_gap + 1 gap_category]\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "def apply_confidence_weighted_temporal_voting(predictions_proba, vote_window=5, confidence_threshold=0.7):\n",
        "    \"\"\"\n",
        "    NEW: Confidence-weighted temporal voting with adaptive window\n",
        "    \n",
        "    Args:\n",
        "        predictions_proba: (n_samples, n_classes) probability matrix\n",
        "        vote_window: base window size for voting\n",
        "        confidence_threshold: threshold to determine if prediction is \"confident\"\n",
        "    \n",
        "    Returns:\n",
        "        voted_predictions: (n_samples,) final class predictions\n",
        "        confidences: (n_samples,) confidence scores\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "    confidences = np.zeros(n_samples)\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Get max probability at position i\n",
        "        max_prob_i = np.max(predictions_proba[i])\n",
        "        \n",
        "        # Adaptive window: larger window for uncertain predictions\n",
        "        if max_prob_i >= confidence_threshold:\n",
        "            current_window = max(3, vote_window // 2)  # Shorter window for confident\n",
        "        else:\n",
        "            current_window = vote_window  # Full window for uncertain\n",
        "        \n",
        "        # Get window boundaries\n",
        "        half_window = current_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "        \n",
        "        # Weighted voting within window\n",
        "        window_proba = predictions_proba[start:end]  # (window_size, n_classes)\n",
        "        window_confidences = np.max(window_proba, axis=1)  # (window_size,)\n",
        "        \n",
        "        # Weight each prediction by its confidence\n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "        \n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "        confidences[i] = np.max(weighted_votes) / np.sum(window_confidences)\n",
        "    \n",
        "    return voted_predictions, confidences\n",
        "\n",
        "def apply_two_stage_confidence_filtering(predictions, confidences, threshold=0.3):\n",
        "    \"\"\"\n",
        "    NEW: Two-stage filtering - use high-confidence neighbors to stabilize uncertain predictions\n",
        "    \n",
        "    Args:\n",
        "        predictions: (n_samples,) class predictions\n",
        "        confidences: (n_samples,) confidence scores\n",
        "        threshold: predictions below this confidence will be reconsidered\n",
        "    \n",
        "    Returns:\n",
        "        filtered_predictions: (n_samples,) refined predictions\n",
        "    \"\"\"\n",
        "    filtered_predictions = predictions.copy()\n",
        "    n_samples = len(predictions)\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        if confidences[i] < threshold:\n",
        "            # Find nearest high-confidence predictions\n",
        "            window_size = 7\n",
        "            start = max(0, i - window_size)\n",
        "            end = min(n_samples, i + window_size + 1)\n",
        "            \n",
        "            window_preds = predictions[start:end]\n",
        "            window_confs = confidences[start:end]\n",
        "            \n",
        "            # Use only high-confidence predictions\n",
        "            high_conf_mask = window_confs > (1 - threshold)\n",
        "            if np.any(high_conf_mask):\n",
        "                high_conf_preds = window_preds[high_conf_mask]\n",
        "                # Use mode of high-confidence predictions\n",
        "                filtered_predictions[i] = mode(high_conf_preds, keepdims=False).mode\n",
        "    \n",
        "    return filtered_predictions\n",
        "\n",
        "print(\"âœ“ Enhanced functions with confidence-weighted voting defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ensemble_models(train_df, n_models=5, base_seed=42, verbose=True):\n",
        "    \"\"\"\n",
        "    NEW: Train multiple models with different seeds for ensemble\n",
        "    \n",
        "    Returns:\n",
        "        models: List of trained Keras models\n",
        "        label_encoder: Fitted label encoder\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"  Training ensemble of {n_models} models...\")\n",
        "    \n",
        "    # Prepare data (same for all models)\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors_with_gaps(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df)\n",
        "    \n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "    \n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, padding='pre', dtype='float32', value=0.0)\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "    \n",
        "    # Train multiple models\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000\n",
        "        set_seeds(model_seed)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"    Model {i+1}/{n_models} (seed {model_seed})...\", end=\" \")\n",
        "        \n",
        "        model = build_bidirectional_gru_model_with_gaps(\n",
        "            input_shape=(X_train_padded.shape[1], X_train_padded.shape[2]),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "        \n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "        \n",
        "        # Train\n",
        "        model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        models.append(model)\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"âœ“\")\n",
        "    \n",
        "    return models, label_encoder\n",
        "\n",
        "def ensemble_predict_with_confidence(models, X_test_padded):\n",
        "    \"\"\"\n",
        "    NEW: Ensemble prediction with confidence scores\n",
        "    \n",
        "    Returns:\n",
        "        ensemble_proba: (n_samples, n_classes) averaged probability matrix\n",
        "    \"\"\"\n",
        "    all_predictions = []\n",
        "    \n",
        "    for model in models:\n",
        "        proba = model.predict(X_test_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "    \n",
        "    # Average probabilities across models\n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "    \n",
        "    return ensemble_proba\n",
        "\n",
        "print(\"âœ“ Ensemble training functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_enhanced_pipeline_single_seed(\n",
        "    train_df, \n",
        "    test_df, \n",
        "    seed=42, \n",
        "    n_ensemble_models=5,\n",
        "    window_size=10,\n",
        "    vote_window=5,\n",
        "    confidence_threshold=0.7,\n",
        "    use_two_stage_filtering=True,\n",
        "    verbose=True\n",
        "):\n",
        "    \"\"\"\n",
        "    ENHANCED PIPELINE with all optimizations\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"Running Enhanced Pipeline - Seed {seed}\")\n",
        "        print(f\"{'='*60}\")\n",
        "        print(f\"Ensemble: {n_ensemble_models} models | Window: {window_size}s | Vote: {vote_window}s\")\n",
        "    \n",
        "    # 1. Train Ensemble Models\n",
        "    models, label_encoder = train_ensemble_models(\n",
        "        train_df, \n",
        "        n_models=n_ensemble_models, \n",
        "        base_seed=seed,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    \n",
        "    # 2. Prepare Test Data with Gap Features\n",
        "    if verbose:\n",
        "        print(\"  Preparing test data with gap features...\")\n",
        "    \n",
        "    test_vector_df = create_beacon_count_vectors_with_gaps(test_df)\n",
        "    X_test_seq, y_test, timestamps = create_sliding_windows_by_day(test_vector_df, window_size=window_size)\n",
        "    X_test_padded = pad_sequences(X_test_seq, padding='pre', dtype='float32', value=0.0)\n",
        "    \n",
        "    # 3. Ensemble Prediction\n",
        "    if verbose:\n",
        "        print(\"  Running ensemble predictions...\")\n",
        "    \n",
        "    ensemble_proba = ensemble_predict_with_confidence(models, X_test_padded)\n",
        "    \n",
        "    # 4. Confidence-Weighted Temporal Voting\n",
        "    if verbose:\n",
        "        print(\"  Applying confidence-weighted temporal voting...\")\n",
        "    \n",
        "    y_pred_voted_encoded, confidences = apply_confidence_weighted_temporal_voting(\n",
        "        ensemble_proba, \n",
        "        vote_window=vote_window,\n",
        "        confidence_threshold=confidence_threshold\n",
        "    )\n",
        "    \n",
        "    # 5. Two-Stage Confidence Filtering (Optional)\n",
        "    if use_two_stage_filtering:\n",
        "        if verbose:\n",
        "            print(\"  Applying two-stage confidence filtering...\")\n",
        "        y_pred_voted_encoded = apply_two_stage_confidence_filtering(\n",
        "            y_pred_voted_encoded, \n",
        "            confidences,\n",
        "            threshold=0.3\n",
        "        )\n",
        "    \n",
        "    # 6. Decode Predictions\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "    \n",
        "    # 7. Final Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "    \n",
        "    # 8. Confidence Statistics\n",
        "    avg_confidence = np.mean(confidences)\n",
        "    low_conf_ratio = np.mean(confidences < 0.3)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n  Results:\")\n",
        "        print(f\"    Macro F1: {macro_f1:.4f}\")\n",
        "        print(f\"    Avg Confidence: {avg_confidence:.4f}\")\n",
        "        print(f\"    Low Confidence Ratio: {low_conf_ratio:.2%}\")\n",
        "    \n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)},\n",
        "        'avg_confidence': avg_confidence,\n",
        "        'low_conf_ratio': low_conf_ratio\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Enhanced pipeline function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Enhanced Pipeline\n",
        "\n",
        "### Configuration Options:\n",
        "- `n_ensemble_models`: Number of models per seed (default: 5)\n",
        "- `window_size`: Sliding window size in seconds (default: 10)\n",
        "- `vote_window`: Temporal voting window size (default: 5)\n",
        "- `confidence_threshold`: Threshold for adaptive window (default: 0.7)\n",
        "- `use_two_stage_filtering`: Enable second stage filtering (default: True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run enhanced pipeline with 10 seeds for each of 4 folds\n",
        "seeds = [42, 123, 456, 789, 2024, 3141, 5926, 8888, 1337, 9999]\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'n_ensemble_models': 5,  # Train 5 models per seed\n",
        "    'window_size': 10,\n",
        "    'vote_window': 5,\n",
        "    'confidence_threshold': 0.7,\n",
        "    'use_two_stage_filtering': True\n",
        "}\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ENHANCED BI-GRU WITH CONFIDENCE-WEIGHTED ENSEMBLE\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Configuration: {CONFIG}\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"\\n[Fold {fold_num}, Seed {seed}]\")\n",
        "        result = run_enhanced_pipeline_single_seed(\n",
        "            train_df, test_df, \n",
        "            seed=seed,\n",
        "            **CONFIG,\n",
        "            verbose=True\n",
        "        )\n",
        "        fold_results.append(result)\n",
        "\n",
        "    all_fold_results[fold_num] = fold_results\n",
        "\n",
        "    # Calculate fold statistics\n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    avg_confidences = [r['avg_confidence'] for r in fold_results]\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"FOLD {fold_num} SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"  Mean Macro F1: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "    print(f\"  Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "    print(f\"  Mean Confidence: {np.mean(avg_confidences):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save enhanced results to text file\n",
        "with open('enhanced_4fold_10seed_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"ENHANCED BI-GRU WITH CONFIDENCE-WEIGHTED ENSEMBLE\\n\")\n",
        "    f.write(\"4-FOLD CROSS-VALIDATION WITH 10 SEEDS PER FOLD\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"Configuration:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    for key, value in CONFIG.items():\n",
        "        f.write(f\"  {key}: {value}\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    # Overall summary\n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "\n",
        "    f.write(\"OVERALL RESULTS (40 runs total):\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} Â± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\")\n",
        "    \n",
        "    # Calculate improvement over baseline (0.3854)\n",
        "    baseline = 0.3854\n",
        "    improvement = np.mean(all_macro_f1) - baseline\n",
        "    improvement_pct = (improvement / baseline) * 100\n",
        "    f.write(f\"\\nImprovement over baseline (0.3854):\\n\")\n",
        "    f.write(f\"  Absolute: +{improvement:.4f}\\n\")\n",
        "    f.write(f\"  Relative: +{improvement_pct:.2f}%\\n\")\n",
        "    f.write(\"\\n\")\n",
        "\n",
        "    # Per-fold results\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\n{'='*80}\\n\")\n",
        "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "        avg_confidences = [r['avg_confidence'] for r in fold_results]\n",
        "\n",
        "        f.write(f\"Macro F1 Scores (10 seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        for i, result in enumerate(fold_results):\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f} (conf: {result['avg_confidence']:.3f})\\n\")\n",
        "\n",
        "        f.write(f\"\\nStatistics:\\n\")\n",
        "        f.write(f\"  Mean Macro F1: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Mean Confidence: {np.mean(avg_confidences):.4f}\\n\")\n",
        "\n",
        "        # Per-class F1 (averaged across 10 seeds)\n",
        "        f.write(f\"\\nPer-Class F1 Scores (averaged across 10 seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Collect all class names\n",
        "        all_classes = set()\n",
        "        for result in fold_results:\n",
        "            all_classes.update(result['per_class_f1'].keys())\n",
        "\n",
        "        # Average per-class F1 across seeds\n",
        "        for class_name in sorted(all_classes):\n",
        "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
        "            mean_f1 = np.mean(class_f1_scores)\n",
        "            std_f1 = np.std(class_f1_scores)\n",
        "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} Â± {std_f1:.4f}\\n\")\n",
        "\n",
        "print(\"âœ… Enhanced results saved to enhanced_4fold_10seed_results.txt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display comparison summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON: BASELINE vs ENHANCED\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "baseline_scores = {\n",
        "    1: 0.4120,\n",
        "    2: 0.3677,\n",
        "    3: 0.3910,\n",
        "    4: 0.3709\n",
        "}\n",
        "\n",
        "print(f\"{'Fold':<6} {'Baseline':<12} {'Enhanced':<12} {'Improvement':<12} {'%Change'}\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    enhanced_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    enhanced_mean = np.mean(enhanced_scores)\n",
        "    baseline = baseline_scores[fold_num]\n",
        "    improvement = enhanced_mean - baseline\n",
        "    pct_change = (improvement / baseline) * 100\n",
        "    \n",
        "    print(f\"Fold {fold_num}  {baseline:.4f}      {enhanced_mean:.4f}      \"\n",
        "          f\"{improvement:+.4f}      {pct_change:+.2f}%\")\n",
        "\n",
        "# Overall comparison\n",
        "baseline_overall = 0.3854\n",
        "all_enhanced_scores = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_enhanced_scores.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "enhanced_overall = np.mean(all_enhanced_scores)\n",
        "overall_improvement = enhanced_overall - baseline_overall\n",
        "overall_pct_change = (overall_improvement / baseline_overall) * 100\n",
        "\n",
        "print(\"-\"*60)\n",
        "print(f\"Overall {baseline_overall:.4f}      {enhanced_overall:.4f}      \"\n",
        "      f\"{overall_improvement:+.4f}      {overall_pct_change:+.2f}%\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Target assessment\n",
        "target = 0.45\n",
        "distance_to_target = target - enhanced_overall\n",
        "print(f\"\\nTarget: {target:.4f}\")\n",
        "print(f\"Distance to target: {distance_to_target:.4f}\")\n",
        "if enhanced_overall >= target:\n",
        "    print(\"ðŸŽ‰ TARGET ACHIEVED!\")\n",
        "elif distance_to_target < 0.01:\n",
        "    print(\"ðŸ’ª VERY CLOSE! Almost there!\")\n",
        "else:\n",
        "    print(f\"ðŸ“Š Progress: {(1 - distance_to_target/0.0646) * 100:.1f}% of improvement goal\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Analysis\n",
        "\n",
        "### Ablation Study (Optional)\n",
        "\n",
        "To understand which components contribute most to the improvement, you can run ablation studies by disabling features one at a time:\n",
        "\n",
        "1. **Without Gap Features**: Set gap features to 0\n",
        "2. **Without Ensemble**: Use `n_ensemble_models=1`\n",
        "3. **Without Adaptive Window**: Set `confidence_threshold=0` (always use full window)\n",
        "4. **Without Two-Stage Filtering**: Set `use_two_stage_filtering=False`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quick ablation test on Fold 1 (optional - uncomment to run)\n",
        "# print(\"\\n\" + \"=\"*80)\n",
        "# print(\"ABLATION STUDY ON FOLD 1\")\n",
        "# print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# ablation_configs = {\n",
        "#     'Full (All features)': {\n",
        "#         'n_ensemble_models': 5,\n",
        "#         'use_two_stage_filtering': True,\n",
        "#         'confidence_threshold': 0.7\n",
        "#     },\n",
        "#     'No Ensemble': {\n",
        "#         'n_ensemble_models': 1,\n",
        "#         'use_two_stage_filtering': True,\n",
        "#         'confidence_threshold': 0.7\n",
        "#     },\n",
        "#     'No Two-Stage Filtering': {\n",
        "#         'n_ensemble_models': 5,\n",
        "#         'use_two_stage_filtering': False,\n",
        "#         'confidence_threshold': 0.7\n",
        "#     },\n",
        "#     'No Adaptive Window': {\n",
        "#         'n_ensemble_models': 5,\n",
        "#         'use_two_stage_filtering': True,\n",
        "#         'confidence_threshold': 0.0  # Always use full window\n",
        "#     }\n",
        "# }\n",
        "\n",
        "# for config_name, config in ablation_configs.items():\n",
        "#     print(f\"\\nTesting: {config_name}\")\n",
        "#     result = run_enhanced_pipeline_single_seed(\n",
        "#         train_df_1, test_df_1,\n",
        "#         seed=42,\n",
        "#         window_size=10,\n",
        "#         vote_window=5,\n",
        "#         **config,\n",
        "#         verbose=False\n",
        "#     )\n",
        "#     print(f\"  Macro F1: {result['macro_f1']:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
