{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SABZgR0aAqX"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BO2SJav2aAs5"
      },
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All imports successful\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "drh4865maAvl"
      },
      "outputs": [],
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"✓ All folds loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iKGLWXInaAyW"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns # Check if we are in 'training' mode\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def build_bidirectional_gru_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU Architecture\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"✅ Basic functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEW: Multi-Directional Window Creation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_multidirectional_windows(vector_df, window_size=10):\n",
        "    \"\"\"\n",
        "    NEW: Create 3 types of sliding windows for multi-directional prediction\n",
        "    \n",
        "    For each position i, create:\n",
        "    - Backward window: [i-window_size+1 to i] - predict at position i (END of window)\n",
        "    - Centered window: [i-window_size//2 to i+window_size//2] - predict at position i (MIDDLE of window)  \n",
        "    - Forward window: [i to i+window_size-1] - predict at position i (START of window)\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with keys 'backward', 'centered', 'forward'\n",
        "        Each contains: (sequences, labels, valid_indices)\n",
        "    \"\"\"\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "    \n",
        "    results = {\n",
        "        'backward': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'centered': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward': {'sequences': [], 'labels': [], 'indices': []}\n",
        "    }\n",
        "    \n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        n = len(vectors)\n",
        "        \n",
        "        # Calculate buffer sizes\n",
        "        half_window = window_size // 2\n",
        "        \n",
        "        for i in range(n):\n",
        "            # BACKWARD: [i-window_size+1, ..., i] predict at i\n",
        "            if i >= window_size - 1:\n",
        "                window = vectors[i - window_size + 1 : i + 1]\n",
        "                results['backward']['sequences'].append(window)\n",
        "                results['backward']['labels'].append(rooms[i])\n",
        "                results['backward']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # CENTERED: [i-half_window, ..., i, ..., i+half_window] predict at i\n",
        "            start_centered = i - half_window\n",
        "            end_centered = i + half_window + 1\n",
        "            if start_centered >= 0 and end_centered <= n:\n",
        "                window = vectors[start_centered : end_centered]\n",
        "                results['centered']['sequences'].append(window)\n",
        "                results['centered']['labels'].append(rooms[i])\n",
        "                results['centered']['indices'].append((day_group['date'].iloc[0], i))\n",
        "            \n",
        "            # FORWARD: [i, ..., i+window_size-1] predict at i\n",
        "            if i + window_size <= n:\n",
        "                window = vectors[i : i + window_size]\n",
        "                results['forward']['sequences'].append(window)\n",
        "                results['forward']['labels'].append(rooms[i])\n",
        "                results['forward']['indices'].append((day_group['date'].iloc[0], i))\n",
        "    \n",
        "    return results\n",
        "\n",
        "print(\"✅ Multi-directional window function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_ensemble_models(train_df, n_models=5, base_seed=42, verbose=False):\n",
        "    \"\"\"\n",
        "    Train multiple models with different seeds for ensemble\n",
        "    \n",
        "    Returns:\n",
        "        models: List of trained Keras models\n",
        "        label_encoder: Fitted label encoder\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"  Training ensemble of {n_models} models...\")\n",
        "    \n",
        "    # Prepare data (same for all models)\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df, max_length=50)\n",
        "    \n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "    \n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50, padding='post', dtype='float32', value=0.0)\n",
        "    \n",
        "    # Compute class weights\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "    \n",
        "    # Train multiple models\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000  # 42, 1042, 2042, 3042, 4042\n",
        "        set_seeds(model_seed)\n",
        "        \n",
        "        if verbose:\n",
        "            print(f\"    Model {i+1}/{n_models} (seed {model_seed})...\", end=\" \")\n",
        "        \n",
        "        model = build_bidirectional_gru_model(\n",
        "            input_shape=(50, 23),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "        \n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "        \n",
        "        # Train\n",
        "        model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        models.append(model)\n",
        "        \n",
        "        if verbose:\n",
        "            print(\"✓\")\n",
        "    \n",
        "    return models, label_encoder\n",
        "\n",
        "print(\"✓ Ensemble training function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEW: Multi-Directional Prediction & Aggregation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_single_direction(models, sequences, max_seq_length=50):\n",
        "    \"\"\"\n",
        "    Get ensemble predictions for a single direction\n",
        "    \n",
        "    Returns:\n",
        "        ensemble_proba: (n_samples, n_classes) averaged probability matrix\n",
        "    \"\"\"\n",
        "    # Pad sequences\n",
        "    X_padded = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "    \n",
        "    # Get predictions from all models\n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        proba = model.predict(X_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "    \n",
        "    # Average probabilities across ensemble\n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "    \n",
        "    return ensemble_proba\n",
        "\n",
        "def combine_directional_predictions(direction_results, method='confidence_weighted'):\n",
        "    \"\"\"\n",
        "    NEW: Combine predictions from multiple directions using confidence weighting\n",
        "    \n",
        "    Args:\n",
        "        direction_results: Dict with keys 'backward', 'centered', 'forward'\n",
        "                          Each value is a dict with 'proba' and 'indices'\n",
        "        method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "    \n",
        "    Returns:\n",
        "        combined_proba: (n_positions, n_classes) final probability matrix\n",
        "        position_map: mapping from (date, position) to array index\n",
        "    \"\"\"\n",
        "    # Build a mapping of all unique positions\n",
        "    all_positions = set()\n",
        "    for direction in ['backward', 'centered', 'forward']:\n",
        "        all_positions.update(direction_results[direction]['indices'])\n",
        "    \n",
        "    # Sort positions for consistent ordering\n",
        "    all_positions = sorted(all_positions)\n",
        "    position_map = {pos: idx for idx, pos in enumerate(all_positions)}\n",
        "    \n",
        "    # Get number of classes from first available direction\n",
        "    n_classes = direction_results['backward']['proba'].shape[1]\n",
        "    n_positions = len(all_positions)\n",
        "    \n",
        "    # Initialize combined predictions\n",
        "    combined_proba = np.zeros((n_positions, n_classes))\n",
        "    position_counts = np.zeros(n_positions)  # Track how many directions contributed\n",
        "    \n",
        "    # For each direction, add its weighted contribution\n",
        "    for direction_name in ['backward', 'centered', 'forward']:\n",
        "        direction_data = direction_results[direction_name]\n",
        "        proba = direction_data['proba']\n",
        "        indices = direction_data['indices']\n",
        "        \n",
        "        # Get confidence (max probability) for each prediction\n",
        "        confidences = np.max(proba, axis=1)\n",
        "        \n",
        "        # Add weighted contribution to combined predictions\n",
        "        for i, pos in enumerate(indices):\n",
        "            pos_idx = position_map[pos]\n",
        "            \n",
        "            if method == 'confidence_weighted':\n",
        "                # Weight by confidence\n",
        "                weight = confidences[i]\n",
        "                combined_proba[pos_idx] += proba[i] * weight\n",
        "            elif method == 'equal':\n",
        "                # Equal weight\n",
        "                combined_proba[pos_idx] += proba[i]\n",
        "            elif method == 'softmax':\n",
        "                # Will apply softmax later\n",
        "                combined_proba[pos_idx] += proba[i] * confidences[i]\n",
        "            \n",
        "            position_counts[pos_idx] += 1 if method == 'equal' else confidences[i]\n",
        "    \n",
        "    # Normalize by total weight\n",
        "    for i in range(n_positions):\n",
        "        if position_counts[i] > 0:\n",
        "            combined_proba[i] /= position_counts[i]\n",
        "    \n",
        "    return combined_proba, position_map\n",
        "\n",
        "print(\"✅ Multi-directional prediction functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_confidence_weighted_voting(predictions_proba, vote_window=5):\n",
        "    \"\"\"\n",
        "    Confidence-weighted temporal voting\n",
        "    \n",
        "    Instead of simple majority voting, weight each prediction by its confidence (max probability).\n",
        "    \n",
        "    Args:\n",
        "        predictions_proba: (n_samples, n_classes) probability matrix from ensemble\n",
        "        vote_window: window size for voting\n",
        "    \n",
        "    Returns:\n",
        "        voted_predictions: (n_samples,) final class predictions\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "    \n",
        "    for i in range(n_samples):\n",
        "        # Get window boundaries\n",
        "        half_window = vote_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "        \n",
        "        # Get probabilities within window\n",
        "        window_proba = predictions_proba[start:end]  # (window_size, n_classes)\n",
        "        \n",
        "        # Get confidence (max probability) for each prediction in window\n",
        "        window_confidences = np.max(window_proba, axis=1)  # (window_size,)\n",
        "        \n",
        "        # Weight each prediction by its confidence\n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            # Each timestep contributes its probability * its confidence\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "        \n",
        "        # Final prediction: class with highest weighted vote\n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "    \n",
        "    return voted_predictions\n",
        "\n",
        "print(\"✅ Temporal voting function defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NEW: Complete Pipeline with Multi-Directional Windows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_multidirectional_pipeline(train_df, test_df, seed, n_ensemble=5, \n",
        "                                  window_size=10, vote_window=5, \n",
        "                                  combination_method='confidence_weighted',\n",
        "                                  verbose=False):\n",
        "    \"\"\"\n",
        "    EXPERIMENT 1: Multi-directional windows with confidence-weighted aggregation\n",
        "    \n",
        "    Pipeline:\n",
        "    1. Train ensemble of models (same as baseline)\n",
        "    2. Create 3 directional windows (backward, centered, forward)\n",
        "    3. Get ensemble predictions for each direction\n",
        "    4. Combine directions using confidence weighting\n",
        "    5. Apply temporal voting\n",
        "    \n",
        "    Args:\n",
        "        combination_method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "    \"\"\"\n",
        "    # 0. Clear session and set seeds\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"\\n  Seed {seed}: Training ensemble...\")\n",
        "    \n",
        "    # 1. Train Ensemble Models (SAME AS BASELINE)\n",
        "    models, label_encoder = train_ensemble_models(\n",
        "        train_df,\n",
        "        n_models=n_ensemble,\n",
        "        base_seed=seed,\n",
        "        verbose=verbose\n",
        "    )\n",
        "    \n",
        "    if verbose:\n",
        "        print(\"  Creating multi-directional windows...\")\n",
        "    \n",
        "    # 2. Prepare Test Data with Multi-Directional Windows (NEW)\n",
        "    test_vectors = create_beacon_count_vectors(test_df)\n",
        "    direction_windows = create_multidirectional_windows(test_vectors, window_size=window_size)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"    Backward windows: {len(direction_windows['backward']['sequences'])}\")\n",
        "        print(f\"    Centered windows: {len(direction_windows['centered']['sequences'])}\")\n",
        "        print(f\"    Forward windows: {len(direction_windows['forward']['sequences'])}\")\n",
        "        print(\"  Getting directional predictions...\")\n",
        "    \n",
        "    # 3. Get Predictions for Each Direction (NEW)\n",
        "    direction_results = {}\n",
        "    for direction_name in ['backward', 'centered', 'forward']:\n",
        "        if verbose:\n",
        "            print(f\"    Predicting {direction_name}...\", end=\" \")\n",
        "        \n",
        "        sequences = direction_windows[direction_name]['sequences']\n",
        "        proba = predict_single_direction(models, sequences, max_seq_length=50)\n",
        "        \n",
        "        direction_results[direction_name] = {\n",
        "            'proba': proba,\n",
        "            'indices': direction_windows[direction_name]['indices'],\n",
        "            'labels': direction_windows[direction_name]['labels']\n",
        "        }\n",
        "        \n",
        "        if verbose:\n",
        "            avg_conf = np.mean(np.max(proba, axis=1))\n",
        "            print(f\"avg confidence: {avg_conf:.3f}\")\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Combining directions using {combination_method}...\")\n",
        "    \n",
        "    # 4. Combine Directional Predictions (NEW)\n",
        "    combined_proba, position_map = combine_directional_predictions(\n",
        "        direction_results, \n",
        "        method=combination_method\n",
        "    )\n",
        "    \n",
        "    # Get ground truth labels in same order as combined predictions\n",
        "    y_test = []\n",
        "    for pos in sorted(position_map.keys()):\n",
        "        # Use label from any direction (they should all be the same for a given position)\n",
        "        for direction_name in ['backward', 'centered', 'forward']:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                y_test.append(direction_results[direction_name]['labels'][idx])\n",
        "                break\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  Applying temporal voting (window={vote_window})...\")\n",
        "    \n",
        "    # 5. Apply Confidence-Weighted Temporal Voting (SAME AS BASELINE)\n",
        "    y_pred_voted_encoded = apply_confidence_weighted_voting(combined_proba, vote_window=vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "    \n",
        "    # 6. Final Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "    \n",
        "    if verbose:\n",
        "        print(f\"  ✓ Macro F1: {macro_f1:.4f}\")\n",
        "    \n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)},\n",
        "        'combination_method': combination_method\n",
        "    }\n",
        "\n",
        "print(\"✅ Complete multi-directional pipeline defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Run Experiment 1: Multi-Directional Windows\n",
        "\n",
        "## Test on Fold 1 First (Quick Validation)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# QUICK TEST: Fold 1 only, 3 seeds, compare baseline vs multi-directional\n",
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 1: MULTI-DIRECTIONAL WINDOWS WITH CONFIDENCE WEIGHTING\")\n",
        "print(\"Testing on Fold 1 with 3 seeds\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seeds = [42, 123, 456]\n",
        "train_df, test_df = train_df_1, test_df_1\n",
        "\n",
        "results_multidirectional = []\n",
        "\n",
        "for seed in seeds:\n",
        "    print(f\"\\nRunning seed {seed}...\")\n",
        "    result = run_multidirectional_pipeline(\n",
        "        train_df, test_df, \n",
        "        seed=seed,\n",
        "        n_ensemble=5,\n",
        "        window_size=10,\n",
        "        vote_window=5,\n",
        "        combination_method='confidence_weighted',\n",
        "        verbose=True\n",
        "    )\n",
        "    results_multidirectional.append(result)\n",
        "\n",
        "# Calculate statistics\n",
        "macro_f1_scores = [r['macro_f1'] for r in results_multidirectional]\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT 1 RESULTS (Fold 1, 3 seeds)\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nMulti-Directional (Confidence-Weighted):\")\n",
        "print(f\"  Mean Macro F1: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
        "print(f\"  Individual runs:\")\n",
        "for result in results_multidirectional:\n",
        "    print(f\"    Seed {result['seed']}: {result['macro_f1']:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON TO BASELINE (from your notes):\")\n",
        "print(\"=\"*80)\n",
        "print(f\"Baseline (single direction, backward 10s):\")\n",
        "print(f\"  Fold 1 typical range: 0.38-0.42 F1\")\n",
        "print(f\"  With ensemble + conf voting: ~0.45 F1\")\n",
        "print(f\"\\nExperiment 1 (multi-directional):\")\n",
        "print(f\"  Mean: {np.mean(macro_f1_scores):.4f}\")\n",
        "print(f\"\\nTarget: Beat 0.41 F1 to prove multi-directional helps\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# If Experiment 1 Works: Run Full 4-Fold CV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FULL EXPERIMENT: All 4 folds, 3 seeds each (or 10 if you want full validation)\n",
        "# Only run this if quick test above shows promise!\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"FULL 4-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seeds = [42, 123, 456]  # Change to 10 seeds if you want full validation\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    fold_results = []\n",
        "    \n",
        "    for seed in seeds:\n",
        "        print(f\"  Running seed {seed}...\", end=\" \")\n",
        "        result = run_multidirectional_pipeline(\n",
        "            train_df, test_df, \n",
        "            seed=seed,\n",
        "            n_ensemble=5,\n",
        "            window_size=10,\n",
        "            vote_window=5,\n",
        "            combination_method='confidence_weighted',\n",
        "            verbose=False\n",
        "        )\n",
        "        fold_results.append(result)\n",
        "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "    \n",
        "    all_fold_results[fold_num] = fold_results\n",
        "    \n",
        "    # Calculate fold statistics\n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
        "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
        "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY - EXPERIMENT 1\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "all_macro_f1 = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON TO BASELINE:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Baseline (from your Approach 24):\")\n",
        "print(\"  Overall: 0.4106 ± 0.0266\")\n",
        "print(f\"\\nExperiment 1 (Multi-directional):\")\n",
        "print(f\"  Overall: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"\\nDifference: {np.mean(all_macro_f1) - 0.4106:+.4f}\")\n",
        "\n",
        "if np.mean(all_macro_f1) > 0.4106:\n",
        "    print(\"\\n✅ SUCCESS! Multi-directional windows improved performance!\")\n",
        "    print(\"   Next step: Try Experiment 2 with more window sizes\")\n",
        "else:\n",
        "    print(\"\\n⚠️  No improvement from multi-directional approach\")\n",
        "    print(\"   Consider: Different combination methods or window sizes\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Save Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to text file\n",
        "with open('experiment1_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"EXPERIMENT 1: MULTI-DIRECTIONAL WINDOWS WITH CONFIDENCE WEIGHTING\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"Configuration:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Window size: 10 seconds\\n\")\n",
        "    f.write(\"Directions: Backward, Centered, Forward\\n\")\n",
        "    f.write(\"Combination method: Confidence-weighted\\n\")\n",
        "    f.write(\"Ensemble size: 5 models\\n\")\n",
        "    f.write(\"Temporal voting window: 5 seconds\\n\\n\")\n",
        "    \n",
        "    # Overall summary\n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "    \n",
        "    f.write(\"OVERALL RESULTS:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
        "    \n",
        "    # Comparison to baseline\n",
        "    f.write(\"COMPARISON TO BASELINE:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Baseline (Approach 24 - single direction backward):\\n\")\n",
        "    f.write(\"  Overall: 0.4106 ± 0.0266\\n\")\n",
        "    f.write(f\"\\nExperiment 1 (Multi-directional):\\n\")\n",
        "    f.write(f\"  Overall: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"\\nAbsolute Difference: {np.mean(all_macro_f1) - 0.4106:+.4f}\\n\")\n",
        "    f.write(f\"Relative Improvement: {((np.mean(all_macro_f1) - 0.4106) / 0.4106 * 100):+.2f}%\\n\\n\")\n",
        "    \n",
        "    # Per-fold results\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\n{'='*80}\\n\")\n",
        "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "        \n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "        \n",
        "        f.write(f\"Macro F1 Scores:\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        for i, result in enumerate(fold_results):\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
        "        \n",
        "        f.write(f\"\\nStatistics:\\n\")\n",
        "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
        "        \n",
        "        # Per-class F1 (averaged across seeds)\n",
        "        f.write(f\"\\nPer-Class F1 Scores (averaged across seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        \n",
        "        # Collect all class names\n",
        "        all_classes = set()\n",
        "        for result in fold_results:\n",
        "            all_classes.update(result['per_class_f1'].keys())\n",
        "        \n",
        "        # Average per-class F1 across seeds\n",
        "        for class_name in sorted(all_classes):\n",
        "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
        "            mean_f1 = np.mean(class_f1_scores)\n",
        "            std_f1 = np.std(class_f1_scores)\n",
        "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} ± {std_f1:.4f}\\n\")\n",
        "\n",
        "print(\"✅ Results saved to experiment1_results.txt\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
