{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "60VMoKcqNeNE",
        "outputId": "2bf6cbf8-8fd8-403e-f5c3-66769a1fa34e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"✓ All imports successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vmy0nM8ONqO2",
        "outputId": "6d7613bb-9cb6-4f5d-9e7a-e4f916a01ddf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All imports successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"✓ All folds loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P0Scobs0NqRs",
        "outputId": "6974fd0a-c699-4e2f-eb65-bd7af8d14a1f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ All folds loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns # Check if we are in 'training' mode\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def create_sliding_windows_by_day(vector_df, window_size=10):\n",
        "    \"\"\"Used for Inference: Creates a sequence for every frame, respecting day boundaries.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        if len(day_group) >= window_size:\n",
        "            vectors = list(day_group['beacon_vector'])\n",
        "            rooms = list(day_group['room'])\n",
        "\n",
        "            for i in range(len(vectors) - window_size + 1):\n",
        "                window = vectors[i : i + window_size]\n",
        "                sequences.append(window)\n",
        "                # Goal: Predict the room at the final timestamp of the window\n",
        "                labels.append(rooms[i + window_size - 1])\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def build_bidirectional_gru_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU Architecture\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "def create_multi_directional_windows_by_day(vector_df, window_size=10):\n",
        "    \"\"\"\n",
        "    Creates 3 types of windows for each timestamp:\n",
        "    1. Backward-looking: [t-9, t-8, ..., t-1, t] + right-padding\n",
        "    2. Centered: [t-5, t-4, ..., t, ..., t+3, t+4] + right-padding\n",
        "    3. Forward-looking: [t, t+1, ..., t+8, t+9] + right-padding\n",
        "\n",
        "    All windows use RIGHT-PADDING for cuDNN compatibility.\n",
        "    \"\"\"\n",
        "    backward_sequences = []\n",
        "    centered_sequences = []\n",
        "    forward_sequences = []\n",
        "    labels = []\n",
        "    window_types = []\n",
        "\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        day_length = len(vectors)\n",
        "\n",
        "        if day_length < window_size:\n",
        "            continue\n",
        "\n",
        "        # For each position in the day\n",
        "        for i in range(day_length):\n",
        "            valid_windows = []\n",
        "            zero_vector = [0.0] * len(vectors[0])\n",
        "\n",
        "            # === BACKWARD WINDOW: [t-9, ..., t] ===\n",
        "            if i >= window_size - 1:\n",
        "                # Full backward window available\n",
        "                backward_window = vectors[i - window_size + 1 : i + 1]\n",
        "            else:\n",
        "                # Not enough history - take what we have and RIGHT-PAD\n",
        "                backward_window = vectors[0 : i + 1]\n",
        "                padding_needed = window_size - len(backward_window)\n",
        "                backward_window = backward_window + [zero_vector] * padding_needed\n",
        "\n",
        "            backward_sequences.append(backward_window)\n",
        "            valid_windows.append('backward')\n",
        "\n",
        "            # === CENTERED WINDOW: [t-5, ..., t, ..., t+4] ===\n",
        "            half_window = window_size // 2\n",
        "            start = max(0, i - half_window)\n",
        "            end = min(day_length, i + half_window)\n",
        "\n",
        "            centered_window = vectors[start : end]\n",
        "            # Right-pad if needed\n",
        "            if len(centered_window) < window_size:\n",
        "                padding_needed = window_size - len(centered_window)\n",
        "                centered_window = centered_window + [zero_vector] * padding_needed\n",
        "\n",
        "            centered_sequences.append(centered_window)\n",
        "            valid_windows.append('centered')\n",
        "\n",
        "            # === FORWARD WINDOW: [t, t+1, ..., t+9] ===\n",
        "            if i + window_size <= day_length:\n",
        "                # Full forward window available\n",
        "                forward_window = vectors[i : i + window_size]\n",
        "            else:\n",
        "                # Not enough future - take what we have and RIGHT-PAD\n",
        "                forward_window = vectors[i : day_length]\n",
        "                padding_needed = window_size - len(forward_window)\n",
        "                forward_window = forward_window + [zero_vector] * padding_needed\n",
        "\n",
        "            forward_sequences.append(forward_window)\n",
        "            valid_windows.append('forward')\n",
        "\n",
        "            labels.append(rooms[i])\n",
        "            window_types.append(valid_windows)\n",
        "\n",
        "    return backward_sequences, centered_sequences, forward_sequences, labels, window_types\n",
        "\n",
        "print(\"✅ Multi-directional ensemble functions defined (RIGHT-PADDING for cuDNN)\")\n",
        "\n",
        "def ensemble_predictions_with_confidence(model, X_backward, X_centered, X_forward, window_types):\n",
        "    \"\"\"\n",
        "    Gets predictions from all 3 window types and selects the one with highest confidence.\n",
        "    Handles edge cases where certain windows are padded/invalid.\n",
        "    \"\"\"\n",
        "    # Get probability predictions from all 3 models\n",
        "    probs_backward = model.predict(X_backward, verbose=0)\n",
        "    probs_centered = model.predict(X_centered, verbose=0)\n",
        "    probs_forward = model.predict(X_forward, verbose=0)\n",
        "\n",
        "    # Extract max confidence and predicted class for each\n",
        "    conf_backward = np.max(probs_backward, axis=1)\n",
        "    pred_backward = np.argmax(probs_backward, axis=1)\n",
        "\n",
        "    conf_centered = np.max(probs_centered, axis=1)\n",
        "    pred_centered = np.argmax(probs_centered, axis=1)\n",
        "\n",
        "    conf_forward = np.max(probs_forward, axis=1)\n",
        "    pred_forward = np.argmax(probs_forward, axis=1)\n",
        "\n",
        "    # For each sample, choose prediction with highest confidence\n",
        "    final_predictions = []\n",
        "\n",
        "    for i in range(len(pred_backward)):\n",
        "        # Compare confidences from all 3 windows\n",
        "        confidences = [conf_backward[i], conf_centered[i], conf_forward[i]]\n",
        "        predictions = [pred_backward[i], pred_centered[i], pred_forward[i]]\n",
        "\n",
        "        # Choose the prediction with highest confidence\n",
        "        max_conf_idx = np.argmax(confidences)\n",
        "        final_predictions.append(predictions[max_conf_idx])\n",
        "\n",
        "    return np.array(final_predictions)\n",
        "\n",
        "print(\"✅ Multi-directional ensemble functions defined\")\n",
        "print(\"✅ Bidirectional GRU model function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FG2rO5MNqUE",
        "outputId": "a3c4055a-dbf5-4fda-f6fb-56ef72174242"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Multi-directional ensemble functions defined (RIGHT-PADDING for cuDNN)\n",
            "✅ Multi-directional ensemble functions defined\n",
            "✅ Bidirectional GRU model function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline_single_seed(train_df, test_df, seed, verbose=False):\n",
        "    \"\"\"\n",
        "    Run realistic pipeline with Multi-Directional Ensemble + Temporal Voting.\n",
        "    1. Resets memory/seeds.\n",
        "    2. Trains on pure segments.\n",
        "    3. Infers with 3 sliding window directions (backward, centered, forward).\n",
        "    4. Ensemble: Chooses prediction with highest confidence.\n",
        "    5. Smooths predictions with a Majority Vote filter.\n",
        "    \"\"\"\n",
        "    # 0. PREVENT MEMORY LEAKS & ENSURE SEEDING\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "\n",
        "    # HYPERPARAMETERS\n",
        "    window_size = 10     # Window size for all 3 directions\n",
        "    vote_window = 5      # The smoothing neighborhood (5 seconds)\n",
        "    max_seq_length = 50\n",
        "\n",
        "    # 1. Preprocessing\n",
        "    train_df = create_room_groups(train_df)\n",
        "    train_vectors = create_beacon_count_vectors(train_df)\n",
        "    test_vectors = create_beacon_count_vectors(test_df)\n",
        "\n",
        "    # 2. Sequence Creation\n",
        "    X_train, y_train = create_sequences_from_groups(train_vectors, max_length=max_seq_length)\n",
        "\n",
        "    # NEW: Create multi-directional windows for test\n",
        "    X_test_backward, X_test_centered, X_test_forward, y_test, window_types = \\\n",
        "        create_multi_directional_windows_by_day(test_vectors, window_size=window_size)\n",
        "\n",
        "    # 3. Encoding & Padding\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(list(y_train) + list(y_test))\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    # Pad all 3 test window types\n",
        "    X_test_backward_padded = pad_sequences(X_test_backward, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "    X_test_centered_padded = pad_sequences(X_test_centered, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "    X_test_forward_padded = pad_sequences(X_test_forward, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    # 4. Train Model with Macro F1 Optimization\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    model = build_bidirectional_gru_model(input_shape=(max_seq_length, 23), num_classes=len(label_encoder.classes_))\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=0)\n",
        "    ]\n",
        "\n",
        "    # Use backward window for validation (standard approach)\n",
        "    model.fit(\n",
        "        X_train_padded, y_train_encoded,\n",
        "        validation_data=(X_test_backward_padded, y_test_encoded),\n",
        "        epochs=100, batch_size=32,\n",
        "        class_weight=class_weight_dict,\n",
        "        callbacks=callbacks, verbose=0\n",
        "    )\n",
        "\n",
        "    # 5. MULTI-DIRECTIONAL ENSEMBLE INFERENCE\n",
        "    y_pred_raw_encoded = ensemble_predictions_with_confidence(\n",
        "        model,\n",
        "        X_test_backward_padded,\n",
        "        X_test_centered_padded,\n",
        "        X_test_forward_padded,\n",
        "        window_types\n",
        "    )\n",
        "\n",
        "    # 6. TEMPORAL VOTING (Smoothing)\n",
        "    def apply_temporal_voting(preds, v_window):\n",
        "        \"\"\"Applies a majority vote filter to smooth room predictions.\"\"\"\n",
        "        smoothed = []\n",
        "        for i in range(len(preds)):\n",
        "            start = max(0, i - v_window // 2)\n",
        "            end = min(len(preds), i + v_window // 2 + 1)\n",
        "            neighborhood = preds[start:end]\n",
        "            smoothed.append(np.bincount(neighborhood).argmax())\n",
        "        return np.array(smoothed)\n",
        "\n",
        "    y_pred_voted_encoded = apply_temporal_voting(y_pred_raw_encoded, vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "\n",
        "    # 7. Final Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)}\n",
        "    }\n",
        "\n",
        "print(\"✓ Pipeline updated: Now uses Multi-Directional Ensemble (Backward + Centered + Forward)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rc_pqeTjhyw",
        "outputId": "4ebf01cc-0ed6-41ec-aa65-fd7ba041d15c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Pipeline updated: Now uses Multi-Directional Ensemble (Backward + Centered + Forward)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Rt7ry8WNqbj",
        "outputId": "71e5111e-bdd3-4d27-bdd2-5dfba4eb8370"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run 10 seeds for each of 4 folds\n",
        "seeds = [42, 123, 456, 789, 2024, 3141, 5926, 8888, 1337, 9999]\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"  Running seed {seed}...\", end=\" \")\n",
        "        result = run_pipeline_single_seed(train_df, test_df, seed, verbose=False)\n",
        "        fold_results.append(result)\n",
        "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "\n",
        "    all_fold_results[fold_num] = fold_results\n",
        "\n",
        "    # Calculate fold statistics\n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
        "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
        "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bOCMii8ENqeP",
        "outputId": "558d0f3a-7886-4cf8-d22c-f6849d0e4a25"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 1\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.3051\n",
            "  Running seed 123... Macro F1: 0.4499\n",
            "  Running seed 456... Macro F1: 0.3712\n",
            "  Running seed 789... Macro F1: 0.5420\n",
            "  Running seed 2024... Macro F1: 0.4566\n",
            "  Running seed 3141... Macro F1: 0.3650\n",
            "  Running seed 5926... Macro F1: 0.5293\n",
            "  Running seed 8888... Macro F1: 0.4271\n",
            "  Running seed 1337... Macro F1: 0.3962\n",
            "  Running seed 9999... Macro F1: 0.4287\n",
            "\n",
            "  Fold 1 Summary:\n",
            "    Mean Macro F1: 0.4271 ± 0.0692\n",
            "    Min: 0.3051, Max: 0.5420\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 2\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4110\n",
            "  Running seed 123... Macro F1: 0.3425\n",
            "  Running seed 456... Macro F1: 0.4468\n",
            "  Running seed 789... Macro F1: 0.3036\n",
            "  Running seed 2024... Macro F1: 0.4156\n",
            "  Running seed 3141... Macro F1: 0.3752\n",
            "  Running seed 5926... Macro F1: 0.3789\n",
            "  Running seed 8888... Macro F1: 0.3647\n",
            "  Running seed 1337... Macro F1: 0.3998\n",
            "  Running seed 9999... Macro F1: 0.3487\n",
            "\n",
            "  Fold 2 Summary:\n",
            "    Mean Macro F1: 0.3787 ± 0.0394\n",
            "    Min: 0.3036, Max: 0.4468\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 3\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.2567\n",
            "  Running seed 123... Macro F1: 0.3158\n",
            "  Running seed 456... Macro F1: 0.3286\n",
            "  Running seed 789... Macro F1: 0.5003\n",
            "  Running seed 2024... Macro F1: 0.3312\n",
            "  Running seed 3141... Macro F1: 0.3081\n",
            "  Running seed 5926... Macro F1: 0.4293\n",
            "  Running seed 8888... Macro F1: 0.4248\n",
            "  Running seed 1337... Macro F1: 0.3829\n",
            "  Running seed 9999... Macro F1: 0.3925\n",
            "\n",
            "  Fold 3 Summary:\n",
            "    Mean Macro F1: 0.3670 ± 0.0685\n",
            "    Min: 0.2567, Max: 0.5003\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 4\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.2987\n",
            "  Running seed 123... Macro F1: 0.3328\n",
            "  Running seed 456... Macro F1: 0.3680\n",
            "  Running seed 789... Macro F1: 0.3119\n",
            "  Running seed 2024... Macro F1: 0.3516\n",
            "  Running seed 3141... Macro F1: 0.4190\n",
            "  Running seed 5926... Macro F1: 0.3943\n",
            "  Running seed 8888... Macro F1: 0.4192\n",
            "  Running seed 1337... Macro F1: 0.3979\n",
            "  Running seed 9999... Macro F1: 0.4246\n",
            "\n",
            "  Fold 4 Summary:\n",
            "    Mean Macro F1: 0.3718 ± 0.0439\n",
            "    Min: 0.2987, Max: 0.4246\n",
            "\n",
            "================================================================================\n",
            "ALL FOLDS COMPLETED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to text file\n",
        "with open('4fold_10seed_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"4-FOLD CROSS-VALIDATION WITH 10 SEEDS PER FOLD\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    # Overall summary\n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "\n",
        "    f.write(\"OVERALL RESULTS (40 runs total):\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
        "\n",
        "    # Per-fold results\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\n{'='*80}\\n\")\n",
        "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "\n",
        "        f.write(f\"Macro F1 Scores (10 seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        for i, result in enumerate(fold_results):\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
        "\n",
        "        f.write(f\"\\nStatistics:\\n\")\n",
        "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
        "\n",
        "        # Per-class F1 (averaged across 10 seeds)\n",
        "        f.write(f\"\\nPer-Class F1 Scores (averaged across 10 seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Collect all class names\n",
        "        all_classes = set()\n",
        "        for result in fold_results:\n",
        "            all_classes.update(result['per_class_f1'].keys())\n",
        "\n",
        "        # Average per-class F1 across seeds\n",
        "        for class_name in sorted(all_classes):\n",
        "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
        "            mean_f1 = np.mean(class_f1_scores)\n",
        "            std_f1 = np.std(class_f1_scores)\n",
        "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} ± {std_f1:.4f}\\n\")\n",
        "\n",
        "print(\"✅ Results saved to 4fold_10seed_results.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNURmdwCN0gY",
        "outputId": "fcf78424-0cd3-4771-c59e-99719f045f79"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Results saved to 4fold_10seed_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY - 4 FOLDS × 10 SEEDS = 40 TOTAL RUNS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "all_macro_f1 = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPP_0K0rN2XE",
        "outputId": "8d3869c5-f5c2-4d4a-9068-eb40ea690cb1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY - 4 FOLDS × 10 SEEDS = 40 TOTAL RUNS\n",
            "================================================================================\n",
            "\n",
            "Fold 1: 0.4271 ± 0.0692\n",
            "Fold 2: 0.3787 ± 0.0394\n",
            "Fold 3: 0.3670 ± 0.0685\n",
            "Fold 4: 0.3718 ± 0.0439\n",
            "\n",
            "================================================================================\n",
            "Overall Mean: 0.3862 ± 0.0618\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}