{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8739d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fd95f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_filter_fold(i):\n",
    "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
    "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
    "    train_df = pd.read_csv(train_dir)\n",
    "    test_df = pd.read_csv(test_dir)\n",
    "\n",
    "    train_labels = list(train_df['room'].unique())\n",
    "    test_labels = list(test_df['room'].unique())\n",
    "    common_labels = list(set(train_labels) & set(test_labels))\n",
    "\n",
    "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
    "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Load all 4 folds\n",
    "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
    "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
    "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
    "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
    "\n",
    "print(\"✓ All folds loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337a51dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "def create_room_groups(df):\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
    "    return df\n",
    "\n",
    "def create_beacon_count_vectors(df):\n",
    "    vectors = []\n",
    "    for _, group in df.groupby('timestamp'):\n",
    "        beacon_counts = group['mac address'].value_counts()\n",
    "        total_readings = len(group)\n",
    "        \n",
    "        vector = [0.0] * 23\n",
    "        for beacon_id, count in beacon_counts.items():\n",
    "            if 1 <= beacon_id <= 23:\n",
    "                vector[beacon_id - 1] = count / total_readings\n",
    "        \n",
    "        vectors.append({\n",
    "            'timestamp': group['timestamp'].iloc[0],\n",
    "            'room': group['room'].iloc[0],\n",
    "            'room_group': group['room_group'].iloc[0],\n",
    "            'beacon_vector': vector\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(vectors)\n",
    "\n",
    "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
    "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
    "        seq_length = len(group)\n",
    "        \n",
    "        if seq_length < min_length:\n",
    "            continue\n",
    "        \n",
    "        if seq_length > max_length:\n",
    "            group = group.tail(max_length)\n",
    "            seq_length = max_length\n",
    "        \n",
    "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(room)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "def build_cnn_lstm_model(input_shape, num_classes):\n",
    "    \"\"\"\n",
    "    CNN-LSTM Architecture:\n",
    "    - 1D CNN layers extract local beacon co-occurrence patterns\n",
    "    - LSTM layers model temporal evolution of these patterns\n",
    "    - Dense layers perform final classification\n",
    "    \"\"\"\n",
    "    \n",
    "    model = Sequential([\n",
    "        # Masking layer for padded sequences\n",
    "        Masking(mask_value=0.0, input_shape=input_shape),\n",
    "        \n",
    "        # First CNN block - detect local beacon patterns\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Second CNN block - learn complex beacon combinations\n",
    "        Conv1D(filters=128, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Third CNN block - higher-level pattern extraction\n",
    "        Conv1D(filters=64, kernel_size=3, activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # LSTM layers - model temporal evolution\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        \n",
    "        # Dense layers - final classification\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"✅ Helper functions defined (with CNN-LSTM architecture)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890eb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipeline_single_seed(train_df, test_df, seed, verbose=False):\n",
    "    \"\"\"Run pipeline for a single seed\"\"\"\n",
    "    set_seeds(seed)\n",
    "    \n",
    "    # Preprocessing\n",
    "    train_df = create_room_groups(train_df)\n",
    "    test_df = create_room_groups(test_df)\n",
    "    \n",
    "    train_vectors = create_beacon_count_vectors(train_df)\n",
    "    test_vectors = create_beacon_count_vectors(test_df)\n",
    "    \n",
    "    # Sequence creation\n",
    "    X_train, y_train = create_sequences_from_groups(train_vectors)\n",
    "    X_test, y_test = create_sequences_from_groups(test_vectors)\n",
    "    \n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(y_train + y_test)\n",
    "    y_train_encoded = label_encoder.transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    # Pad sequences\n",
    "    max_seq_length = 50\n",
    "    X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, dtype='float32', padding='pre', value=0.0)\n",
    "    X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, dtype='float32', padding='pre', value=0.0)\n",
    "    \n",
    "    # Calculate class weights\n",
    "    class_weights = compute_class_weight(\n",
    "        class_weight='balanced',\n",
    "        classes=np.unique(y_train_encoded),\n",
    "        y=y_train_encoded\n",
    "    )\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    # Build and train model\n",
    "    model = build_cnn_lstm_model(\n",
    "        input_shape=(max_seq_length, 23),\n",
    "        num_classes=len(label_encoder.classes_)\n",
    "    )\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    history = model.fit(\n",
    "        X_train_padded, y_train_encoded,\n",
    "        validation_data=(X_test_padded, y_test_encoded),\n",
    "        epochs=100,\n",
    "        batch_size=32,\n",
    "        class_weight=class_weight_dict,\n",
    "        callbacks=callbacks,\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Predictions (sequence level only - no frame propagation)\n",
    "    y_pred_probs = model.predict(X_test_padded, verbose=0)\n",
    "    y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    \n",
    "    # Calculate macro F1\n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    \n",
    "    # Calculate per-class F1\n",
    "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
    "    per_class_f1_dict = {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)}\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': per_class_f1_dict\n",
    "    }\n",
    "\n",
    "print(\"✓ Pipeline function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfc71925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run 10 seeds for each of 4 folds\n",
    "seeds = [42, 123, 456, 789, 2024, 3141, 5926, 8888, 1337, 9999]\n",
    "folds = {\n",
    "    1: (train_df_1, test_df_1),\n",
    "    2: (train_df_2, test_df_2),\n",
    "    3: (train_df_3, test_df_3),\n",
    "    4: (train_df_4, test_df_4)\n",
    "}\n",
    "\n",
    "all_fold_results = {}\n",
    "\n",
    "for fold_num, (train_df, test_df) in folds.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING FOLD {fold_num}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"  Running seed {seed}...\", end=\" \")\n",
    "        result = run_pipeline_single_seed(train_df, test_df, seed, verbose=False)\n",
    "        fold_results.append(result)\n",
    "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
    "    \n",
    "    all_fold_results[fold_num] = fold_results\n",
    "    \n",
    "    # Calculate fold statistics\n",
    "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
    "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
    "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
    "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL FOLDS COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82693ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to text file\n",
    "with open('4fold_10seed_results.txt', 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"4-FOLD CROSS-VALIDATION WITH 10 SEEDS PER FOLD\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    # Overall summary\n",
    "    all_macro_f1 = []\n",
    "    for fold_num in [1, 2, 3, 4]:\n",
    "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
    "        all_macro_f1.extend(fold_scores)\n",
    "    \n",
    "    f.write(\"OVERALL RESULTS (40 runs total):\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\\n\")\n",
    "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
    "    \n",
    "    # Per-fold results\n",
    "    for fold_num in [1, 2, 3, 4]:\n",
    "        f.write(f\"\\n{'='*80}\\n\")\n",
    "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
    "        f.write(f\"{'='*80}\\n\\n\")\n",
    "        \n",
    "        fold_results = all_fold_results[fold_num]\n",
    "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
    "        \n",
    "        f.write(f\"Macro F1 Scores (10 seeds):\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for i, result in enumerate(fold_results):\n",
    "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nStatistics:\\n\")\n",
    "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\\n\")\n",
    "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
    "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
    "        \n",
    "        # Per-class F1 (averaged across 10 seeds)\n",
    "        f.write(f\"\\nPer-Class F1 Scores (averaged across 10 seeds):\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        # Collect all class names\n",
    "        all_classes = set()\n",
    "        for result in fold_results:\n",
    "            all_classes.update(result['per_class_f1'].keys())\n",
    "        \n",
    "        # Average per-class F1 across seeds\n",
    "        for class_name in sorted(all_classes):\n",
    "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
    "            mean_f1 = np.mean(class_f1_scores)\n",
    "            std_f1 = np.std(class_f1_scores)\n",
    "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} ± {std_f1:.4f}\\n\")\n",
    "\n",
    "print(\"✅ Results saved to 4fold_10seed_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY - 4 FOLDS × 10 SEEDS = 40 TOTAL RUNS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for fold_num in [1, 2, 3, 4]:\n",
    "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
    "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
    "\n",
    "all_macro_f1 = []\n",
    "for fold_num in [1, 2, 3, 4]:\n",
    "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
