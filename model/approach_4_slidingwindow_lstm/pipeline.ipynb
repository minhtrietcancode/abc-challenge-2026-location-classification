{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8739d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful\n"
     ]
    }
   ],
   "source": [
    "# Setup and Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ All imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7fd95f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All folds loaded\n"
     ]
    }
   ],
   "source": [
    "def load_and_filter_fold(i):\n",
    "    train_dir = f'../../cleaned_dataset/split_data/fold{i}/train.csv'  \n",
    "    test_dir = f'../../cleaned_dataset/split_data/fold{i}/test.csv'   \n",
    "    train_df = pd.read_csv(train_dir)\n",
    "    test_df = pd.read_csv(test_dir)\n",
    "\n",
    "    train_labels = list(train_df['room'].unique())\n",
    "    test_labels = list(test_df['room'].unique())\n",
    "    common_labels = list(set(train_labels) & set(test_labels))\n",
    "\n",
    "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
    "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "# Load all 4 folds\n",
    "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
    "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
    "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
    "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
    "\n",
    "print(\"✓ All folds loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337a51dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "\n",
    "def create_room_groups(df):\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
    "    return df\n",
    "\n",
    "def create_beacon_count_vectors(df):\n",
    "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
    "    vectors = []\n",
    "    has_groups = 'room_group' in df.columns # Check if we are in 'training' mode\n",
    "    \n",
    "    for _, group in df.groupby('timestamp'):\n",
    "        beacon_counts = group['mac address'].value_counts()\n",
    "        total_readings = len(group)\n",
    "        \n",
    "        vector = [0.0] * 23\n",
    "        for beacon_id, count in beacon_counts.items():\n",
    "            if 1 <= beacon_id <= 23:\n",
    "                vector[int(beacon_id) - 1] = count / total_readings\n",
    "        \n",
    "        entry = {\n",
    "            'timestamp': group['timestamp'].iloc[0],\n",
    "            'room': group['room'].iloc[0],\n",
    "            'beacon_vector': vector\n",
    "        }\n",
    "        \n",
    "        if has_groups:\n",
    "            entry['room_group'] = group['room_group'].iloc[0]\n",
    "            \n",
    "        vectors.append(entry)\n",
    "    \n",
    "    return pd.DataFrame(vectors)\n",
    "\n",
    "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
    "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
    "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
    "        seq_length = len(group)\n",
    "        \n",
    "        if seq_length < min_length:\n",
    "            continue\n",
    "        \n",
    "        if seq_length > max_length:\n",
    "            group = group.tail(max_length)\n",
    "        \n",
    "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
    "        sequences.append(sequence)\n",
    "        labels.append(room)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "def create_sliding_windows_by_day(vector_df, window_size=20):\n",
    "    \"\"\"Used for Inference: Creates a sequence for every frame, respecting day boundaries.\"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    # Ensure chronological order and group by day\n",
    "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
    "    vector_df['date'] = vector_df['dt'].dt.date\n",
    "    \n",
    "    for _, day_group in vector_df.groupby('date'):\n",
    "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        if len(day_group) >= window_size:\n",
    "            vectors = list(day_group['beacon_vector'])\n",
    "            rooms = list(day_group['room'])\n",
    "            \n",
    "            for i in range(len(vectors) - window_size + 1):\n",
    "                window = vectors[i : i + window_size]\n",
    "                sequences.append(window)\n",
    "                # Goal: Predict the room at the final timestamp of the window\n",
    "                labels.append(rooms[i + window_size - 1])\n",
    "            \n",
    "    return sequences, labels\n",
    "\n",
    "def build_lstm_model(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Masking(mask_value=0.0, input_shape=input_shape),\n",
    "        LSTM(128, return_sequences=True),\n",
    "        Dropout(0.3),\n",
    "        LSTM(64, return_sequences=False),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "    return model\n",
    "\n",
    "print(\"✓ Updated Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2890eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Updated Pipeline function defined\n"
     ]
    }
   ],
   "source": [
    "def run_pipeline_single_seed(train_df, test_df, seed, verbose=False):\n",
    "    \"\"\"Run realistic pipeline: Train on segments, Test on sliding windows.\"\"\"\n",
    "    set_seeds(seed)\n",
    "    window_size = 20    # Sliding window for inference\n",
    "    max_seq_length = 50 # Maximum sequence length for the LSTM\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    # Only train uses room_group segmentation to learn pure room signatures\n",
    "    train_df = create_room_groups(train_df)\n",
    "    \n",
    "    train_vectors = create_beacon_count_vectors(train_df)\n",
    "    test_vectors = create_beacon_count_vectors(test_df)\n",
    "    \n",
    "    # 2. Sequence Creation\n",
    "    X_train, y_train = create_sequences_from_groups(train_vectors, max_length=max_seq_length)\n",
    "    X_test, y_test = create_sliding_windows_by_day(test_vectors, window_size=window_size)\n",
    "    \n",
    "    # 3. Encoding & Padding\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(list(y_train) + list(y_test))\n",
    "    y_train_encoded = label_encoder.transform(y_train)\n",
    "    y_test_encoded = label_encoder.transform(y_test)\n",
    "    \n",
    "    # Pad to match LSTM input shape (max_seq_length, 23)\n",
    "    X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, dtype='float32', padding='pre', value=0.0)\n",
    "    X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, dtype='float32', padding='pre', value=0.0)\n",
    "    \n",
    "    # 4. Train Model with Macro F1 Optimization (Class Weights)\n",
    "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
    "    class_weight_dict = dict(enumerate(class_weights))\n",
    "    \n",
    "    model = build_lstm_model(input_shape=(max_seq_length, 23), num_classes=len(label_encoder.classes_))\n",
    "    \n",
    "    callbacks = [\n",
    "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=0)\n",
    "    ]\n",
    "    \n",
    "    model.fit(\n",
    "        X_train_padded, y_train_encoded, \n",
    "        validation_data=(X_test_padded, y_test_encoded),\n",
    "        epochs=100, batch_size=32, \n",
    "        class_weight=class_weight_dict, \n",
    "        callbacks=callbacks, verbose=0\n",
    "    )\n",
    "    \n",
    "    # 5. Evaluate (Predict on every frame generated by the sliding window)\n",
    "    y_pred_probs = model.predict(X_test_padded, verbose=0)\n",
    "    y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
    "    y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "    \n",
    "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
    "    \n",
    "    return {\n",
    "        'seed': seed,\n",
    "        'macro_f1': macro_f1,\n",
    "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)}\n",
    "    }\n",
    "\n",
    "print(\"✓ Updated Pipeline function defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bfc71925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "PROCESSING FOLD 1\n",
      "================================================================================\n",
      "\n",
      "  Running seed 42... Macro F1: 0.3626\n",
      "  Running seed 123... Macro F1: 0.3176\n",
      "  Running seed 456... Macro F1: 0.3756\n",
      "  Running seed 789... Macro F1: 0.3873\n",
      "  Running seed 2024... Macro F1: 0.3128\n",
      "  Running seed 3141... Macro F1: 0.4255\n",
      "  Running seed 5926... Macro F1: 0.3649\n",
      "  Running seed 8888... Macro F1: 0.3303\n",
      "  Running seed 1337... Macro F1: 0.3812\n",
      "  Running seed 9999... Macro F1: 0.3257\n",
      "\n",
      "  Fold 1 Summary:\n",
      "    Mean Macro F1: 0.3584 ± 0.0344\n",
      "    Min: 0.3128, Max: 0.4255\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FOLD 2\n",
      "================================================================================\n",
      "\n",
      "  Running seed 42... Macro F1: 0.2307\n",
      "  Running seed 123... Macro F1: 0.2407\n",
      "  Running seed 456... Macro F1: 0.2652\n",
      "  Running seed 789... Macro F1: 0.3072\n",
      "  Running seed 2024... Macro F1: 0.2921\n",
      "  Running seed 3141... Macro F1: 0.2686\n",
      "  Running seed 5926... Macro F1: 0.2752\n",
      "  Running seed 8888... Macro F1: 0.2673\n",
      "  Running seed 1337... Macro F1: 0.3087\n",
      "  Running seed 9999... Macro F1: 0.2098\n",
      "\n",
      "  Fold 2 Summary:\n",
      "    Mean Macro F1: 0.2666 ± 0.0306\n",
      "    Min: 0.2098, Max: 0.3087\n",
      "\n",
      "================================================================================\n",
      "PROCESSING FOLD 3\n",
      "================================================================================\n",
      "\n",
      "  Running seed 42... Macro F1: 0.2691\n",
      "  Running seed 123... Macro F1: 0.2570\n",
      "  Running seed 456... "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m seed \u001b[38;5;129;01min\u001b[39;00m seeds:\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  Running seed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mseed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mrun_pipeline_single_seed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend(result)\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMacro F1: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro_f1\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 39\u001b[0m, in \u001b[0;36mrun_pipeline_single_seed\u001b[1;34m(train_df, test_df, seed, verbose)\u001b[0m\n\u001b[0;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m build_lstm_model(input_shape\u001b[38;5;241m=\u001b[39m(max_seq_length, \u001b[38;5;241m23\u001b[39m), num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(label_encoder\u001b[38;5;241m.\u001b[39mclasses_))\n\u001b[0;32m     34\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     35\u001b[0m     EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m),\n\u001b[0;32m     36\u001b[0m     ReduceLROnPlateau(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     37\u001b[0m ]\n\u001b[1;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_encoded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test_padded\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_encoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_weight_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     45\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# 5. Evaluate (Predict on every frame generated by the sliding window)\u001b[39;00m\n\u001b[0;32m     48\u001b[0m y_pred_probs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test_padded, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:423\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_eval_epoch_iterator\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_eval_epoch_iterator \u001b[38;5;241m=\u001b[39m TFEpochIterator(\n\u001b[0;32m    414\u001b[0m         x\u001b[38;5;241m=\u001b[39mval_x,\n\u001b[0;32m    415\u001b[0m         y\u001b[38;5;241m=\u001b[39mval_y,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    421\u001b[0m         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    422\u001b[0m     )\n\u001b[1;32m--> 423\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_y\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_batch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m    \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_use_cached_eval_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    433\u001b[0m val_logs \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    434\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: val \u001b[38;5;28;01mfor\u001b[39;00m name, val \u001b[38;5;129;01min\u001b[39;00m val_logs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m    435\u001b[0m }\n\u001b[0;32m    436\u001b[0m epoch_logs\u001b[38;5;241m.\u001b[39mupdate(val_logs)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:511\u001b[0m, in \u001b[0;36mTensorFlowTrainer.evaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, return_dict, **kwargs)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[0;32m    510\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_begin(begin_step)\n\u001b[1;32m--> 511\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtest_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    512\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_test_batch_end(end_step, logs)\n\u001b[0;32m    513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstop_evaluating:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:242\u001b[0m, in \u001b[0;36mTensorFlowTrainer._make_function.<locals>.function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[0;32m    239\u001b[0m     iterator, (tf\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mIterator, tf\u001b[38;5;241m.\u001b[39mdistribute\u001b[38;5;241m.\u001b[39mDistributedIterator)\n\u001b[0;32m    240\u001b[0m ):\n\u001b[0;32m    241\u001b[0m     opt_outputs \u001b[38;5;241m=\u001b[39m multi_step_on_iterator(iterator)\n\u001b[1;32m--> 242\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mopt_outputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhas_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[0;32m    244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m opt_outputs\u001b[38;5;241m.\u001b[39mget_value()\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\data\\ops\\optional_ops.py:176\u001b[0m, in \u001b[0;36m_OptionalImpl.has_value\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhas_value\u001b[39m(\u001b[38;5;28mself\u001b[39m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mcolocate_with(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variant_tensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgen_optional_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptional_has_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variant_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\tensorflow\\python\\ops\\gen_optional_ops.py:172\u001b[0m, in \u001b[0;36moptional_has_value\u001b[1;34m(optional, name)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[0;32m    171\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 172\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mOptionalHasValue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptional\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[0;32m    175\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run 10 seeds for each of 4 folds\n",
    "seeds = [42, 123, 456, 789, 2024, 3141, 5926, 8888, 1337, 9999]\n",
    "folds = {\n",
    "    1: (train_df_1, test_df_1),\n",
    "    2: (train_df_2, test_df_2),\n",
    "    3: (train_df_3, test_df_3),\n",
    "    4: (train_df_4, test_df_4)\n",
    "}\n",
    "\n",
    "all_fold_results = {}\n",
    "\n",
    "for fold_num, (train_df, test_df) in folds.items():\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"PROCESSING FOLD {fold_num}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    fold_results = []\n",
    "    \n",
    "    for seed in seeds:\n",
    "        print(f\"  Running seed {seed}...\", end=\" \")\n",
    "        result = run_pipeline_single_seed(train_df, test_df, seed, verbose=False)\n",
    "        fold_results.append(result)\n",
    "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
    "    \n",
    "    all_fold_results[fold_num] = fold_results\n",
    "    \n",
    "    # Calculate fold statistics\n",
    "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
    "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
    "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
    "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL FOLDS COMPLETED!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82693ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to text file\n",
    "with open('4fold_10seed_results.txt', 'w') as f:\n",
    "    f.write(\"=\"*80 + \"\\n\")\n",
    "    f.write(\"4-FOLD CROSS-VALIDATION WITH 10 SEEDS PER FOLD\\n\")\n",
    "    f.write(\"=\"*80 + \"\\n\\n\")\n",
    "    \n",
    "    # Overall summary\n",
    "    all_macro_f1 = []\n",
    "    for fold_num in [1, 2, 3, 4]:\n",
    "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
    "        all_macro_f1.extend(fold_scores)\n",
    "    \n",
    "    f.write(\"OVERALL RESULTS (40 runs total):\\n\")\n",
    "    f.write(\"-\"*80 + \"\\n\")\n",
    "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\\n\")\n",
    "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
    "    \n",
    "    # Per-fold results\n",
    "    for fold_num in [1, 2, 3, 4]:\n",
    "        f.write(f\"\\n{'='*80}\\n\")\n",
    "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
    "        f.write(f\"{'='*80}\\n\\n\")\n",
    "        \n",
    "        fold_results = all_fold_results[fold_num]\n",
    "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
    "        \n",
    "        f.write(f\"Macro F1 Scores (10 seeds):\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        for i, result in enumerate(fold_results):\n",
    "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
    "        \n",
    "        f.write(f\"\\nStatistics:\\n\")\n",
    "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\\n\")\n",
    "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
    "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
    "        \n",
    "        # Per-class F1 (averaged across 10 seeds)\n",
    "        f.write(f\"\\nPer-Class F1 Scores (averaged across 10 seeds):\\n\")\n",
    "        f.write(\"-\"*80 + \"\\n\")\n",
    "        \n",
    "        # Collect all class names\n",
    "        all_classes = set()\n",
    "        for result in fold_results:\n",
    "            all_classes.update(result['per_class_f1'].keys())\n",
    "        \n",
    "        # Average per-class F1 across seeds\n",
    "        for class_name in sorted(all_classes):\n",
    "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
    "            mean_f1 = np.mean(class_f1_scores)\n",
    "            std_f1 = np.std(class_f1_scores)\n",
    "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} ± {std_f1:.4f}\\n\")\n",
    "\n",
    "print(\"✅ Results saved to 4fold_10seed_results.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e1102e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY - 4 FOLDS × 10 SEEDS = 40 TOTAL RUNS\")\n",
    "print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "for fold_num in [1, 2, 3, 4]:\n",
    "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
    "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} ± {np.std(macro_f1_scores):.4f}\")\n",
    "\n",
    "all_macro_f1 = []\n",
    "for fold_num in [1, 2, 3, 4]:\n",
    "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} ± {np.std(all_macro_f1):.4f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
