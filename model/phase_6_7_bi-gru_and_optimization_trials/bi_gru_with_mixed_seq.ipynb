{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCteGDqVMHm1",
        "outputId": "f422ad5e-00a0-4b2a-a8cb-ecdb16510112"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")"
      ],
      "metadata": {
        "id": "1g2zbYa7z3B-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "53076bfa-8f4a-45d0-e191-e0097aaf69a4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All imports successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"âœ“ All folds loaded\")"
      ],
      "metadata": {
        "id": "e_L4-lEf0AOf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c44c9e14-21be-4e53-8bf5-e1eef6e77b3e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All folds loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"\n",
        "    Enhanced version that includes temporal features.\n",
        "    Aggregates readings into 1s vectors with:\n",
        "    - 23 beacon percentage features\n",
        "    - 1 time_delta feature (seconds since last reading)\n",
        "    - 1 time_of_day feature (normalized hour)\n",
        "    Total: 25 features per timestamp\n",
        "    \"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns\n",
        "\n",
        "    # Ensure chronological order\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['dt'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    prev_timestamp = None\n",
        "\n",
        "    for timestamp, group in df.groupby('timestamp', sort=False):\n",
        "        # Calculate time delta\n",
        "        current_timestamp = group['dt'].iloc[0]\n",
        "        if prev_timestamp is not None:\n",
        "            time_delta = (current_timestamp - prev_timestamp).total_seconds()\n",
        "            # Clip to reasonable range (max 60 seconds)\n",
        "            time_delta = min(time_delta, 60.0)\n",
        "        else:\n",
        "            time_delta = 1.0  # Default for first reading\n",
        "\n",
        "        prev_timestamp = current_timestamp\n",
        "\n",
        "        # Original beacon features\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        percentage_vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                percentage_vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        # NEW: Add temporal features\n",
        "        time_delta_normalized = min(time_delta / 10.0, 1.0)  # Normalize: 10s = 1.0\n",
        "        time_of_day = current_timestamp.hour / 24.0  # Hour of day [0, 1]\n",
        "\n",
        "        # Combine: beacon features (23) + temporal features (2) = 25 total\n",
        "        combined_vector = percentage_vector + [time_delta_normalized, time_of_day]\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': combined_vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "print(\"âœ… Beacon vector creation updated with temporal features (25 features)\")\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"\n",
        "    Creates TWO types of training sequences:\n",
        "    1. Pure sequences (original approach): all same room\n",
        "    2. Transition sequences (NEW): includes room changes with temporal gaps\n",
        "\n",
        "    This helps the model learn to handle boundaries during inference.\n",
        "    \"\"\"\n",
        "    pure_sequences = []\n",
        "    pure_labels = []\n",
        "\n",
        "    transition_sequences = []\n",
        "    transition_labels = []\n",
        "\n",
        "    # Part 1: Create pure sequences (original)\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        pure_sequences.append(sequence)\n",
        "        pure_labels.append(room)\n",
        "\n",
        "    # Part 2: Create transition sequences (NEW!)\n",
        "    # Collect all room groups with metadata\n",
        "    all_groups = []\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=True)\n",
        "        if len(group) >= min_length:\n",
        "            all_groups.append({\n",
        "                'room': room,\n",
        "                'group': group,\n",
        "                'start_time': pd.to_datetime(group['timestamp'].iloc[0]),\n",
        "                'end_time': pd.to_datetime(group['timestamp'].iloc[-1])\n",
        "            })\n",
        "\n",
        "    # Sort by time\n",
        "    all_groups.sort(key=lambda x: x['start_time'])\n",
        "\n",
        "    # Create transition sequences between consecutive room visits\n",
        "    for i in range(len(all_groups) - 1):\n",
        "        current = all_groups[i]\n",
        "        next_room = all_groups[i + 1]\n",
        "\n",
        "        # Skip if same room (shouldn't happen, but safety check)\n",
        "        if current['room'] == next_room['room']:\n",
        "            continue\n",
        "\n",
        "        # Check temporal proximity (within 30 seconds)\n",
        "        time_gap = (next_room['start_time'] - current['end_time']).total_seconds()\n",
        "\n",
        "        if time_gap < 30:  # Only create transition if rooms are temporally close\n",
        "            # Take last 5-7 timesteps of current room\n",
        "            tail_size = min(7, len(current['group']))\n",
        "            current_tail = current['group'].tail(tail_size)\n",
        "\n",
        "            # Take first 3-5 timesteps of next room\n",
        "            head_size = min(5, len(next_room['group']))\n",
        "            next_head = next_room['group'].head(head_size)\n",
        "\n",
        "            # Combine them\n",
        "            transition_data = pd.concat([current_tail, next_head])\n",
        "\n",
        "            # Ensure we don't exceed max_length\n",
        "            if len(transition_data) > max_length:\n",
        "                transition_data = transition_data.tail(max_length)\n",
        "\n",
        "            # Only add if we have enough data\n",
        "            if len(transition_data) >= min_length:\n",
        "                transition_seq = [row['beacon_vector'] for _, row in transition_data.iterrows()]\n",
        "\n",
        "                # Label is the room we're transitioning TO (destination)\n",
        "                transition_sequences.append(transition_seq)\n",
        "                transition_labels.append(next_room['room'])\n",
        "\n",
        "    # Combine pure and transition sequences\n",
        "    all_sequences = pure_sequences + transition_sequences\n",
        "    all_labels = pure_labels + transition_labels\n",
        "\n",
        "    print(f\"ðŸ“Š Training data created:\")\n",
        "    print(f\"   Pure sequences:       {len(pure_sequences):5d} ({100*len(pure_sequences)/len(all_sequences):5.1f}%)\")\n",
        "    print(f\"   Transition sequences: {len(transition_sequences):5d} ({100*len(transition_sequences)/len(all_sequences):5.1f}%)\")\n",
        "    print(f\"   Total:                {len(all_sequences):5d}\")\n",
        "\n",
        "    return all_sequences, all_labels\n",
        "\n",
        "print(\"âœ… Sequence creation updated with transition sequences\")\n",
        "\n",
        "def create_sliding_windows_by_day(vector_df, window_size=10):\n",
        "    \"\"\"Used for Inference: Creates a sequence for every frame, respecting day boundaries.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "        if len(day_group) >= window_size:\n",
        "            vectors = list(day_group['beacon_vector'])\n",
        "            rooms = list(day_group['room'])\n",
        "\n",
        "            for i in range(len(vectors) - window_size + 1):\n",
        "                window = vectors[i : i + window_size]\n",
        "                sequences.append(window)\n",
        "                # Goal: Predict the room at the final timestamp of the window\n",
        "                labels.append(rooms[i + window_size - 1])\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def build_bidirectional_gru_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU Architecture\n",
        "    NOTE: input_shape is now (max_seq_length, 25) instead of (max_seq_length, 23)\n",
        "    Features: 23 beacons + time_delta + time_of_day = 25 total\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"âœ… Bidirectional GRU model function defined (supports 25 features)\")"
      ],
      "metadata": {
        "id": "3-iQL_Go0AX-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ecb8526-f1d6-4b6e-e6d5-9e0481f97bf5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Beacon vector creation updated with temporal features (25 features)\n",
            "âœ… Sequence creation updated with transition sequences\n",
            "âœ… Bidirectional GRU model function defined (supports 25 features)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_pipeline_single_seed(train_df, test_df, seed, verbose=False):\n",
        "    \"\"\"\n",
        "    Run realistic pipeline with Temporal Features + Transition Training + Temporal Voting.\n",
        "    1. Resets memory/seeds.\n",
        "    2. Trains on pure sequences + transition sequences.\n",
        "    3. Infers with sliding window (now with temporal features).\n",
        "    4. Smooths predictions with a Majority Vote filter.\n",
        "    \"\"\"\n",
        "    # 0. PREVENT MEMORY LEAKS & ENSURE SEEDING\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "\n",
        "    # HYPERPARAMETERS\n",
        "    window_size = 10     # Your sliding window size\n",
        "    vote_window = 5      # The smoothing neighborhood (5 seconds)\n",
        "    max_seq_length = 50\n",
        "\n",
        "    # 1. Preprocessing\n",
        "    train_df = create_room_groups(train_df)\n",
        "    train_vectors = create_beacon_count_vectors(train_df)  # Now returns 25 features\n",
        "    test_vectors = create_beacon_count_vectors(test_df)    # Now returns 25 features\n",
        "\n",
        "    # 2. Sequence Creation (now includes transition sequences!)\n",
        "    X_train, y_train = create_sequences_from_groups(train_vectors, max_length=max_seq_length)\n",
        "    X_test, y_test = create_sliding_windows_by_day(test_vectors, window_size=window_size)\n",
        "\n",
        "    # 3. Encoding & Padding\n",
        "    label_encoder = LabelEncoder()\n",
        "    label_encoder.fit(list(y_train) + list(y_test))\n",
        "    y_train_encoded = label_encoder.transform(y_train)\n",
        "    y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "    X_train_padded = pad_sequences(X_train, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "    X_test_padded = pad_sequences(X_test, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    # 4. Train Model with Macro F1 Optimization\n",
        "    class_weights = compute_class_weight('balanced', classes=np.unique(y_train_encoded), y=y_train_encoded)\n",
        "    class_weight_dict = dict(enumerate(class_weights))\n",
        "\n",
        "    # UPDATED: Now using 25 features instead of 23\n",
        "    model = build_bidirectional_gru_model(input_shape=(max_seq_length, 25), num_classes=len(label_encoder.classes_))\n",
        "\n",
        "    callbacks = [\n",
        "        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0),\n",
        "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=0)\n",
        "    ]\n",
        "\n",
        "    model.fit(\n",
        "        X_train_padded, y_train_encoded,\n",
        "        validation_data=(X_test_padded, y_test_encoded),\n",
        "        epochs=100, batch_size=32,\n",
        "        class_weight=class_weight_dict,\n",
        "        callbacks=callbacks, verbose=0\n",
        "    )\n",
        "\n",
        "    # 5. INFERENCE\n",
        "    y_pred_probs = model.predict(X_test_padded, verbose=0)\n",
        "    y_pred_raw_encoded = np.argmax(y_pred_probs, axis=1)\n",
        "\n",
        "    # 6. TEMPORAL VOTING (Smoothing)\n",
        "    def apply_temporal_voting(preds, v_window):\n",
        "        \"\"\"Applies a majority vote filter to smooth room predictions.\"\"\"\n",
        "        smoothed = []\n",
        "        for i in range(len(preds)):\n",
        "            start = max(0, i - v_window // 2)\n",
        "            end = min(len(preds), i + v_window // 2 + 1)\n",
        "            neighborhood = preds[start:end]\n",
        "            smoothed.append(np.bincount(neighborhood).argmax())\n",
        "        return np.array(smoothed)\n",
        "\n",
        "    y_pred_voted_encoded = apply_temporal_voting(y_pred_raw_encoded, vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "\n",
        "    # 7. Final Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)}\n",
        "    }\n",
        "\n",
        "print(\"âœ“ Pipeline updated: Temporal Features (25-dim) + Transition Sequences + Voting\")"
      ],
      "metadata": {
        "id": "AKeSNENp0AdC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e9434e-6ef9-47fb-c2bb-3d08143fba85"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ Pipeline updated: Temporal Features (25-dim) + Transition Sequences + Voting\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g639-oCB_51l",
        "outputId": "76b010b5-b5c1-48bb-eb5c-afa2c6486cf2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Run 10 seeds for each of 4 folds\n",
        "seeds = [42, 123, 456, 789, 2024, 3141, 5926, 8888, 1337, 9999]\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"  Running seed {seed}...\", end=\" \")\n",
        "        result = run_pipeline_single_seed(train_df, test_df, seed, verbose=False)\n",
        "        fold_results.append(result)\n",
        "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "\n",
        "    all_fold_results[fold_num] = fold_results\n",
        "\n",
        "    # Calculate fold statistics\n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
        "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "id": "UwtmIkdc0AkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f14ca7e1-f6dd-448b-b93e-25ea2cdb9f2c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 1\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.4283\n",
            "  Running seed 123... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.4443\n",
            "  Running seed 456... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.3391\n",
            "  Running seed 789... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.4538\n",
            "  Running seed 2024... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.4406\n",
            "  Running seed 3141... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.3472\n",
            "  Running seed 5926... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.3954\n",
            "  Running seed 8888... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.3796\n",
            "  Running seed 1337... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.4246\n",
            "  Running seed 9999... ðŸ“Š Training data created:\n",
            "   Pure sequences:         204 ( 57.8%)\n",
            "   Transition sequences:   149 ( 42.2%)\n",
            "   Total:                  353\n",
            "Macro F1: 0.4621\n",
            "\n",
            "  Fold 1 Summary:\n",
            "    Mean Macro F1: 0.4115 Â± 0.0417\n",
            "    Min: 0.3391, Max: 0.4621\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 2\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3468\n",
            "  Running seed 123... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3606\n",
            "  Running seed 456... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3612\n",
            "  Running seed 789... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3553\n",
            "  Running seed 2024... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3410\n",
            "  Running seed 3141... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3117\n",
            "  Running seed 5926... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3198\n",
            "  Running seed 8888... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3620\n",
            "  Running seed 1337... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3081\n",
            "  Running seed 9999... ðŸ“Š Training data created:\n",
            "   Pure sequences:         214 ( 54.9%)\n",
            "   Transition sequences:   176 ( 45.1%)\n",
            "   Total:                  390\n",
            "Macro F1: 0.3405\n",
            "\n",
            "  Fold 2 Summary:\n",
            "    Mean Macro F1: 0.3407 Â± 0.0196\n",
            "    Min: 0.3081, Max: 0.3620\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 3\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.2672\n",
            "  Running seed 123... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3565\n",
            "  Running seed 456... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.4188\n",
            "  Running seed 789... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3428\n",
            "  Running seed 2024... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3830\n",
            "  Running seed 3141... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3613\n",
            "  Running seed 5926... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3356\n",
            "  Running seed 8888... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3825\n",
            "  Running seed 1337... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3921\n",
            "  Running seed 9999... ðŸ“Š Training data created:\n",
            "   Pure sequences:         225 ( 56.7%)\n",
            "   Transition sequences:   172 ( 43.3%)\n",
            "   Total:                  397\n",
            "Macro F1: 0.3759\n",
            "\n",
            "  Fold 3 Summary:\n",
            "    Mean Macro F1: 0.3616 Â± 0.0391\n",
            "    Min: 0.2672, Max: 0.4188\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 4\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.3331\n",
            "  Running seed 123... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.3324\n",
            "  Running seed 456... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4152\n",
            "  Running seed 789... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.3401\n",
            "  Running seed 2024... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4428\n",
            "  Running seed 3141... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4315\n",
            "  Running seed 5926... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4138\n",
            "  Running seed 8888... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4209\n",
            "  Running seed 1337... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4089\n",
            "  Running seed 9999... ðŸ“Š Training data created:\n",
            "   Pure sequences:         172 ( 61.2%)\n",
            "   Transition sequences:   109 ( 38.8%)\n",
            "   Total:                  281\n",
            "Macro F1: 0.4227\n",
            "\n",
            "  Fold 4 Summary:\n",
            "    Mean Macro F1: 0.3961 Â± 0.0409\n",
            "    Min: 0.3324, Max: 0.4428\n",
            "\n",
            "================================================================================\n",
            "ALL FOLDS COMPLETED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to text file\n",
        "with open('4fold_10seed_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"4-FOLD CROSS-VALIDATION WITH 10 SEEDS PER FOLD\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    # Overall summary\n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "\n",
        "    f.write(\"OVERALL RESULTS (40 runs total):\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} Â± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
        "\n",
        "    # Per-fold results\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\n{'='*80}\\n\")\n",
        "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "\n",
        "        f.write(f\"Macro F1 Scores (10 seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        for i, result in enumerate(fold_results):\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
        "\n",
        "        f.write(f\"\\nStatistics:\\n\")\n",
        "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
        "\n",
        "        # Per-class F1 (averaged across 10 seeds)\n",
        "        f.write(f\"\\nPer-Class F1 Scores (averaged across 10 seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Collect all class names\n",
        "        all_classes = set()\n",
        "        for result in fold_results:\n",
        "            all_classes.update(result['per_class_f1'].keys())\n",
        "\n",
        "        # Average per-class F1 across seeds\n",
        "        for class_name in sorted(all_classes):\n",
        "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
        "            mean_f1 = np.mean(class_f1_scores)\n",
        "            std_f1 = np.std(class_f1_scores)\n",
        "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} Â± {std_f1:.4f}\\n\")\n",
        "\n",
        "print(\"âœ… Results saved to 4fold_10seed_results.txt\")"
      ],
      "metadata": {
        "id": "qW1ONWux0Aqi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7041ea1-9d4f-432c-8e60-6443b70128df"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Results saved to 4fold_10seed_results.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SUMMARY - 4 FOLDS Ã— 10 SEEDS = 40 TOTAL RUNS\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "all_macro_f1 = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} Â± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"{'='*80}\")"
      ],
      "metadata": {
        "id": "DvBBNZMT0JNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f8b371a-b045-456b-ac9e-edcc197d2378"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SUMMARY - 4 FOLDS Ã— 10 SEEDS = 40 TOTAL RUNS\n",
            "================================================================================\n",
            "\n",
            "Fold 1: 0.4115 Â± 0.0417\n",
            "Fold 2: 0.3407 Â± 0.0196\n",
            "Fold 3: 0.3616 Â± 0.0391\n",
            "Fold 4: 0.3961 Â± 0.0409\n",
            "\n",
            "================================================================================\n",
            "Overall Mean: 0.3775 Â± 0.0459\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}