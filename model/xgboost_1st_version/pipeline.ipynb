{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10caa7d9",
   "metadata": {},
   "source": [
    "# Firstly load the train and test data and take a final look before making the moves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b254e709",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"../../cleaned_dataset/split_data/model_selection/train.csv\")\n",
    "test_df = pd.read_csv(\"../../cleaned_dataset/split_data/model_selection/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8aaac2a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>mac address</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-10 14:21:46+09:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-93</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-10 14:21:46+09:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-93</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-10 14:21:46+09:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-93</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-10 14:21:46+09:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-93</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-10 14:21:46+09:00</td>\n",
       "      <td>6</td>\n",
       "      <td>-93</td>\n",
       "      <td>kitchen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  mac address  RSSI     room\n",
       "0  2023-04-10 14:21:46+09:00            6   -93  kitchen\n",
       "1  2023-04-10 14:21:46+09:00            6   -93  kitchen\n",
       "2  2023-04-10 14:21:46+09:00            6   -93  kitchen\n",
       "3  2023-04-10 14:21:46+09:00            6   -93  kitchen\n",
       "4  2023-04-10 14:21:46+09:00            6   -93  kitchen"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b29e6686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>mac address</th>\n",
       "      <th>RSSI</th>\n",
       "      <th>room</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023-04-12 10:00:53+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-92</td>\n",
       "      <td>cafeteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023-04-12 10:00:53+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-96</td>\n",
       "      <td>cafeteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023-04-12 10:00:53+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-96</td>\n",
       "      <td>cafeteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023-04-12 10:00:53+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-96</td>\n",
       "      <td>cafeteria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023-04-12 10:00:53+09:00</td>\n",
       "      <td>4</td>\n",
       "      <td>-83</td>\n",
       "      <td>cafeteria</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   timestamp  mac address  RSSI       room\n",
       "0  2023-04-12 10:00:53+09:00            4   -92  cafeteria\n",
       "1  2023-04-12 10:00:53+09:00            4   -96  cafeteria\n",
       "2  2023-04-12 10:00:53+09:00            4   -96  cafeteria\n",
       "3  2023-04-12 10:00:53+09:00            4   -96  cafeteria\n",
       "4  2023-04-12 10:00:53+09:00            4   -83  cafeteria"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dfae1",
   "metadata": {},
   "source": [
    "# Starting the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d6456e",
   "metadata": {},
   "source": [
    "### Step 1: Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa9d63d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for training set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_21172\\2309261335.py:20: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  df['window'] = df['timestamp'].dt.floor(window_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 923954\n",
      "Aggregated windows: 15092\n",
      "Features per window: 75\n",
      "\n",
      "Engineering features for test set...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_21172\\2309261335.py:20: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  df['window'] = df['timestamp'].dt.floor(window_size)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original records: 145030\n",
      "Aggregated windows: 5976\n",
      "Features per window: 75\n",
      "\n",
      "Feature engineering complete!\n",
      "Train set shape: (15092, 77)\n",
      "Test set shape: (5976, 77)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "def engineer_features(df, window_size='1S'):\n",
    "    \"\"\"\n",
    "    Engineer features from BLE data using time windows.\n",
    "    \n",
    "    Parameters:\n",
    "    - df: DataFrame with columns ['timestamp', 'mac address', 'RSSI', 'room']\n",
    "    - window_size: Time window for aggregation (default '1S' for 1 second)\n",
    "    \n",
    "    Returns:\n",
    "    - Aggregated DataFrame with 75 features (25 beacons Ã— 3 stats)\n",
    "    \"\"\"\n",
    "    # Convert timestamp to datetime\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    \n",
    "    # Create window identifier\n",
    "    df['window'] = df['timestamp'].dt.floor(window_size)\n",
    "    \n",
    "    # Initialize feature list\n",
    "    features_list = []\n",
    "    \n",
    "    # Get unique windows\n",
    "    windows = df['window'].unique()\n",
    "    \n",
    "    for window in windows:\n",
    "        window_data = df[df['window'] == window]\n",
    "        \n",
    "        # Get the room label for this window (should be consistent within window)\n",
    "        room_label = window_data['room'].mode()[0] if len(window_data['room'].mode()) > 0 else window_data['room'].iloc[0]\n",
    "        \n",
    "        # Initialize feature dict for this window\n",
    "        features = {'window': window, 'room': room_label}\n",
    "        \n",
    "        # For each of 25 beacons, compute statistics\n",
    "        for beacon_id in range(1, 26):\n",
    "            beacon_data = window_data[window_data['mac address'] == beacon_id]\n",
    "            \n",
    "            if len(beacon_data) > 0:\n",
    "                # Mean RSSI\n",
    "                features[f'beacon_{beacon_id}_mean'] = beacon_data['RSSI'].mean()\n",
    "                # Standard deviation\n",
    "                features[f'beacon_{beacon_id}_std'] = beacon_data['RSSI'].std() if len(beacon_data) > 1 else 0\n",
    "                # Count\n",
    "                features[f'beacon_{beacon_id}_count'] = len(beacon_data)\n",
    "            else:\n",
    "                # Beacon not detected in this window\n",
    "                features[f'beacon_{beacon_id}_mean'] = 0\n",
    "                features[f'beacon_{beacon_id}_std'] = 0\n",
    "                features[f'beacon_{beacon_id}_count'] = 0\n",
    "        \n",
    "        features_list.append(features)\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    features_df = pd.DataFrame(features_list)\n",
    "    \n",
    "    print(f\"Original records: {len(df)}\")\n",
    "    print(f\"Aggregated windows: {len(features_df)}\")\n",
    "    print(f\"Features per window: {len(features_df.columns) - 2}\")  # Exclude 'window' and 'room'\n",
    "    \n",
    "    return features_df\n",
    "\n",
    "# Engineer features for train and test\n",
    "print(\"Engineering features for training set...\")\n",
    "train_features = engineer_features(train_df, window_size='1S')\n",
    "\n",
    "print(\"\\nEngineering features for test set...\")\n",
    "test_features = engineer_features(test_df, window_size='1S')\n",
    "\n",
    "print(\"\\nFeature engineering complete!\")\n",
    "print(f\"Train set shape: {train_features.shape}\")\n",
    "print(f\"Test set shape: {test_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d61a40c",
   "metadata": {},
   "source": [
    "### Step 2: Train XGBoost Model (with unseen class handling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8d98bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "CLASS DISTRIBUTION ANALYSIS\n",
      "============================================================\n",
      "Classes in train: 18\n",
      "Classes in test: 19\n",
      "Unseen classes in test: 3\n",
      "Unseen class names: ['503', '505', '515']\n",
      "============================================================\n",
      "\n",
      "Original test samples: 5976\n",
      "Filtered test samples (seen classes only): 5889\n",
      "Removed samples: 87\n",
      "\n",
      "Number of classes in model: 18\n",
      "Training samples: 15092\n",
      "\n",
      "Training XGBoost model...\n",
      "Training complete!\n",
      "Model, label encoder, and metadata saved!\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Prepare features and labels\n",
    "X_train = train_features.drop(columns=['window', 'room'])\n",
    "y_train = train_features['room']\n",
    "\n",
    "X_test = test_features.drop(columns=['window', 'room'])\n",
    "y_test = test_features['room']\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "\n",
    "# Find classes in train vs test\n",
    "train_classes = set(label_encoder.classes_)\n",
    "test_classes = set(y_test.unique())\n",
    "unseen_classes = test_classes - train_classes\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"CLASS DISTRIBUTION ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Classes in train: {len(train_classes)}\")\n",
    "print(f\"Classes in test: {len(test_classes)}\")\n",
    "print(f\"Unseen classes in test: {len(unseen_classes)}\")\n",
    "if unseen_classes:\n",
    "    print(f\"Unseen class names: {sorted(unseen_classes)}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Filter test set to only include seen classes\n",
    "test_mask = y_test.isin(train_classes)\n",
    "X_test_filtered = X_test[test_mask].copy()\n",
    "y_test_filtered = y_test[test_mask].copy()\n",
    "test_features_filtered = test_features[test_mask].copy()\n",
    "\n",
    "print(f\"\\nOriginal test samples: {len(y_test)}\")\n",
    "print(f\"Filtered test samples (seen classes only): {len(y_test_filtered)}\")\n",
    "print(f\"Removed samples: {len(y_test) - len(y_test_filtered)}\")\n",
    "\n",
    "# Encode filtered test labels\n",
    "y_test_encoded = label_encoder.transform(y_test_filtered)\n",
    "\n",
    "# Calculate sample weights for class balancing\n",
    "class_counts = Counter(y_train_encoded)\n",
    "total = len(y_train_encoded)\n",
    "n_classes = len(label_encoder.classes_)\n",
    "sample_weights = np.array([total / (n_classes * class_counts[y]) for y in y_train_encoded])\n",
    "\n",
    "print(f\"\\nNumber of classes in model: {len(label_encoder.classes_)}\")\n",
    "print(f\"Training samples: {len(X_train)}\")\n",
    "\n",
    "# Train XGBoost\n",
    "print(\"\\nTraining XGBoost model...\")\n",
    "model = XGBClassifier(\n",
    "    n_estimators=300,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=7,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric='mlogloss'\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train_encoded, sample_weight=sample_weights)\n",
    "\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save model, label encoder, and metadata\n",
    "with open('xgboost_model.pkl', 'wb') as f:\n",
    "    pickle.dump(model, f)\n",
    "with open('label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'train_classes': list(train_classes),\n",
    "    'test_classes': list(test_classes),\n",
    "    'unseen_classes': list(unseen_classes),\n",
    "    'n_train_samples': len(X_train),\n",
    "    'n_test_samples_original': len(y_test),\n",
    "    'n_test_samples_filtered': len(y_test_filtered)\n",
    "}\n",
    "with open('model_metadata.pkl', 'wb') as f:\n",
    "    pickle.dump(metadata, f)\n",
    "\n",
    "print(\"Model, label encoder, and metadata saved!\")\n",
    "\n",
    "# Store for next cells\n",
    "globals()['X_test_eval'] = X_test_filtered\n",
    "globals()['y_test_eval'] = y_test_filtered\n",
    "globals()['y_test_eval_encoded'] = y_test_encoded\n",
    "globals()['test_features_filtered'] = test_features_filtered\n",
    "globals()['unseen_classes'] = unseen_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4d30ec",
   "metadata": {},
   "source": [
    "### Step 3: Evaluate on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a2813ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "MODEL EVALUATION RESULTS\n",
      "============================================================\n",
      "\n",
      "ðŸŽ¯ Macro F1 Score: 0.2183\n",
      "\n",
      "âš ï¸  Note: Evaluation excludes 87 samples\n",
      "   from 3 unseen class(es): ['503', '505', '515']\n",
      "\n",
      "============================================================\n",
      "CLASSIFICATION REPORT\n",
      "============================================================\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "          501     0.1494    0.8667    0.2549        15\n",
      "          502     0.7143    0.0725    0.1316        69\n",
      "          506     0.0000    0.0000    0.0000        97\n",
      "          508     0.0000    0.0000    0.0000         0\n",
      "          511     0.1143    0.0870    0.0988        46\n",
      "          512     0.5278    0.2857    0.3707       133\n",
      "          513     0.7200    0.1000    0.1756       180\n",
      "          516     0.0000    0.0000    0.0000         0\n",
      "          517     0.0000    0.0000    0.0000         2\n",
      "          518     0.0000    0.0000    0.0000        13\n",
      "          520     0.5497    0.4368    0.4868       190\n",
      "          522     0.0476    0.1765    0.0750        17\n",
      "          523     0.4610    0.6455    0.5379       110\n",
      "    cafeteria     0.3371    0.4752    0.3944      1248\n",
      "     cleaning     0.4319    0.1937    0.2674       475\n",
      "      hallway     0.0767    0.0767    0.0767       326\n",
      "      kitchen     0.6302    0.4433    0.5205      1703\n",
      "nurse station     0.4733    0.6245    0.5385      1265\n",
      "\n",
      "     accuracy                         0.4228      5889\n",
      "    macro avg     0.2907    0.2491    0.2183      5889\n",
      " weighted avg     0.4645    0.4228    0.4183      5889\n",
      "\n",
      "\n",
      "============================================================\n",
      "PER-CLASS F1 SCORES\n",
      "============================================================\n",
      "506                 : 0.0000\n",
      "508                 : 0.0000\n",
      "516                 : 0.0000\n",
      "517                 : 0.0000\n",
      "518                 : 0.0000\n",
      "522                 : 0.0750\n",
      "hallway             : 0.0767\n",
      "511                 : 0.0988\n",
      "502                 : 0.1316\n",
      "513                 : 0.1756\n",
      "501                 : 0.2549\n",
      "cleaning            : 0.2674\n",
      "512                 : 0.3707\n",
      "cafeteria           : 0.3944\n",
      "520                 : 0.4868\n",
      "kitchen             : 0.5205\n",
      "523                 : 0.5379\n",
      "nurse station       : 0.5385\n",
      "\n",
      "âœ… Results saved to: model_evaluation_results.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "# Predict on filtered test set\n",
    "y_pred_encoded = model.predict(X_test_eval)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Calculate Macro F1 Score\n",
    "macro_f1 = f1_score(y_test_eval_encoded, y_pred_encoded, average='macro')\n",
    "\n",
    "# Calculate per-class F1 scores\n",
    "per_class_f1 = f1_score(y_test_eval_encoded, y_pred_encoded, average=None)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL EVALUATION RESULTS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nðŸŽ¯ Macro F1 Score: {macro_f1:.4f}\")\n",
    "print(f\"\\nâš ï¸  Note: Evaluation excludes {len(y_test) - len(y_test_eval)} samples\")\n",
    "print(f\"   from {len(unseen_classes)} unseen class(es): {sorted(unseen_classes)}\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_test_eval, y_pred, digits=4))\n",
    "\n",
    "# Per-class F1 scores\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS F1 SCORES\")\n",
    "print(\"=\"*60)\n",
    "class_f1_dict = dict(zip(label_encoder.classes_, per_class_f1))\n",
    "for room, f1 in sorted(class_f1_dict.items(), key=lambda x: x[1]):\n",
    "    print(f\"{room:20s}: {f1:.4f}\")\n",
    "\n",
    "# Save results to file\n",
    "with open('model_evaluation_results.txt', 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"MODEL EVALUATION RESULTS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"\\nMacro F1 Score: {macro_f1:.4f}\\n\")\n",
    "    f.write(f\"\\nNote: Evaluation excludes {len(y_test) - len(y_test_eval)} samples\\n\")\n",
    "    f.write(f\"from {len(unseen_classes)} unseen class(es): {sorted(unseen_classes)}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"CLASSIFICATION REPORT\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(classification_report(y_test_eval, y_pred, digits=4))\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"PER-CLASS F1 SCORES\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    for room, f1 in sorted(class_f1_dict.items(), key=lambda x: x[1]):\n",
    "        f.write(f\"{room:20s}: {f1:.4f}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"CONFUSION MATRIX\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(str(confusion_matrix(y_test_eval, y_pred)))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(\"\\nâœ… Results saved to: model_evaluation_results.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f3654c",
   "metadata": {},
   "source": [
    "### Step 4: Generate Predictions for Original Test Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0dde0f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assigning predictions to original test records...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ADMIN\\AppData\\Local\\Temp\\ipykernel_21172\\1829811279.py:7: FutureWarning: 'S' is deprecated and will be removed in a future version, please use 's' instead.\n",
      "  test_df_copy['window'] = test_df_copy['timestamp'].dt.floor('1S')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predictions assigned to 145030 TEST records\n",
      "   - 141425 records with predictions\n",
      "   - 3605 records from unseen classes\n",
      "\n",
      "âœ… Predictions saved to: test_predictions.csv\n",
      "\n",
      "Sample predictions (seen classes):\n",
      "                  timestamp       room predicted_room\n",
      "0 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "1 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "2 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "3 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "4 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "5 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "6 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "7 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "8 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "9 2023-04-12 10:00:53+09:00  cafeteria      cafeteria\n",
      "\n",
      "Sample predictions (unseen classes):\n",
      "                      timestamp room predicted_room\n",
      "53780 2023-04-12 11:05:08+09:00  515  UNKNOWN_CLASS\n",
      "53781 2023-04-12 11:05:08+09:00  515  UNKNOWN_CLASS\n",
      "53782 2023-04-12 11:05:08+09:00  515  UNKNOWN_CLASS\n",
      "53783 2023-04-12 11:05:08+09:00  515  UNKNOWN_CLASS\n",
      "53784 2023-04-12 11:05:08+09:00  515  UNKNOWN_CLASS\n"
     ]
    }
   ],
   "source": [
    "# Assign predictions to original TEST records only (frame-level)\n",
    "print(\"Assigning predictions to original test records...\")\n",
    "\n",
    "# Convert timestamp in original test_df (this is already test set only)\n",
    "test_df_copy = test_df.copy()\n",
    "test_df_copy['timestamp'] = pd.to_datetime(test_df_copy['timestamp'])\n",
    "test_df_copy['window'] = test_df_copy['timestamp'].dt.floor('1S')\n",
    "\n",
    "# Create mapping from window to predicted room (only for filtered windows)\n",
    "window_predictions = dict(zip(test_features_filtered['window'], y_pred))\n",
    "\n",
    "# Assign predictions to each test record based on its window\n",
    "test_df_copy['predicted_room'] = test_df_copy['window'].map(window_predictions)\n",
    "\n",
    "# Mark records from unseen classes\n",
    "if len(unseen_classes) > 0:\n",
    "    unseen_mask = test_df_copy['room'].isin(unseen_classes)\n",
    "    test_df_copy.loc[unseen_mask, 'predicted_room'] = 'UNKNOWN_CLASS'\n",
    "\n",
    "# Fill any remaining NaN (shouldn't happen but just in case)\n",
    "test_df_copy['predicted_room'] = test_df_copy['predicted_room'].fillna('UNKNOWN')\n",
    "\n",
    "# Save predictions (test set only)\n",
    "predictions_df = test_df_copy[['timestamp', 'room', 'predicted_room']].copy()\n",
    "predictions_df.to_csv('test_predictions.csv', index=False)\n",
    "\n",
    "print(f\"âœ… Predictions assigned to {len(predictions_df)} TEST records\")\n",
    "print(f\"   - {len(predictions_df[predictions_df['predicted_room'] != 'UNKNOWN_CLASS'])} records with predictions\")\n",
    "if len(unseen_classes) > 0:\n",
    "    print(f\"   - {len(predictions_df[predictions_df['predicted_room'] == 'UNKNOWN_CLASS'])} records from unseen classes\")\n",
    "print(f\"\\nâœ… Predictions saved to: test_predictions.csv\")\n",
    "\n",
    "# Show sample predictions\n",
    "print(\"\\nSample predictions (seen classes):\")\n",
    "print(predictions_df[predictions_df['predicted_room'] != 'UNKNOWN_CLASS'].head(10))\n",
    "\n",
    "if len(unseen_classes) > 0:\n",
    "    print(\"\\nSample predictions (unseen classes):\")\n",
    "    print(predictions_df[predictions_df['predicted_room'] == 'UNKNOWN_CLASS'].head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63eef0",
   "metadata": {},
   "source": [
    "### Step 5: Calculate performance statistics for frame - level - actual metrics here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b40f8392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FRAME-LEVEL EVALUATION (True Performance)\n",
      "============================================================\n",
      "\n",
      "Total test frames: 145030\n",
      "Frames with predictions: 141425\n",
      "Frames from unseen classes: 3605\n",
      "\n",
      "============================================================\n",
      "FRAME-LEVEL METRICS\n",
      "============================================================\n",
      "ðŸŽ¯ Macro F1 Score (Frame-level): 0.2251\n",
      "ðŸ“Š Accuracy (Frame-level): 0.4466\n",
      "\n",
      "============================================================\n",
      "FRAME-LEVEL CLASSIFICATION REPORT\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               precision    recall  f1-score   support\n",
      "\n",
      "          501     0.1106    0.7373    0.1924       628\n",
      "          502     0.2968    0.1042    0.1542      1325\n",
      "          506     0.0000    0.0000    0.0000       883\n",
      "          508     0.0000    0.0000    0.0000         0\n",
      "          511     0.2724    0.0744    0.1169      2042\n",
      "          512     0.4839    0.1783    0.2606      5635\n",
      "          513     0.8427    0.0837    0.1522     12932\n",
      "          516     0.0000    0.0000    0.0000         0\n",
      "          517     0.0000    0.0000    0.0000        42\n",
      "          518     0.0000    0.0000    0.0000       180\n",
      "          520     0.7207    0.4041    0.5178      6125\n",
      "          522     0.0469    0.0883    0.0613       917\n",
      "          523     0.6178    0.7277    0.6683      8484\n",
      "    cafeteria     0.4302    0.4437    0.4368     25229\n",
      "     cleaning     0.4496    0.2165    0.2923     10817\n",
      "      hallway     0.0135    0.0590    0.0220      3612\n",
      "      kitchen     0.6278    0.5749    0.6002     28076\n",
      "nurse station     0.5322    0.6292    0.5767     34498\n",
      "\n",
      "     accuracy                         0.4466    141425\n",
      "    macro avg     0.3025    0.2401    0.2251    141425\n",
      " weighted avg     0.5381    0.4466    0.4519    141425\n",
      "\n",
      "\n",
      "============================================================\n",
      "PER-CLASS F1 SCORES (Frame-level)\n",
      "============================================================\n",
      "506                 : 0.0000\n",
      "517                 : 0.0000\n",
      "518                 : 0.0000\n",
      "hallway             : 0.0220\n",
      "522                 : 0.0613\n",
      "511                 : 0.1169\n",
      "513                 : 0.1522\n",
      "502                 : 0.1542\n",
      "501                 : 0.1924\n",
      "512                 : 0.2606\n",
      "cleaning            : 0.2923\n",
      "cafeteria           : 0.4368\n",
      "520                 : 0.5178\n",
      "nurse station       : 0.5767\n",
      "kitchen             : 0.6002\n",
      "523                 : 0.6683\n",
      "\n",
      "============================================================\n",
      "COMPARISON: Window-level vs Frame-level\n",
      "============================================================\n",
      "Window-level Macro F1: 0.2183\n",
      "Frame-level Macro F1:  0.2251\n",
      "Difference:            0.0068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Frame-level results saved to: frame_level_evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FRAME-LEVEL EVALUATION (True Performance)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Get ground truth and predictions at frame level\n",
    "y_true_frame = predictions_df['room']\n",
    "y_pred_frame = predictions_df['predicted_room']\n",
    "\n",
    "# Filter out UNKNOWN_CLASS for fair evaluation (same as window-level)\n",
    "known_mask = y_pred_frame != 'UNKNOWN_CLASS'\n",
    "y_true_frame_filtered = y_true_frame[known_mask]\n",
    "y_pred_frame_filtered = y_pred_frame[known_mask]\n",
    "\n",
    "print(f\"\\nTotal test frames: {len(predictions_df)}\")\n",
    "print(f\"Frames with predictions: {len(y_true_frame_filtered)}\")\n",
    "print(f\"Frames from unseen classes: {len(predictions_df) - len(y_true_frame_filtered)}\")\n",
    "\n",
    "# Calculate Macro F1 Score (frame-level)\n",
    "macro_f1_frame = f1_score(y_true_frame_filtered, y_pred_frame_filtered, average='macro')\n",
    "\n",
    "# Calculate Accuracy\n",
    "accuracy_frame = accuracy_score(y_true_frame_filtered, y_pred_frame_filtered)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FRAME-LEVEL METRICS\")\n",
    "print(\"=\"*60)\n",
    "print(f\"ðŸŽ¯ Macro F1 Score (Frame-level): {macro_f1_frame:.4f}\")\n",
    "print(f\"ðŸ“Š Accuracy (Frame-level): {accuracy_frame:.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"FRAME-LEVEL CLASSIFICATION REPORT\")\n",
    "print(\"=\"*60)\n",
    "print(classification_report(y_true_frame_filtered, y_pred_frame_filtered, digits=4))\n",
    "\n",
    "# Per-class F1 scores at frame level\n",
    "per_class_f1_frame = f1_score(y_true_frame_filtered, y_pred_frame_filtered, average=None, labels=sorted(y_true_frame_filtered.unique()))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PER-CLASS F1 SCORES (Frame-level)\")\n",
    "print(\"=\"*60)\n",
    "class_f1_dict_frame = dict(zip(sorted(y_true_frame_filtered.unique()), per_class_f1_frame))\n",
    "for room, f1 in sorted(class_f1_dict_frame.items(), key=lambda x: x[1]):\n",
    "    print(f\"{room:20s}: {f1:.4f}\")\n",
    "\n",
    "# Compare window-level vs frame-level\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPARISON: Window-level vs Frame-level\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Window-level Macro F1: {macro_f1:.4f}\")\n",
    "print(f\"Frame-level Macro F1:  {macro_f1_frame:.4f}\")\n",
    "print(f\"Difference:            {abs(macro_f1 - macro_f1_frame):.4f}\")\n",
    "\n",
    "# Save frame-level results\n",
    "with open('frame_level_evaluation_results.txt', 'w') as f:\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(\"FRAME-LEVEL EVALUATION RESULTS\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"\\nTotal test frames: {len(predictions_df)}\\n\")\n",
    "    f.write(f\"Frames with predictions: {len(y_true_frame_filtered)}\\n\")\n",
    "    f.write(f\"Frames from unseen classes: {len(predictions_df) - len(y_true_frame_filtered)}\\n\")\n",
    "    f.write(f\"\\nMacro F1 Score (Frame-level): {macro_f1_frame:.4f}\\n\")\n",
    "    f.write(f\"Accuracy (Frame-level): {accuracy_frame:.4f}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"FRAME-LEVEL CLASSIFICATION REPORT\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(classification_report(y_true_frame_filtered, y_pred_frame_filtered, digits=4))\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"PER-CLASS F1 SCORES (Frame-level)\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    for room, f1 in sorted(class_f1_dict_frame.items(), key=lambda x: x[1]):\n",
    "        f.write(f\"{room:20s}: {f1:.4f}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"COMPARISON: Window-level vs Frame-level\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(f\"Window-level Macro F1: {macro_f1:.4f}\\n\")\n",
    "    f.write(f\"Frame-level Macro F1:  {macro_f1_frame:.4f}\\n\")\n",
    "    f.write(f\"Difference:            {abs(macro_f1 - macro_f1_frame):.4f}\\n\")\n",
    "    f.write(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "    f.write(\"CONFUSION MATRIX (Frame-level)\\n\")\n",
    "    f.write(\"=\"*60 + \"\\n\")\n",
    "    f.write(str(confusion_matrix(y_true_frame_filtered, y_pred_frame_filtered)))\n",
    "    f.write(\"\\n\")\n",
    "\n",
    "print(\"\\nâœ… Frame-level results saved to: frame_level_evaluation_results.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
