{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2499746b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "import pandas as pd\n",
    "train_df = pd.read_csv(\"../../cleaned_dataset/split_data/model_validation/train.csv\")\n",
    "test_df = pd.read_csv(\"../../cleaned_dataset/split_data/model_validation/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb85223e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all of the unique labels for train / test sets \n",
    "train_labels = list(train_df['room'].unique())\n",
    "test_labels = list(test_df['room'].unique())\n",
    "\n",
    "# Labels that appear in BOTH train and test\n",
    "common_labels = list(set(train_labels) & set(test_labels))\n",
    "\n",
    "# Now filter the train_df and test_df so that\n",
    "# remove all record in train_df whose labels not in test labels \n",
    "# and remove all record in test_df whose labels not in train labels \n",
    "# therefore we can have a proper train and test df such that the labels appear \n",
    "# in train and test should be the same and vice versa\n",
    "train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
    "test_df  = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b5cf07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 1: Create room groups for sequences\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def create_room_groups(df):\n",
    "    \"\"\"\n",
    "    Create room_group column to identify consecutive room visits.\n",
    "    Each time room changes, we start a new group.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
    "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
    "    return df\n",
    "\n",
    "# Apply to both train and test\n",
    "train_df = create_room_groups(train_df)\n",
    "test_df = create_room_groups(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7688ab58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating beacon vectors with RSSI stats for train set...\n",
      "âœ“ Train vectors created: 19280 windows\n",
      "âœ“ Feature dimension: 69 (23 beacons Ã— 3 features)\n",
      "\n",
      "Creating beacon vectors with RSSI stats for test set...\n",
      "âœ“ Test vectors created: 2481 windows\n",
      "âœ“ Feature dimension: 69 (23 beacons Ã— 3 features)\n",
      "\n",
      "============================================================\n",
      "Example feature vector breakdown:\n",
      "============================================================\n",
      "Timestamp: 2023-04-10 14:21:46+09:00\n",
      "Room: kitchen\n",
      "\n",
      "Beacon 4 (index 3):\n",
      "  Count %: 0.6937\n",
      "  RSSI Mean: -94.03 dBm\n",
      "  RSSI Std: 2.6919 dBm\n",
      "\n",
      "Beacon 14 (index 13):\n",
      "  Count %: 0.0000\n",
      "  RSSI Mean: 0.00 dBm\n",
      "  RSSI Std: 0.0000 dBm\n"
     ]
    }
   ],
   "source": [
    "# CELL 2: Create 69-dimensional beacon vectors (count + mean + std)\n",
    "def create_beacon_vectors_with_rssi(df):\n",
    "    \"\"\"\n",
    "    For each window, create a 69-dim vector where each beacon has 3 features:\n",
    "    - count_pct: percentage of times beacon appeared\n",
    "    - rssi_mean: average RSSI value (or -100 if not detected)\n",
    "    - rssi_std: standard deviation of RSSI (or 0 if not detected)\n",
    "    \n",
    "    This captures both detection frequency AND signal strength patterns.\n",
    "    \"\"\"\n",
    "    vectors = []\n",
    "    \n",
    "    for timestamp, group in df.groupby('timestamp'):\n",
    "        # Total readings in this window\n",
    "        total_readings = len(group)\n",
    "        \n",
    "        # Initialize feature arrays for 23 beacons\n",
    "        count_features = [0.0] * 23\n",
    "        mean_features = [0.0] * 23  # Sentinel value for \"not detected\"\n",
    "        std_features = [0.0] * 23\n",
    "        \n",
    "        # Process each beacon\n",
    "        for beacon_id in range(1, 24):  # Beacons 1-23\n",
    "            beacon_data = group[group['mac address'] == beacon_id]\n",
    "            \n",
    "            if len(beacon_data) > 0:\n",
    "                # Count percentage (your original feature)\n",
    "                count_features[beacon_id - 1] = len(beacon_data) / total_readings\n",
    "                \n",
    "                # RSSI statistics (NEW features)\n",
    "                rssi_values = beacon_data['RSSI'].values\n",
    "                mean_features[beacon_id - 1] = np.mean(rssi_values)\n",
    "                std_features[beacon_id - 1] = np.std(rssi_values) if len(rssi_values) > 1 else 0.0\n",
    "        \n",
    "        # Combine all features: count (23) + mean (23) + std (23) = 69 features\n",
    "        combined_vector = count_features + mean_features + std_features\n",
    "        \n",
    "        vectors.append({\n",
    "            'timestamp': timestamp,\n",
    "            'room': group['room'].iloc[0],\n",
    "            'room_group': group['room_group'].iloc[0],\n",
    "            'beacon_vector': combined_vector\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(vectors)\n",
    "\n",
    "# Create vectors for train and test\n",
    "print(\"Creating beacon vectors with RSSI stats for train set...\")\n",
    "train_vectors = create_beacon_vectors_with_rssi(train_df)\n",
    "print(f\"âœ“ Train vectors created: {len(train_vectors)} windows\")\n",
    "print(f\"âœ“ Feature dimension: {len(train_vectors.iloc[0]['beacon_vector'])} (23 beacons Ã— 3 features)\")\n",
    "\n",
    "print(\"\\nCreating beacon vectors with RSSI stats for test set...\")\n",
    "test_vectors = create_beacon_vectors_with_rssi(test_df)\n",
    "print(f\"âœ“ Test vectors created: {len(test_vectors)} windows\")\n",
    "print(f\"âœ“ Feature dimension: {len(test_vectors.iloc[0]['beacon_vector'])} (23 beacons Ã— 3 features)\")\n",
    "\n",
    "# Show example\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Example feature vector breakdown:\")\n",
    "print(\"=\"*60)\n",
    "example_vec = train_vectors.iloc[0]['beacon_vector']\n",
    "print(f\"Timestamp: {train_vectors.iloc[0]['timestamp']}\")\n",
    "print(f\"Room: {train_vectors.iloc[0]['room']}\")\n",
    "print(f\"\\nBeacon 4 (index 3):\")\n",
    "print(f\"  Count %: {example_vec[3]:.4f}\")\n",
    "print(f\"  RSSI Mean: {example_vec[3 + 23]:.2f} dBm\")\n",
    "print(f\"  RSSI Std: {example_vec[3 + 46]:.4f} dBm\")\n",
    "print(f\"\\nBeacon 14 (index 13):\")\n",
    "print(f\"  Count %: {example_vec[13]:.4f}\")\n",
    "print(f\"  RSSI Mean: {example_vec[13 + 23]:.2f} dBm\")\n",
    "print(f\"  RSSI Std: {example_vec[13 + 46]:.4f} dBm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49b4f0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating sequences from room groups...\n",
      "\n",
      "âœ“ Train sequences: 204\n",
      "âœ“ Test sequences: 50\n",
      "\n",
      "Train sequence lengths:\n",
      "  Min: 3, Max: 50, Mean: 30.1\n",
      "\n",
      "Test sequence lengths:\n",
      "  Min: 4, Max: 50, Mean: 29.2\n",
      "\n",
      "Train label distribution:\n",
      "505               1\n",
      "506               3\n",
      "508               1\n",
      "513               2\n",
      "515               1\n",
      "517               1\n",
      "518               2\n",
      "520               5\n",
      "523              13\n",
      "cafeteria        64\n",
      "cleaning         18\n",
      "kitchen          38\n",
      "nurse station    55\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Test label distribution:\n",
      "505               1\n",
      "506               1\n",
      "508               2\n",
      "513               1\n",
      "515               1\n",
      "517               1\n",
      "518               1\n",
      "520               1\n",
      "523               1\n",
      "cafeteria        14\n",
      "cleaning          5\n",
      "kitchen          13\n",
      "nurse station     8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CELL 3: Create sequences grouped by room_group\n",
    "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
    "    \"\"\"\n",
    "    Group consecutive windows into sequences based on room_group.\n",
    "    Filter out sequences that are too short or too long.\n",
    "    \"\"\"\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    \n",
    "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
    "        group = group.sort_values('timestamp')\n",
    "        seq_length = len(group)\n",
    "        \n",
    "        # Filter by length\n",
    "        if seq_length < min_length:\n",
    "            continue  # Too short, skip\n",
    "        \n",
    "        if seq_length > max_length:\n",
    "            # Too long, truncate to last max_length windows\n",
    "            group = group.tail(max_length)\n",
    "            seq_length = max_length\n",
    "        \n",
    "        # Extract beacon vectors as sequence\n",
    "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
    "        \n",
    "        sequences.append(sequence)\n",
    "        labels.append(room)\n",
    "    \n",
    "    return sequences, labels\n",
    "\n",
    "# Create sequences\n",
    "print(\"Creating sequences from room groups...\")\n",
    "X_train_seq, y_train = create_sequences_from_groups(train_vectors, min_length=3, max_length=50)\n",
    "X_test_seq, y_test = create_sequences_from_groups(test_vectors, min_length=3, max_length=50)\n",
    "\n",
    "print(f\"\\nâœ“ Train sequences: {len(X_train_seq)}\")\n",
    "print(f\"âœ“ Test sequences: {len(X_test_seq)}\")\n",
    "\n",
    "# Show sequence length statistics\n",
    "train_lengths = [len(seq) for seq in X_train_seq]\n",
    "test_lengths = [len(seq) for seq in X_test_seq]\n",
    "\n",
    "print(f\"\\nTrain sequence lengths:\")\n",
    "print(f\"  Min: {min(train_lengths)}, Max: {max(train_lengths)}, Mean: {np.mean(train_lengths):.1f}\")\n",
    "\n",
    "print(f\"\\nTest sequence lengths:\")\n",
    "print(f\"  Min: {min(test_lengths)}, Max: {max(test_lengths)}, Mean: {np.mean(test_lengths):.1f}\")\n",
    "\n",
    "# Show label distribution\n",
    "print(f\"\\nTrain label distribution:\")\n",
    "print(pd.Series(y_train).value_counts().sort_index())\n",
    "print(f\"\\nTest label distribution:\")\n",
    "print(pd.Series(y_test).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f07192dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\export\\tf2onnx_lib.py:8: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, \"object\"):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 13\n",
      "Classes: ['505' '506' '508' '513' '515' '517' '518' '520' '523' 'cafeteria'\n",
      " 'cleaning' 'kitchen' 'nurse station']\n",
      "\n",
      "âœ“ Padded train shape: (204, 50, 69)\n",
      "  (num_sequences, max_sequence_length, num_features)\n",
      "âœ“ Padded test shape: (50, 50, 69)\n"
     ]
    }
   ],
   "source": [
    "# CELL 4: Pad sequences to same length for LSTM\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "print(f\"Number of classes: {len(label_encoder.classes_)}\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n",
    "\n",
    "# Pad sequences (pad with zeros at the beginning)\n",
    "# Convert list of sequences to numpy array\n",
    "X_train_padded = pad_sequences(X_train_seq, padding='pre', dtype='float32', value=0.0)\n",
    "X_test_padded = pad_sequences(X_test_seq, padding='pre', dtype='float32', value=0.0)\n",
    "\n",
    "print(f\"\\nâœ“ Padded train shape: {X_train_padded.shape}\")\n",
    "print(f\"  (num_sequences, max_sequence_length, num_features)\")\n",
    "print(f\"âœ“ Padded test shape: {X_test_padded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c668f38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model architecture:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ADMIN\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\masking.py:48: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Masking</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">69</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">101,376</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)        â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">429</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ masking (\u001b[38;5;33mMasking\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m69\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚       \u001b[38;5;34m101,376\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m, \u001b[38;5;34m128\u001b[0m)        â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚        \u001b[38;5;34m49,408\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚         \u001b[38;5;34m2,080\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_2 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m)             â”‚           \u001b[38;5;34m429\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,293</span> (598.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m153,293\u001b[0m (598.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">153,293</span> (598.80 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m153,293\u001b[0m (598.80 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# CELL 5: Build and compile LSTM model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "num_classes = len(label_encoder.classes_)\n",
    "y_train_onehot = to_categorical(y_train_encoded, num_classes=num_classes)\n",
    "y_test_onehot = to_categorical(y_test_encoded, num_classes=num_classes)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential([\n",
    "    # Masking layer to ignore padded zeros\n",
    "    Masking(mask_value=0.0, input_shape=(X_train_padded.shape[1], X_train_padded.shape[2])),\n",
    "    \n",
    "    # LSTM layers\n",
    "    LSTM(128, return_sequences=True),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.3),\n",
    "    \n",
    "    # Dense layers\n",
    "    Dense(32, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    # Output layer\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "print(\"Model architecture:\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37e87e75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing class distribution in training set...\n",
      "\n",
      "Class counts:\n",
      "  505: 1 sequences\n",
      "  506: 3 sequences\n",
      "  508: 1 sequences\n",
      "  513: 2 sequences\n",
      "  515: 1 sequences\n",
      "  517: 1 sequences\n",
      "  518: 2 sequences\n",
      "  520: 5 sequences\n",
      "  523: 13 sequences\n",
      "  cafeteria: 64 sequences\n",
      "  cleaning: 18 sequences\n",
      "  kitchen: 38 sequences\n",
      "  nurse station: 55 sequences\n",
      "\n",
      "Classes with >= 2 samples: 9/13\n",
      "\n",
      "Filtered training data:\n",
      "  Original: 204 sequences\n",
      "  Filtered: 200 sequences\n",
      "  Removed: 4 sequences\n",
      "\n",
      "Train split: 160 sequences\n",
      "Val split: 40 sequences\n",
      "\n",
      "Class weights (to handle imbalance):\n",
      "  506: 7.41\n",
      "  513: 11.11\n",
      "  518: 11.11\n",
      "  520: 4.44\n",
      "  523: 1.71\n",
      "  cafeteria: 0.35\n",
      "  cleaning: 1.23\n",
      "  kitchen: 0.58\n",
      "  nurse station: 0.40\n",
      "\n",
      "================================================================================\n",
      "Training LSTM model (optimizing for MACRO F1)...\n",
      "================================================================================\n",
      "Epoch 1/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.0418 - loss: 2.2836\n",
      "Epoch 1: val_macro_f1 = 0.0238\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 309ms/step - accuracy: 0.0500 - loss: 2.7361 - val_accuracy: 0.0500 - val_loss: 2.3729 - learning_rate: 0.0010\n",
      "Epoch 2/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.0829 - loss: 2.9307\n",
      "Epoch 2: val_macro_f1 = 0.0505\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.1125 - loss: 2.5181 - val_accuracy: 0.0750 - val_loss: 2.2483 - learning_rate: 0.0010\n",
      "Epoch 3/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.1629 - loss: 2.3745\n",
      "Epoch 3: val_macro_f1 = 0.0576\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step - accuracy: 0.1688 - loss: 2.3817 - val_accuracy: 0.1000 - val_loss: 2.1398 - learning_rate: 0.0010\n",
      "Epoch 4/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.2598 - loss: 2.5884\n",
      "Epoch 4: val_macro_f1 = 0.1952\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.2625 - loss: 2.2239 - val_accuracy: 0.2000 - val_loss: 2.0343 - learning_rate: 0.0010\n",
      "Epoch 5/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.2627 - loss: 2.2197\n",
      "Epoch 5: val_macro_f1 = 0.2166\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 71ms/step - accuracy: 0.2875 - loss: 2.0464 - val_accuracy: 0.2750 - val_loss: 1.9283 - learning_rate: 0.0010\n",
      "Epoch 6/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.3651 - loss: 1.6839\n",
      "Epoch 6: val_macro_f1 = 0.2269\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step - accuracy: 0.3750 - loss: 1.8428 - val_accuracy: 0.3250 - val_loss: 1.8213 - learning_rate: 0.0010\n",
      "Epoch 7/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.3499 - loss: 1.7441\n",
      "Epoch 7: val_macro_f1 = 0.2512\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step - accuracy: 0.3875 - loss: 1.7061 - val_accuracy: 0.3750 - val_loss: 1.7064 - learning_rate: 0.0010\n",
      "Epoch 8/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step - accuracy: 0.4394 - loss: 1.5499\n",
      "Epoch 8: val_macro_f1 = 0.2387\n",
      "  â†’ No improvement (patience: 1/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 96ms/step - accuracy: 0.4938 - loss: 1.6040 - val_accuracy: 0.4250 - val_loss: 1.6001 - learning_rate: 0.0010\n",
      "Epoch 9/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - accuracy: 0.5193 - loss: 1.5110\n",
      "Epoch 9: val_macro_f1 = 0.4300\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 92ms/step - accuracy: 0.4688 - loss: 1.3741 - val_accuracy: 0.4500 - val_loss: 1.5056 - learning_rate: 0.0010\n",
      "Epoch 10/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step - accuracy: 0.5266 - loss: 1.2868\n",
      "Epoch 10: val_macro_f1 = 0.5760\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.5312 - loss: 1.2956 - val_accuracy: 0.5250 - val_loss: 1.4041 - learning_rate: 0.0010\n",
      "Epoch 11/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.6108 - loss: 1.0590\n",
      "Epoch 11: val_macro_f1 = 0.5872\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.6062 - loss: 1.0919 - val_accuracy: 0.5500 - val_loss: 1.3165 - learning_rate: 0.0010\n",
      "Epoch 12/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.6138 - loss: 1.0322\n",
      "Epoch 12: val_macro_f1 = 0.5687\n",
      "  â†’ No improvement (patience: 1/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 109ms/step - accuracy: 0.6000 - loss: 0.9147 - val_accuracy: 0.5000 - val_loss: 1.2230 - learning_rate: 0.0010\n",
      "Epoch 13/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.5557 - loss: 0.8966\n",
      "Epoch 13: val_macro_f1 = 0.6224\n",
      "  â†’ New best macro F1! Saving weights.\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 106ms/step - accuracy: 0.6250 - loss: 0.9450 - val_accuracy: 0.6000 - val_loss: 1.1352 - learning_rate: 0.0010\n",
      "Epoch 14/50\n",
      "\u001b[1m4/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 52ms/step - accuracy: 0.7083 - loss: 0.7331\n",
      "Epoch 14: val_macro_f1 = 0.6041\n",
      "  â†’ No improvement (patience: 1/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.6375 - loss: 0.7846 - val_accuracy: 0.5750 - val_loss: 1.0895 - learning_rate: 0.0010\n",
      "Epoch 15/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step - accuracy: 0.6376 - loss: 0.7197\n",
      "Epoch 15: val_macro_f1 = 0.4319\n",
      "  â†’ No improvement (patience: 2/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step - accuracy: 0.7063 - loss: 0.7364 - val_accuracy: 0.5000 - val_loss: 1.0123 - learning_rate: 0.0010\n",
      "Epoch 16/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.7028 - loss: 0.5636\n",
      "Epoch 16: val_macro_f1 = 0.4268\n",
      "  â†’ No improvement (patience: 3/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.7250 - loss: 0.5307 - val_accuracy: 0.5000 - val_loss: 1.0075 - learning_rate: 0.0010\n",
      "Epoch 17/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step - accuracy: 0.6821 - loss: 0.5375\n",
      "Epoch 17: val_macro_f1 = 0.4268\n",
      "  â†’ No improvement (patience: 4/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7437 - loss: 0.5028 - val_accuracy: 0.5000 - val_loss: 0.9876 - learning_rate: 0.0010\n",
      "Epoch 18/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.7395 - loss: 0.4575\n",
      "Epoch 18: val_macro_f1 = 0.4424\n",
      "  â†’ No improvement (patience: 5/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step - accuracy: 0.7312 - loss: 0.4439 - val_accuracy: 0.5250 - val_loss: 0.9500 - learning_rate: 0.0010\n",
      "Epoch 19/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7442 - loss: 0.3747\n",
      "Epoch 19: val_macro_f1 = 0.4833\n",
      "  â†’ No improvement (patience: 6/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 91ms/step - accuracy: 0.7312 - loss: 0.3792 - val_accuracy: 0.6000 - val_loss: 0.9668 - learning_rate: 0.0010\n",
      "Epoch 20/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step - accuracy: 0.7249 - loss: 0.4834\n",
      "Epoch 20: val_macro_f1 = 0.4867\n",
      "  â†’ No improvement (patience: 7/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.7625 - loss: 0.4402 - val_accuracy: 0.6000 - val_loss: 0.9442 - learning_rate: 0.0010\n",
      "Epoch 21/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 0.8166 - loss: 0.3622\n",
      "Epoch 21: val_macro_f1 = 0.4715\n",
      "  â†’ No improvement (patience: 8/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step - accuracy: 0.8250 - loss: 0.3600 - val_accuracy: 0.5750 - val_loss: 0.9043 - learning_rate: 0.0010\n",
      "Epoch 22/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step - accuracy: 0.8291 - loss: 0.3603\n",
      "Epoch 22: val_macro_f1 = 0.4715\n",
      "  â†’ No improvement (patience: 9/10)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step - accuracy: 0.8250 - loss: 0.3698 - val_accuracy: 0.5750 - val_loss: 0.8988 - learning_rate: 0.0010\n",
      "Epoch 23/50\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step - accuracy: 0.8482 - loss: 0.3036\n",
      "Epoch 23: val_macro_f1 = 0.5938\n",
      "  â†’ No improvement (patience: 10/10)\n",
      "\n",
      "âš ï¸ Early stopping triggered! Restoring best weights (macro F1 = 0.6224)\n",
      "\u001b[1m5/5\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step - accuracy: 0.8375 - loss: 0.2874 - val_accuracy: 0.6500 - val_loss: 0.8914 - learning_rate: 0.0010\n",
      "\n",
      "âœ“ Training complete!\n",
      "âœ“ Best validation macro F1: 0.6224\n"
     ]
    }
   ],
   "source": [
    "# CELL 6: Train the model (OPTIMIZED FOR MACRO F1) - FIXED\n",
    "from tensorflow.keras.callbacks import Callback, ReduceLROnPlateau\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class MacroF1Callback(Callback):\n",
    "    \"\"\"\n",
    "    Custom callback to monitor macro F1 score on validation set\n",
    "    and implement early stopping based on macro F1\n",
    "    \"\"\"\n",
    "    def __init__(self, validation_data, patience=10):\n",
    "        super().__init__()\n",
    "        self.X_val, self.y_val = validation_data\n",
    "        self.patience = patience\n",
    "        self.best_macro_f1 = 0\n",
    "        self.wait = 0\n",
    "        self.best_weights = None\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Predict on validation set\n",
    "        y_pred_probs = self.model.predict(self.X_val, verbose=0)\n",
    "        y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
    "        y_true_encoded = np.argmax(self.y_val, axis=1)\n",
    "        \n",
    "        # Calculate macro F1\n",
    "        macro_f1 = f1_score(y_true_encoded, y_pred_encoded, average='macro', zero_division=0)\n",
    "        \n",
    "        print(f\"\\nEpoch {epoch + 1}: val_macro_f1 = {macro_f1:.4f}\")\n",
    "        \n",
    "        # Check if this is the best macro F1\n",
    "        if macro_f1 > self.best_macro_f1:\n",
    "            self.best_macro_f1 = macro_f1\n",
    "            self.wait = 0\n",
    "            self.best_weights = self.model.get_weights()\n",
    "            print(f\"  â†’ New best macro F1! Saving weights.\")\n",
    "        else:\n",
    "            self.wait += 1\n",
    "            print(f\"  â†’ No improvement (patience: {self.wait}/{self.patience})\")\n",
    "            \n",
    "            if self.wait >= self.patience:\n",
    "                print(f\"\\nâš ï¸ Early stopping triggered! Restoring best weights (macro F1 = {self.best_macro_f1:.4f})\")\n",
    "                self.model.stop_training = True\n",
    "                self.model.set_weights(self.best_weights)\n",
    "\n",
    "# Check class distribution and filter out classes with only 1 sample\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Analyzing class distribution in training set...\")\n",
    "class_counts = Counter(y_train_encoded)\n",
    "print(f\"\\nClass counts:\")\n",
    "for class_idx, count in sorted(class_counts.items()):\n",
    "    class_name = label_encoder.classes_[class_idx]\n",
    "    print(f\"  {class_name}: {count} sequences\")\n",
    "\n",
    "# Find classes with at least 2 samples (needed for train/val split)\n",
    "valid_classes = [class_idx for class_idx, count in class_counts.items() if count >= 2]\n",
    "print(f\"\\nClasses with >= 2 samples: {len(valid_classes)}/{len(class_counts)}\")\n",
    "\n",
    "# Filter data to only include valid classes\n",
    "valid_mask = np.isin(y_train_encoded, valid_classes)\n",
    "X_train_filtered = X_train_padded[valid_mask]\n",
    "y_train_filtered = y_train_onehot[valid_mask]\n",
    "y_train_encoded_filtered = y_train_encoded[valid_mask]\n",
    "\n",
    "print(f\"\\nFiltered training data:\")\n",
    "print(f\"  Original: {len(X_train_padded)} sequences\")\n",
    "print(f\"  Filtered: {len(X_train_filtered)} sequences\")\n",
    "print(f\"  Removed: {len(X_train_padded) - len(X_train_filtered)} sequences\")\n",
    "\n",
    "# Split train data for validation (20%) - NO stratification\n",
    "X_train_split, X_val_split, y_train_split, y_val_split = train_test_split(\n",
    "    X_train_filtered, y_train_filtered, \n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    "    # No stratify - some classes still too small\n",
    ")\n",
    "\n",
    "print(f\"\\nTrain split: {len(X_train_split)} sequences\")\n",
    "print(f\"Val split: {len(X_val_split)} sequences\")\n",
    "\n",
    "# Callbacks\n",
    "macro_f1_callback = MacroF1Callback(\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    patience=10\n",
    ")\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='loss',  # Monitor training loss\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=1e-6,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Compute class weights on the filtered data\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_encoded_filtered),\n",
    "    y=y_train_encoded_filtered\n",
    ")\n",
    "class_weight_dict = {class_idx: class_weights[i] for i, class_idx in enumerate(np.unique(y_train_encoded_filtered))}\n",
    "\n",
    "print(\"\\nClass weights (to handle imbalance):\")\n",
    "for class_idx, weight in sorted(class_weight_dict.items()):\n",
    "    class_name = label_encoder.classes_[class_idx]\n",
    "    print(f\"  {class_name}: {weight:.2f}\")\n",
    "\n",
    "# Train\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Training LSTM model (optimizing for MACRO F1)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_split, y_train_split,\n",
    "    validation_data=(X_val_split, y_val_split),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    class_weight=class_weight_dict,  # Balance classes\n",
    "    callbacks=[macro_f1_callback, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Training complete!\")\n",
    "print(f\"âœ“ Best validation macro F1: {macro_f1_callback.best_macro_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65cd2be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
      "================================================================================\n",
      "LSTM MODEL EVALUATION RESULTS\n",
      "================================================================================\n",
      "\n",
      "ğŸ¯ MACRO F1 SCORE: 0.3861\n",
      "   XGBoost baseline was: 0.30\n",
      "   XGBoost + temporal features was: 0.29\n",
      "   âœ“ Improvement over baseline: +28.7%\n",
      "\n",
      "================================================================================\n",
      "PER-CLASS F1 SCORES\n",
      "================================================================================\n",
      "\n",
      "Room                 F1 Score     Support   \n",
      "--------------------------------------------------------------------------------\n",
      "cleaning             0.8000       5         \n",
      "kitchen              0.6875       13        \n",
      "506                  0.6667       1         \n",
      "518                  0.6667       1         \n",
      "520                  0.6667       1         \n",
      "nurse station        0.6316       8         \n",
      "513                  0.5000       1         \n",
      "cafeteria            0.4000       14        \n",
      "505                  0.0000       1         \n",
      "508                  0.0000       2         \n",
      "515                  0.0000       1         \n",
      "517                  0.0000       1         \n",
      "523                  0.0000       1         \n",
      "\n",
      "================================================================================\n",
      "SUMMARY STATISTICS\n",
      "================================================================================\n",
      "Average F1 (macro): 0.3861\n",
      "Best performing room: cleaning (F1=0.8000)\n",
      "Worst performing room: 523 (F1=0.0000)\n",
      "Rooms with F1 > 0.5: 6/13\n",
      "Rooms with F1 < 0.2: 5/13\n"
     ]
    }
   ],
   "source": [
    "# CELL 7: Evaluate on test set\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# Make predictions\n",
    "y_pred_probs = model.predict(X_test_padded)\n",
    "y_pred_encoded = np.argmax(y_pred_probs, axis=1)\n",
    "y_pred = label_encoder.inverse_transform(y_pred_encoded)\n",
    "\n",
    "# Calculate macro F1\n",
    "macro_f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LSTM MODEL EVALUATION RESULTS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ¯ MACRO F1 SCORE: {macro_f1:.4f}\")\n",
    "print(f\"   XGBoost baseline was: 0.30\")\n",
    "print(f\"   XGBoost + temporal features was: 0.29\")\n",
    "\n",
    "if macro_f1 > 0.30:\n",
    "    improvement = (macro_f1 - 0.30) / 0.30 * 100\n",
    "    print(f\"   âœ“ Improvement over baseline: +{improvement:.1f}%\")\n",
    "else:\n",
    "    decline = (0.30 - macro_f1) / 0.30 * 100\n",
    "    print(f\"   âœ— Decline: -{decline:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"PER-CLASS F1 SCORES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get per-class F1 scores\n",
    "report = classification_report(y_test, y_pred, output_dict=True, zero_division=0)\n",
    "\n",
    "class_f1_scores = []\n",
    "for class_name in sorted(set(y_test)):\n",
    "    if class_name in report:\n",
    "        f1 = report[class_name]['f1-score']\n",
    "        support = report[class_name]['support']\n",
    "        class_f1_scores.append((class_name, f1, support))\n",
    "\n",
    "class_f1_scores.sort(key=lambda x: -x[1])\n",
    "\n",
    "print(f\"\\n{'Room':<20s} {'F1 Score':<12s} {'Support':<10s}\")\n",
    "print(\"-\" * 80)\n",
    "for room, f1, support in class_f1_scores:\n",
    "    print(f\"{room:<20s} {f1:<12.4f} {int(support):<10d}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"SUMMARY STATISTICS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Average F1 (macro): {macro_f1:.4f}\")\n",
    "print(f\"Best performing room: {class_f1_scores[0][0]} (F1={class_f1_scores[0][1]:.4f})\")\n",
    "print(f\"Worst performing room: {class_f1_scores[-1][0]} (F1={class_f1_scores[-1][1]:.4f})\")\n",
    "print(f\"Rooms with F1 > 0.5: {sum(1 for _, f1, _ in class_f1_scores if f1 > 0.5)}/{len(class_f1_scores)}\")\n",
    "print(f\"Rooms with F1 < 0.2: {sum(1 for _, f1, _ in class_f1_scores if f1 < 0.2)}/{len(class_f1_scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f06151b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
