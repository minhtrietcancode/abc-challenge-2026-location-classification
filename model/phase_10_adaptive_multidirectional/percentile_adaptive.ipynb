{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7p8GcuxMaIQ",
        "outputId": "03e3062e-5e7d-4f4b-dd3f-433f2780858c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì All imports successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGYH5Iq3MaK2",
        "outputId": "1e82b1fe-24a2-4d33-975a-9795a6222505"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All imports successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"‚úì All folds loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuMOOi6mMaNi",
        "outputId": "335bc9be-f07a-4012-8522-a11f8b20ce61"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All folds loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def build_bidirectional_gru_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU Architecture\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"‚úÖ Basic functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9I9l7FpIMaQe",
        "outputId": "7592d25b-4ade-41f5-d4fe-4ed4d503db16"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Basic functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_extended_multidirectional_windows(vector_df):\n",
        "    \"\"\"\n",
        "    Create 7 types of sliding windows (same as Experiment 2)\n",
        "    \"\"\"\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    results = {\n",
        "        'backward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'centered_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'backward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_past': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_future': {'sequences': [], 'labels': [], 'indices': []},\n",
        "    }\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        n = len(vectors)\n",
        "\n",
        "        for i in range(n):\n",
        "            if i >= 9:\n",
        "                window = vectors[i - 9 : i + 1]\n",
        "                results['backward_10']['sequences'].append(window)\n",
        "                results['backward_10']['labels'].append(rooms[i])\n",
        "                results['backward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            if i >= 4 and i + 5 < n:\n",
        "                window = vectors[i - 4 : i + 6]\n",
        "                results['centered_10']['sequences'].append(window)\n",
        "                results['centered_10']['labels'].append(rooms[i])\n",
        "                results['centered_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            if i + 9 < n:\n",
        "                window = vectors[i : i + 10]\n",
        "                results['forward_10']['sequences'].append(window)\n",
        "                results['forward_10']['labels'].append(rooms[i])\n",
        "                results['forward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            if i >= 14:\n",
        "                window = vectors[i - 14 : i + 1]\n",
        "                results['backward_15']['sequences'].append(window)\n",
        "                results['backward_15']['labels'].append(rooms[i])\n",
        "                results['backward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            if i + 14 < n:\n",
        "                window = vectors[i : i + 15]\n",
        "                results['forward_15']['sequences'].append(window)\n",
        "                results['forward_15']['labels'].append(rooms[i])\n",
        "                results['forward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            if i >= 11 and i + 3 < n:\n",
        "                window = vectors[i - 11 : i + 4]\n",
        "                results['asymm_past']['sequences'].append(window)\n",
        "                results['asymm_past']['labels'].append(rooms[i])\n",
        "                results['asymm_past']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            if i >= 3 and i + 11 < n:\n",
        "                window = vectors[i - 3 : i + 12]\n",
        "                results['asymm_future']['sequences'].append(window)\n",
        "                results['asymm_future']['labels'].append(rooms[i])\n",
        "                results['asymm_future']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Multi-directional window function defined (7 directions)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1zIAclxeMaTD",
        "outputId": "b5b6a0ff-7990-4aec-da5f-52d0b4f0538f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Multi-directional window function defined (7 directions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ensemble_models(train_df, n_models=5, base_seed=42, verbose=False):\n",
        "    \"\"\"\n",
        "    Train ensemble (same as Experiment 2)\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"  Training ensemble of {n_models} models...\")\n",
        "\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df, max_length=50)\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000\n",
        "        set_seeds(model_seed)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    Model {i+1}/{n_models} (seed {model_seed})...\", end=\" \")\n",
        "\n",
        "        model = build_bidirectional_gru_model(\n",
        "            input_shape=(50, 23),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "\n",
        "        model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"‚úì\")\n",
        "\n",
        "    return models, label_encoder\n",
        "\n",
        "print(\"‚úì Ensemble training function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "egn5WzGTMaVu",
        "outputId": "6c2ba195-bb12-4717-bbd7-25488bf651fe"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Ensemble training function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_direction(models, sequences, max_seq_length=50):\n",
        "    \"\"\"\n",
        "    Get ensemble predictions for a single direction\n",
        "    \"\"\"\n",
        "    X_padded = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        proba = model.predict(X_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "\n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    return ensemble_proba\n",
        "\n",
        "def calculate_percentile_thresholds(direction_results, verbose=False):\n",
        "    \"\"\"\n",
        "    NEW: Calculate thresholds using PERCENTILES instead of mean+std\n",
        "\n",
        "    Key advantages:\n",
        "    1. Robust to outliers (not affected by extreme confidence values)\n",
        "    2. Distribution-agnostic (works for any shape, not just normal)\n",
        "    3. Directly interpretable (75th percentile = top 25% of predictions)\n",
        "    4. Automatically adapts to each fold's confidence distribution\n",
        "\n",
        "    Strategy:\n",
        "    - Collect ALL confidence values from all 7 directions\n",
        "    - Calculate percentiles directly from the empirical distribution\n",
        "    - Use different percentiles for different boosting strategies\n",
        "\n",
        "    Returns:\n",
        "        dict with percentile-based thresholds\n",
        "    \"\"\"\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    # Collect all confidences\n",
        "    all_confidences = []\n",
        "    centered_confidences = []\n",
        "\n",
        "    for direction_name in direction_names:\n",
        "        proba = direction_results[direction_name]['proba']\n",
        "        confidences = np.max(proba, axis=1)\n",
        "        all_confidences.extend(confidences)\n",
        "\n",
        "        if direction_name == 'centered_10':\n",
        "            centered_confidences = confidences\n",
        "\n",
        "    all_confidences = np.array(all_confidences)\n",
        "    centered_confidences = np.array(centered_confidences)\n",
        "\n",
        "    # Calculate percentile-based thresholds\n",
        "    # These percentiles chosen based on adaptive strategy needs:\n",
        "\n",
        "    # For \"very high confidence\" boost (Strategy 1):\n",
        "    # Use 85th percentile = top 15% of predictions\n",
        "    # These are the MOST confident predictions that deserve strong boosting\n",
        "    very_high_conf_threshold = np.percentile(all_confidences, 85)\n",
        "\n",
        "    # For \"high confidence\" boost (Strategy 1):\n",
        "    # Use 70th percentile = top 30% of predictions\n",
        "    # These are confident but not extreme\n",
        "    high_conf_threshold = np.percentile(all_confidences, 70)\n",
        "\n",
        "    # For \"centered boost\" (Strategy 2):\n",
        "    # Use 60th percentile of CENTERED predictions specifically\n",
        "    # This means we boost centered when it's above its own median confidence\n",
        "    centered_boost_threshold = np.percentile(centered_confidences, 60)\n",
        "\n",
        "    # For \"low confidence\" detection (Strategy 3):\n",
        "    # Use 40th percentile = bottom 40% of predictions\n",
        "    # When predictions are this uncertain, use equal weighting\n",
        "    low_conf_threshold = np.percentile(all_confidences, 40)\n",
        "\n",
        "    thresholds = {\n",
        "        'very_high_conf': very_high_conf_threshold,\n",
        "        'high_conf': high_conf_threshold,\n",
        "        'centered_boost': centered_boost_threshold,\n",
        "        'low_conf': low_conf_threshold\n",
        "    }\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Percentile-based thresholds:\")\n",
        "        print(f\"    Confidence distribution: min={np.min(all_confidences):.3f}, \"\n",
        "              f\"median={np.median(all_confidences):.3f}, max={np.max(all_confidences):.3f}\")\n",
        "        print(f\"    ‚Üí Very high conf (85th percentile): {very_high_conf_threshold:.3f}\")\n",
        "        print(f\"    ‚Üí High conf (70th percentile): {high_conf_threshold:.3f}\")\n",
        "        print(f\"    ‚Üí Centered boost (60th of centered): {centered_boost_threshold:.3f}\")\n",
        "        print(f\"    ‚Üí Low conf (40th percentile): {low_conf_threshold:.3f}\")\n",
        "\n",
        "    return thresholds\n",
        "\n",
        "def combine_directional_predictions_percentile(direction_results,\n",
        "                                               percentile_thresholds,\n",
        "                                               verbose=False):\n",
        "    \"\"\"\n",
        "    NEW: Adaptive combination using PERCENTILE-BASED thresholds\n",
        "\n",
        "    Same adaptive logic as Exp 4/5, but with robust percentile thresholds!\n",
        "    \"\"\"\n",
        "    # Unpack percentile thresholds\n",
        "    very_high_conf_threshold = percentile_thresholds['very_high_conf']\n",
        "    high_conf_threshold = percentile_thresholds['high_conf']\n",
        "    centered_boost_threshold = percentile_thresholds['centered_boost']\n",
        "    low_conf_threshold = percentile_thresholds['low_conf']\n",
        "\n",
        "    # Build position mapping\n",
        "    all_positions = set()\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction in direction_names:\n",
        "        all_positions.update(direction_results[direction]['indices'])\n",
        "\n",
        "    all_positions = sorted(all_positions)\n",
        "    position_map = {pos: idx for idx, pos in enumerate(all_positions)}\n",
        "\n",
        "    n_classes = direction_results['backward_10']['proba'].shape[1]\n",
        "    n_positions = len(all_positions)\n",
        "\n",
        "    combined_proba = np.zeros((n_positions, n_classes))\n",
        "\n",
        "    # Pre-compute confidences\n",
        "    direction_confidences = {}\n",
        "    for direction_name in direction_names:\n",
        "        proba = direction_results[direction_name]['proba']\n",
        "        direction_confidences[direction_name] = np.max(proba, axis=1)\n",
        "\n",
        "    # Apply adaptive weighting per position\n",
        "    for pos_idx, pos in enumerate(all_positions):\n",
        "        position_directions = {}\n",
        "        position_confs = {}\n",
        "\n",
        "        for direction_name in direction_names:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                position_directions[direction_name] = direction_results[direction_name]['proba'][idx]\n",
        "                position_confs[direction_name] = direction_confidences[direction_name][idx]\n",
        "\n",
        "        if not position_directions:\n",
        "            continue\n",
        "\n",
        "        # ADAPTIVE WEIGHTING with percentile thresholds\n",
        "        weights = {}\n",
        "\n",
        "        max_conf = max(position_confs.values())\n",
        "        avg_conf = np.mean(list(position_confs.values()))\n",
        "        centered_conf = position_confs.get('centered_10', 0)\n",
        "\n",
        "        # Strategy 1: Very high confidence (top 15% of all predictions)\n",
        "        if max_conf >= very_high_conf_threshold:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                if conf >= very_high_conf_threshold:\n",
        "                    weights[direction_name] = conf * 2.5  # Strong boost\n",
        "                elif conf >= high_conf_threshold:\n",
        "                    weights[direction_name] = conf * 1.2  # Moderate boost\n",
        "                else:\n",
        "                    weights[direction_name] = conf * 0.5  # Reduce low-conf\n",
        "\n",
        "        # Strategy 2: Centered is confident (above its 60th percentile)\n",
        "        elif centered_conf >= centered_boost_threshold:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                if direction_name == 'centered_10':\n",
        "                    weights[direction_name] = conf * 1.8  # Boost centered\n",
        "                else:\n",
        "                    weights[direction_name] = conf * 0.8  # Slight reduction for others\n",
        "\n",
        "        # Strategy 3: All low confidence (bottom 40%)\n",
        "        elif avg_conf < low_conf_threshold:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                weights[direction_name] = 1.0  # Equal weight when uncertain\n",
        "\n",
        "        # Strategy 4: Normal case (standard confidence weighting)\n",
        "        else:\n",
        "            for direction_name, conf in position_confs.items():\n",
        "                weights[direction_name] = conf\n",
        "\n",
        "        # Combine predictions\n",
        "        total_weight = sum(weights.values())\n",
        "        for direction_name, weight in weights.items():\n",
        "            combined_proba[pos_idx] += position_directions[direction_name] * weight\n",
        "\n",
        "        if total_weight > 0:\n",
        "            combined_proba[pos_idx] /= total_weight\n",
        "\n",
        "    return combined_proba, position_map\n",
        "\n",
        "print(\"‚úÖ Percentile-based threshold calculation and combination defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8EB6zTJhMaYf",
        "outputId": "8eaff506-e899-4ab8-896a-3756c028159e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Percentile-based threshold calculation and combination defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_confidence_weighted_voting(predictions_proba, vote_window=5):\n",
        "    \"\"\"\n",
        "    Confidence-weighted temporal voting (same as Experiment 2)\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        half_window = vote_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "\n",
        "        window_proba = predictions_proba[start:end]\n",
        "        window_confidences = np.max(window_proba, axis=1)\n",
        "\n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "\n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "\n",
        "    return voted_predictions\n",
        "\n",
        "print(\"‚úÖ Temporal voting function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onlY0Cq1MabO",
        "outputId": "862f6dc2-3e28-4b05-abe0-7f876c7f7a62"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Temporal voting function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_percentile_adaptive_pipeline(train_df, test_df, seed, n_ensemble=5,\n",
        "                                     vote_window=5,\n",
        "                                     verbose=False):\n",
        "    \"\"\"\n",
        "    EXPERIMENT 6: 7 directions + PERCENTILE-BASED adaptive thresholds\n",
        "\n",
        "    Key innovation: Use percentiles (robust, distribution-agnostic) instead of mean+std!\n",
        "\n",
        "    Pipeline:\n",
        "    1. Train ensemble (5 models)\n",
        "    2. Create 7 directional windows\n",
        "    3. Get predictions for all directions\n",
        "    4. NEW: Calculate percentile-based thresholds from confidence distributions\n",
        "    5. Apply adaptive weighting with percentile thresholds\n",
        "    6. Temporal voting\n",
        "    \"\"\"\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  Seed {seed}: Training ensemble...\")\n",
        "\n",
        "    # 1. Train Ensemble\n",
        "    models, label_encoder = train_ensemble_models(\n",
        "        train_df,\n",
        "        n_models=n_ensemble,\n",
        "        base_seed=seed,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  Creating multi-directional windows (7 directions)...\")\n",
        "\n",
        "    # 2. Create Windows\n",
        "    test_vectors = create_beacon_count_vectors(test_df)\n",
        "    direction_windows = create_extended_multidirectional_windows(test_vectors)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  Getting directional predictions...\")\n",
        "\n",
        "    # 3. Get Predictions\n",
        "    direction_results = {}\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction_name in direction_names:\n",
        "        sequences = direction_windows[direction_name]['sequences']\n",
        "        proba = predict_single_direction(models, sequences, max_seq_length=50)\n",
        "\n",
        "        direction_results[direction_name] = {\n",
        "            'proba': proba,\n",
        "            'indices': direction_windows[direction_name]['indices'],\n",
        "            'labels': direction_windows[direction_name]['labels']\n",
        "        }\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  Calculating percentile-based thresholds...\")\n",
        "\n",
        "    # 4. NEW: Calculate percentile thresholds\n",
        "    percentile_thresholds = calculate_percentile_thresholds(direction_results, verbose=verbose)\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  Combining directions with percentile-based adaptive weighting...\")\n",
        "\n",
        "    # 5. Adaptive Combination with percentile thresholds\n",
        "    combined_proba, position_map = combine_directional_predictions_percentile(\n",
        "        direction_results,\n",
        "        percentile_thresholds,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    # Get ground truth\n",
        "    y_test = []\n",
        "    for pos in sorted(position_map.keys()):\n",
        "        for direction_name in direction_names:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                y_test.append(direction_results[direction_name]['labels'][idx])\n",
        "                break\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Applying temporal voting (window={vote_window})...\")\n",
        "\n",
        "    # 6. Temporal Voting\n",
        "    y_pred_voted_encoded = apply_confidence_weighted_voting(combined_proba, vote_window=vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "\n",
        "    # 7. Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  ‚úì Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)},\n",
        "        'percentile_thresholds': percentile_thresholds\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Complete percentile-based adaptive pipeline defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdsmYuJgMady",
        "outputId": "2eb19955-d262-4d06-bda2-2578be1436b7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete percentile-based adaptive pipeline defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FULL EXPERIMENT\n",
        "print(\"=\"*80)\n",
        "print(\"FULL 4-FOLD CROSS-VALIDATION - EXPERIMENT 6\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seeds = [42, 123, 456]\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"  Running seed {seed}...\", end=\" \")\n",
        "        result = run_percentile_adaptive_pipeline(\n",
        "            train_df, test_df,\n",
        "            seed=seed,\n",
        "            n_ensemble=5,\n",
        "            vote_window=5,\n",
        "            verbose=False\n",
        "        )\n",
        "        fold_results.append(result)\n",
        "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "\n",
        "    all_fold_results[fold_num] = fold_results\n",
        "\n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
        "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} ¬± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "    # Show average percentile thresholds for this fold\n",
        "    avg_thresholds = {\n",
        "        'very_high': np.mean([r['percentile_thresholds']['very_high_conf'] for r in fold_results]),\n",
        "        'high': np.mean([r['percentile_thresholds']['high_conf'] for r in fold_results]),\n",
        "        'centered': np.mean([r['percentile_thresholds']['centered_boost'] for r in fold_results]),\n",
        "        'low': np.mean([r['percentile_thresholds']['low_conf'] for r in fold_results])\n",
        "    }\n",
        "    print(f\"    Avg percentile thresholds: centered={avg_thresholds['centered']:.3f}, \"\n",
        "          f\"high={avg_thresholds['high']:.3f}, very_high={avg_thresholds['very_high']:.3f}, \"\n",
        "          f\"low={avg_thresholds['low']:.3f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v3ekTwRTMahG",
        "outputId": "6784e5aa-812b-4a89-a9f7-e45689801a9d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FULL 4-FOLD CROSS-VALIDATION - EXPERIMENT 6\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 1\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.5275\n",
            "  Running seed 123... Macro F1: 0.4963\n",
            "  Running seed 456... Macro F1: 0.4904\n",
            "\n",
            "  Fold 1 Summary:\n",
            "    Mean Macro F1: 0.5047 ¬± 0.0163\n",
            "    Avg percentile thresholds: centered=0.684, high=0.757, very_high=0.847, low=0.613\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 2\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4142\n",
            "  Running seed 123... Macro F1: 0.4303\n",
            "  Running seed 456... Macro F1: 0.4269\n",
            "\n",
            "  Fold 2 Summary:\n",
            "    Mean Macro F1: 0.4238 ¬± 0.0070\n",
            "    Avg percentile thresholds: centered=0.682, high=0.734, very_high=0.786, low=0.575\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 3\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4056\n",
            "  Running seed 123... Macro F1: 0.4313\n",
            "  Running seed 456... Macro F1: 0.3963\n",
            "\n",
            "  Fold 3 Summary:\n",
            "    Mean Macro F1: 0.4111 ¬± 0.0148\n",
            "    Avg percentile thresholds: centered=0.677, high=0.748, very_high=0.808, low=0.603\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 4\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4239\n",
            "  Running seed 123... Macro F1: 0.4070\n",
            "  Running seed 456... Macro F1: 0.4231\n",
            "\n",
            "  Fold 4 Summary:\n",
            "    Mean Macro F1: 0.4180 ¬± 0.0078\n",
            "    Avg percentile thresholds: centered=0.685, high=0.737, very_high=0.794, low=0.624\n",
            "\n",
            "================================================================================\n",
            "ALL FOLDS COMPLETED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Summary\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY - EXPERIMENT 6 (PERCENTILE-BASED)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} ¬± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "all_macro_f1 = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} ¬± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPLETE PROGRESSION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Baseline (Approach 24): 0.4106\")\n",
        "print(\"Exp 2 (7 dir, standard): 0.4384\")\n",
        "print(\"Exp 4 (fixed thresholds): 0.4392 (Fold 1 great, others declined)\")\n",
        "print(\"Exp 5 (mean+std adaptive): 0.4390\")\n",
        "print(f\"Exp 6 (percentile adaptive): {np.mean(all_macro_f1):.4f}\")\n",
        "\n",
        "# Per-fold comparison\n",
        "exp2_folds = [0.4896, 0.4295, 0.4113, 0.4230]\n",
        "exp4_folds = [0.5094, 0.4231, 0.4072, 0.4172]\n",
        "exp5_folds = [0.4993, 0.4230, 0.4135, 0.4204]\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(\"PER-FOLD COMPARISON:\")\n",
        "print(f\"{'='*80}\")\n",
        "print(f\"{'Fold':>6} {'Exp2':>8} {'Exp4':>8} {'Exp5':>8} {'Exp6':>8} {'vs Exp2':>9} {'vs Exp4':>9}\")\n",
        "print(\"-\" * 80)\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    exp6_mean = np.mean([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "    exp2_mean = exp2_folds[fold_num - 1]\n",
        "    exp4_mean = exp4_folds[fold_num - 1]\n",
        "    vs_exp2 = exp6_mean - exp2_mean\n",
        "    vs_exp4 = exp6_mean - exp4_mean\n",
        "    print(f\"{fold_num:>6} {exp2_mean:>8.4f} {exp4_mean:>8.4f} {exp5_folds[fold_num-1]:>8.4f} \"\n",
        "          f\"{exp6_mean:>8.4f} {vs_exp2:>+9.4f} {vs_exp4:>+9.4f}\")\n",
        "\n",
        "total_gain = np.mean(all_macro_f1) - 0.4106\n",
        "gain_from_exp2 = np.mean(all_macro_f1) - 0.4384\n",
        "target_gap = 0.45 - np.mean(all_macro_f1)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Total gain from baseline (0.4106): {total_gain:+.4f}\")\n",
        "print(f\"Gain from Exp 2 (0.4384): {gain_from_exp2:+.4f}\")\n",
        "print(f\"Gap to target (0.45): {target_gap:.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if np.mean(all_macro_f1) >= 0.45:\n",
        "    print(\"\\nüéØüéØüéØ TARGET ACHIEVED! 0.45 F1 REACHED! üéØüéØüéØ\")\n",
        "    print(\"\\nüéâ PERCENTILE-BASED THRESHOLDS WORKED!\")\n",
        "elif np.mean(all_macro_f1) > 0.4390:\n",
        "    print(f\"\\n‚úÖ Percentile thresholds improve over previous! +{gain_from_exp2:.4f}\")\n",
        "    if target_gap < 0.01:\n",
        "        print(f\"   SO CLOSE! Gap is only {target_gap:.4f}\")\n",
        "        print(\"   Try: Hyperparameter tuning (vote_window, ensemble_size)\")\n",
        "    else:\n",
        "        print(f\"   Remaining gap: {target_gap:.4f}\")\n",
        "elif gain_from_exp2 > -0.005:\n",
        "    print(\"\\nüìä Percentiles maintain performance (similar to Exp 2)\")\n",
        "    print(\"   But MORE ROBUST: adapts to each fold's distribution!\")\n",
        "    print(f\"   Next: Try hyperparameter tuning to close {target_gap:.4f} gap\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  Percentiles didn't improve\")\n",
        "    print(\"   Recommendation: Try hyperparameter tuning instead\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YychL4KhMajd",
        "outputId": "d2bebfca-c1eb-48e9-dbd2-5e4d5ac961cf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY - EXPERIMENT 6 (PERCENTILE-BASED)\n",
            "================================================================================\n",
            "\n",
            "Fold 1: 0.5047 ¬± 0.0163\n",
            "Fold 2: 0.4238 ¬± 0.0070\n",
            "Fold 3: 0.4111 ¬± 0.0148\n",
            "Fold 4: 0.4180 ¬± 0.0078\n",
            "\n",
            "================================================================================\n",
            "Overall Mean: 0.4394 ¬± 0.0399\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "COMPLETE PROGRESSION:\n",
            "================================================================================\n",
            "Baseline (Approach 24): 0.4106\n",
            "Exp 2 (7 dir, standard): 0.4384\n",
            "Exp 4 (fixed thresholds): 0.4392 (Fold 1 great, others declined)\n",
            "Exp 5 (mean+std adaptive): 0.4390\n",
            "Exp 6 (percentile adaptive): 0.4394\n",
            "\n",
            "================================================================================\n",
            "PER-FOLD COMPARISON:\n",
            "================================================================================\n",
            "  Fold     Exp2     Exp4     Exp5     Exp6   vs Exp2   vs Exp4\n",
            "--------------------------------------------------------------------------------\n",
            "     1   0.4896   0.5094   0.4993   0.5047   +0.0151   -0.0047\n",
            "     2   0.4295   0.4231   0.4230   0.4238   -0.0057   +0.0007\n",
            "     3   0.4113   0.4072   0.4135   0.4111   -0.0002   +0.0039\n",
            "     4   0.4230   0.4172   0.4204   0.4180   -0.0050   +0.0008\n",
            "\n",
            "================================================================================\n",
            "Total gain from baseline (0.4106): +0.0288\n",
            "Gain from Exp 2 (0.4384): +0.0010\n",
            "Gap to target (0.45): 0.0106\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Percentile thresholds improve over previous! +0.0010\n",
            "   Remaining gap: 0.0106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results\n",
        "with open('experiment6_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"EXPERIMENT 6: PERCENTILE-BASED ADAPTIVE THRESHOLDS\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"Key Innovation:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Use PERCENTILES instead of mean+std for threshold calculation:\\n\")\n",
        "    f.write(\"  - Very high conf: 85th percentile (top 15%)\\n\")\n",
        "    f.write(\"  - High conf: 70th percentile (top 30%)\\n\")\n",
        "    f.write(\"  - Centered boost: 60th percentile of centered predictions\\n\")\n",
        "    f.write(\"  - Low conf: 40th percentile (bottom 40%)\\n\")\n",
        "    f.write(\"\\nAdvantages:\\n\")\n",
        "    f.write(\"  - Robust to outliers\\n\")\n",
        "    f.write(\"  - Distribution-agnostic (no normal distribution assumption)\\n\")\n",
        "    f.write(\"  - Directly interpretable\\n\")\n",
        "    f.write(\"  - Automatically adapts to each fold\\n\\n\")\n",
        "\n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "\n",
        "    f.write(\"OVERALL RESULTS:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} ¬± {np.std(all_macro_f1):.4f}\\n\\n\")\n",
        "\n",
        "    f.write(\"PROGRESSION:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Baseline: 0.4106\\n\")\n",
        "    f.write(\"Exp 2: 0.4384 (7 directions, standard weighting)\\n\")\n",
        "    f.write(\"Exp 4: 0.4392 (fixed thresholds, fold-specific)\\n\")\n",
        "    f.write(\"Exp 5: 0.4390 (mean+std adaptive)\\n\")\n",
        "    f.write(f\"Exp 6: {np.mean(all_macro_f1):.4f} (percentile-based)\\n\\n\")\n",
        "\n",
        "    f.write(\"Per-Fold Results:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\nFold {fold_num}:\\n\")\n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "\n",
        "        for result in fold_results:\n",
        "            t = result['percentile_thresholds']\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\")\n",
        "            f.write(f\" (thresholds: centered={t['centered_boost']:.3f}, \"\n",
        "                   f\"high={t['high_conf']:.3f}, very_high={t['very_high_conf']:.3f}, \"\n",
        "                   f\"low={t['low_conf']:.3f})\\n\")\n",
        "\n",
        "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} ¬± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "\n",
        "print(\"‚úÖ Results saved to experiment6_results.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jx8MMFEiMamf",
        "outputId": "ad1f1520-1689-4a3d-d01f-9afb85359884"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Results saved to experiment6_results.txt\n"
          ]
        }
      ]
    }
  ]
}