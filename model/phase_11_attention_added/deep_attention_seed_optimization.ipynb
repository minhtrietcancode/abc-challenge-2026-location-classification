{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1kAEk4xjWQR",
        "outputId": "b00dd82e-0f36-453e-dda2-ba50b230ee94"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to6kTSNJjWTB",
        "outputId": "f287f94a-4c2c-4815-e496-3c46e6f23e6f"
      },
      "outputs": [],
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Input, Multiply, Permute, Reshape, Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ†• Custom Attention Layer (Deep Architecture)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Custom Attention Layer for Sequence Models\n",
        "    \n",
        "    This layer learns which timesteps in the sequence are most important\n",
        "    for the classification task. It computes attention weights and returns\n",
        "    a weighted sum of the input sequence.\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "    \n",
        "    def build(self, input_shape):\n",
        "        # input_shape: (batch_size, timesteps, features)\n",
        "        self.W = self.add_weight(\n",
        "            name='attention_weight',\n",
        "            shape=(input_shape[-1], 1),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            name='attention_bias',\n",
        "            shape=(1,),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "    \n",
        "    def call(self, inputs, mask=None):\n",
        "        # inputs shape: (batch_size, timesteps, features)\n",
        "        \n",
        "        # Compute attention scores: (batch_size, timesteps, 1)\n",
        "        attention_scores = K.tanh(K.dot(inputs, self.W) + self.b)\n",
        "        \n",
        "        # Apply mask if provided (for padded sequences)\n",
        "        if mask is not None:\n",
        "            # Expand mask to match attention_scores shape\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            mask = K.expand_dims(mask, axis=-1)\n",
        "            # Set masked positions to very negative value\n",
        "            attention_scores = attention_scores * mask + (1 - mask) * (-1e10)\n",
        "        \n",
        "        # Compute attention weights: (batch_size, timesteps, 1)\n",
        "        attention_weights = K.softmax(attention_scores, axis=1)\n",
        "        \n",
        "        # Compute weighted sum: (batch_size, features)\n",
        "        context_vector = K.sum(inputs * attention_weights, axis=1)\n",
        "        \n",
        "        return context_vector\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Output shape: (batch_size, features)\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "    \n",
        "    def get_config(self):\n",
        "        return super(AttentionLayer, self).get_config()\n",
        "\n",
        "print(\"âœ… Custom Attention Layer defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkOej6oujWVw",
        "outputId": "a93d5410-699b-4de2-fd14-627721ae76af"
      },
      "outputs": [],
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"âœ“ All folds loaded\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMw3Pu6ljWYZ",
        "outputId": "6d373c6f-2511-4f8e-bc5b-75aba0420d15"
      },
      "outputs": [],
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns # Check if we are in 'training' mode\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "print(\"âœ… Basic functions defined\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸ† Deep Attention Model Architecture (BEST PERFORMER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_bidirectional_gru_model_with_deep_attention(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Deep Bidirectional GRU Architecture with Attention\n",
        "    \n",
        "    Architecture:\n",
        "    1. Masking layer\n",
        "    2. First Bi-GRU (128 units) with return_sequences=True\n",
        "    3. Dropout (0.3)\n",
        "    4. Second Bi-GRU (64 units) with return_sequences=True\n",
        "    5. Dropout (0.3)\n",
        "    6. ðŸ†• ATTENTION LAYER - aggregates the deep sequence features\n",
        "    7. Dense layers + Output\n",
        "    \n",
        "    This version keeps both Bi-GRU layers and adds attention on top.\n",
        "    The second GRU acts as implicit regularization, making attention more stable.\n",
        "    \n",
        "    Proven to be the most stable across all folds with low variance.\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape, name='input_layer')\n",
        "    \n",
        "    # Masking for padded sequences\n",
        "    masked = Masking(mask_value=0.0, name='masking')(inputs)\n",
        "    \n",
        "    # First Bi-GRU layer\n",
        "    gru1 = Bidirectional(\n",
        "        GRU(128, return_sequences=True, name='gru_layer_1'),\n",
        "        name='bidirectional_gru_1'\n",
        "    )(masked)\n",
        "    gru1 = Dropout(0.3, name='dropout_1')(gru1)\n",
        "    \n",
        "    # Second Bi-GRU layer (provides stability)\n",
        "    gru2 = Bidirectional(\n",
        "        GRU(64, return_sequences=True, name='gru_layer_2'),\n",
        "        name='bidirectional_gru_2'\n",
        "    )(gru1)\n",
        "    gru2 = Dropout(0.3, name='dropout_2')(gru2)\n",
        "    \n",
        "    # ðŸ†• ATTENTION MECHANISM\n",
        "    # Attention aggregates the deep bi-directional features\n",
        "    attention_output = AttentionLayer(name='attention_layer')(gru2)\n",
        "    \n",
        "    # Dense layers for classification\n",
        "    dense1 = Dense(32, activation='relu', name='dense_1')(attention_output)\n",
        "    dense1 = Dropout(0.2, name='dropout_3')(dense1)\n",
        "    \n",
        "    # Output layer\n",
        "    outputs = Dense(num_classes, activation='softmax', name='output_layer')(dense1)\n",
        "    \n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='Deep_BiGRU_with_Attention')\n",
        "    \n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    \n",
        "    return model\n",
        "\n",
        "print(\"âœ… Deep Attention-based Bi-GRU model architecture defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4VTHYpRjWbF",
        "outputId": "3fa784fa-fd09-49b7-fa4b-15313d2fa47e"
      },
      "outputs": [],
      "source": [
        "def create_extended_multidirectional_windows(vector_df):\n",
        "    \"\"\"\n",
        "    Create 7 types of sliding windows for extended multi-directional prediction\n",
        "\n",
        "    Directions:\n",
        "    1. backward_10:  [i-9 to i]     - 10s history, predict at i\n",
        "    2. centered_10:  [i-4 to i+5]   - 10s centered, predict at i\n",
        "    3. forward_10:   [i to i+9]     - 10s future, predict at i\n",
        "    4. backward_15:  [i-14 to i]    - 15s history (more context)\n",
        "    5. forward_15:   [i to i+14]    - 15s future (earlier transition detection)\n",
        "    6. asymm_past:   [i-11 to i+3]  - 12s past + 4s future (transition from old room)\n",
        "    7. asymm_future: [i-3 to i+11]  - 4s past + 12s future (entering new room)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with direction names as keys\n",
        "        Each contains: (sequences, labels, valid_indices)\n",
        "    \"\"\"\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    results = {\n",
        "        'backward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'centered_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'backward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_past': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_future': {'sequences': [], 'labels': [], 'indices': []},\n",
        "    }\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        n = len(vectors)\n",
        "\n",
        "        for i in range(n):\n",
        "            # 1. BACKWARD_10: [i-9, ..., i] predict at i\n",
        "            if i >= 9:\n",
        "                window = vectors[i - 9 : i + 1]\n",
        "                results['backward_10']['sequences'].append(window)\n",
        "                results['backward_10']['labels'].append(rooms[i])\n",
        "                results['backward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 2. CENTERED_10: [i-4, ..., i, ..., i+5] predict at i\n",
        "            if i >= 4 and i + 5 < n:\n",
        "                window = vectors[i - 4 : i + 6]\n",
        "                results['centered_10']['sequences'].append(window)\n",
        "                results['centered_10']['labels'].append(rooms[i])\n",
        "                results['centered_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 3. FORWARD_10: [i, ..., i+9] predict at i\n",
        "            if i + 9 < n:\n",
        "                window = vectors[i : i + 10]\n",
        "                results['forward_10']['sequences'].append(window)\n",
        "                results['forward_10']['labels'].append(rooms[i])\n",
        "                results['forward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 4. BACKWARD_15: [i-14, ..., i] predict at i (MORE HISTORY)\n",
        "            if i >= 14:\n",
        "                window = vectors[i - 14 : i + 1]\n",
        "                results['backward_15']['sequences'].append(window)\n",
        "                results['backward_15']['labels'].append(rooms[i])\n",
        "                results['backward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 5. FORWARD_15: [i, ..., i+14] predict at i (EARLIER TRANSITION DETECTION)\n",
        "            if i + 14 < n:\n",
        "                window = vectors[i : i + 15]\n",
        "                results['forward_15']['sequences'].append(window)\n",
        "                results['forward_15']['labels'].append(rooms[i])\n",
        "                results['forward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 6. ASYMM_PAST: [i-11, ..., i, ..., i+3] predict at i (HEAVY PAST BIAS)\n",
        "            # Good for detecting we're leaving a room\n",
        "            if i >= 11 and i + 3 < n:\n",
        "                window = vectors[i - 11 : i + 4]\n",
        "                results['asymm_past']['sequences'].append(window)\n",
        "                results['asymm_past']['labels'].append(rooms[i])\n",
        "                results['asymm_past']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 7. ASYMM_FUTURE: [i-3, ..., i, ..., i+11] predict at i (HEAVY FUTURE BIAS)\n",
        "            # Good for detecting we're entering a room\n",
        "            if i >= 3 and i + 11 < n:\n",
        "                window = vectors[i - 3 : i + 12]\n",
        "                results['asymm_future']['sequences'].append(window)\n",
        "                results['asymm_future']['labels'].append(rooms[i])\n",
        "                results['asymm_future']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Extended multi-directional window function defined (7 directions)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOUm2J7wjWd9",
        "outputId": "9dfa96f4-a12c-4258-fc34-e3e81d4ecf9f"
      },
      "outputs": [],
      "source": [
        "def train_ensemble_models(train_df, n_models=1, base_seed=42, verbose=False):\n",
        "    \"\"\"\n",
        "    ðŸ†• UPDATED: Train models with DEEP ATTENTION\n",
        "    \n",
        "    Note: For seed optimization, n_models=1 (single model per seed)\n",
        "          For final production, can increase to n_models=5-7 (ensemble per seed)\n",
        "\n",
        "    Returns:\n",
        "        models: List of trained Keras models (with deep attention)\n",
        "        label_encoder: Fitted label encoder\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"  Training {n_models} model(s) with DEEP ATTENTION...\")\n",
        "\n",
        "    # Prepare data (same for all models)\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df, max_length=50)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "\n",
        "    # Train multiple models\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000\n",
        "        set_seeds(model_seed)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    Model {i+1}/{n_models} (seed {model_seed})...\", end=\" \")\n",
        "\n",
        "        # ðŸ†• BUILD DEEP ATTENTION MODEL\n",
        "        model = build_bidirectional_gru_model_with_deep_attention(\n",
        "            input_shape=(50, 23),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "\n",
        "        # Train\n",
        "        model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"âœ“\")\n",
        "\n",
        "    return models, label_encoder\n",
        "\n",
        "print(\"âœ… Ensemble training function defined (with deep attention support)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsnhVa94jWgj",
        "outputId": "8e00d76c-f5be-4c04-9b39-c6e77f37a950"
      },
      "outputs": [],
      "source": [
        "def predict_single_direction(models, sequences, max_seq_length=50):\n",
        "    \"\"\"\n",
        "    Get ensemble predictions for a single direction\n",
        "\n",
        "    Returns:\n",
        "        ensemble_proba: (n_samples, n_classes) averaged probability matrix\n",
        "    \"\"\"\n",
        "    # Pad sequences\n",
        "    X_padded = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    # Get predictions from all models\n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        proba = model.predict(X_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "\n",
        "    # Average probabilities across ensemble\n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    return ensemble_proba\n",
        "\n",
        "def combine_directional_predictions(direction_results, method='confidence_weighted'):\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple directions using confidence weighting\n",
        "    Handles 7 directions\n",
        "\n",
        "    Args:\n",
        "        direction_results: Dict with keys for all 7 directions\n",
        "                          Each value is a dict with 'proba' and 'indices'\n",
        "        method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "\n",
        "    Returns:\n",
        "        combined_proba: (n_positions, n_classes) final probability matrix\n",
        "        position_map: mapping from (date, position) to array index\n",
        "    \"\"\"\n",
        "    # Build a mapping of all unique positions\n",
        "    all_positions = set()\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction in direction_names:\n",
        "        all_positions.update(direction_results[direction]['indices'])\n",
        "\n",
        "    # Sort positions for consistent ordering\n",
        "    all_positions = sorted(all_positions)\n",
        "    position_map = {pos: idx for idx, pos in enumerate(all_positions)}\n",
        "\n",
        "    # Get number of classes from first available direction\n",
        "    n_classes = direction_results['backward_10']['proba'].shape[1]\n",
        "    n_positions = len(all_positions)\n",
        "\n",
        "    # Initialize combined predictions\n",
        "    combined_proba = np.zeros((n_positions, n_classes))\n",
        "    position_counts = np.zeros(n_positions)  # Track how many directions contributed\n",
        "\n",
        "    # For each direction, add its weighted contribution\n",
        "    for direction_name in direction_names:\n",
        "        direction_data = direction_results[direction_name]\n",
        "        proba = direction_data['proba']\n",
        "        indices = direction_data['indices']\n",
        "\n",
        "        # Get confidence (max probability) for each prediction\n",
        "        confidences = np.max(proba, axis=1)\n",
        "\n",
        "        # Add weighted contribution to combined predictions\n",
        "        for i, pos in enumerate(indices):\n",
        "            pos_idx = position_map[pos]\n",
        "\n",
        "            if method == 'confidence_weighted':\n",
        "                # Weight by confidence\n",
        "                weight = confidences[i]\n",
        "                combined_proba[pos_idx] += proba[i] * weight\n",
        "            elif method == 'equal':\n",
        "                # Equal weight\n",
        "                combined_proba[pos_idx] += proba[i]\n",
        "            elif method == 'softmax':\n",
        "                # Will apply softmax later\n",
        "                combined_proba[pos_idx] += proba[i] * confidences[i]\n",
        "\n",
        "            position_counts[pos_idx] += 1 if method == 'equal' else confidences[i]\n",
        "\n",
        "    # Normalize by total weight\n",
        "    for i in range(n_positions):\n",
        "        if position_counts[i] > 0:\n",
        "            combined_proba[i] /= position_counts[i]\n",
        "\n",
        "    return combined_proba, position_map\n",
        "\n",
        "print(\"âœ… Multi-directional prediction functions defined (handles 7 directions)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fPkdO7hjWjd",
        "outputId": "272daf16-c12c-492d-d88e-96c37556dd39"
      },
      "outputs": [],
      "source": [
        "def apply_confidence_weighted_voting(predictions_proba, vote_window=5):\n",
        "    \"\"\"\n",
        "    Confidence-weighted temporal voting\n",
        "\n",
        "    Instead of simple majority voting, weight each prediction by its confidence (max probability).\n",
        "\n",
        "    Args:\n",
        "        predictions_proba: (n_samples, n_classes) probability matrix from ensemble\n",
        "        vote_window: window size for voting\n",
        "\n",
        "    Returns:\n",
        "        voted_predictions: (n_samples,) final class predictions\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Get window boundaries\n",
        "        half_window = vote_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "\n",
        "        # Get probabilities within window\n",
        "        window_proba = predictions_proba[start:end]  # (window_size, n_classes)\n",
        "\n",
        "        # Get confidence (max probability) for each prediction in window\n",
        "        window_confidences = np.max(window_proba, axis=1)  # (window_size,)\n",
        "\n",
        "        # Weight each prediction by its confidence\n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            # Each timestep contributes its probability * its confidence\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "\n",
        "        # Final prediction: class with highest weighted vote\n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "\n",
        "    return voted_predictions\n",
        "\n",
        "print(\"âœ… Temporal voting function defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py5bXt5QjWmB",
        "outputId": "fb851507-2438-49af-fc8a-4e58888bd26a"
      },
      "outputs": [],
      "source": [
        "def run_extended_multidirectional_pipeline(train_df, test_df, seed, n_ensemble=1,\n",
        "                                           vote_window=5,\n",
        "                                           combination_method='confidence_weighted',\n",
        "                                           verbose=False):\n",
        "    \"\"\"\n",
        "    ðŸ†• UPDATED: Extended multi-directional windows with DEEP ATTENTION\n",
        "\n",
        "    Pipeline:\n",
        "    1. Train model(s) with DEEP ATTENTION\n",
        "    2. Create 7 directional windows\n",
        "    3. Get predictions for each direction\n",
        "    4. Combine directions using confidence weighting\n",
        "    5. Apply temporal voting\n",
        "\n",
        "    Args:\n",
        "        n_ensemble: Number of models to train per seed (1 for seed testing, 5-7 for production)\n",
        "        combination_method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "    \"\"\"\n",
        "    # 0. Clear session and set seeds\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  Seed {seed}: Training with deep attention...\")\n",
        "\n",
        "    # 1. Train Model(s) WITH DEEP ATTENTION\n",
        "    models, label_encoder = train_ensemble_models(\n",
        "        train_df,\n",
        "        n_models=n_ensemble,\n",
        "        base_seed=seed,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  Creating extended multi-directional windows (7 directions)...\")\n",
        "\n",
        "    # 2. Prepare Test Data with Extended Multi-Directional Windows\n",
        "    test_vectors = create_beacon_count_vectors(test_df)\n",
        "    direction_windows = create_extended_multidirectional_windows(test_vectors)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"    Backward_10 windows: {len(direction_windows['backward_10']['sequences'])}\")\n",
        "        print(f\"    Centered_10 windows: {len(direction_windows['centered_10']['sequences'])}\")\n",
        "        print(f\"    Forward_10 windows: {len(direction_windows['forward_10']['sequences'])}\")\n",
        "        print(f\"    Backward_15 windows: {len(direction_windows['backward_15']['sequences'])}\")\n",
        "        print(f\"    Forward_15 windows: {len(direction_windows['forward_15']['sequences'])}\")\n",
        "        print(f\"    Asymm_past windows: {len(direction_windows['asymm_past']['sequences'])}\")\n",
        "        print(f\"    Asymm_future windows: {len(direction_windows['asymm_future']['sequences'])}\")\n",
        "        print(\"  Getting directional predictions...\")\n",
        "\n",
        "    # 3. Get Predictions for Each Direction\n",
        "    direction_results = {}\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction_name in direction_names:\n",
        "        if verbose:\n",
        "            print(f\"    Predicting {direction_name}...\", end=\" \")\n",
        "\n",
        "        sequences = direction_windows[direction_name]['sequences']\n",
        "        proba = predict_single_direction(models, sequences, max_seq_length=50)\n",
        "\n",
        "        direction_results[direction_name] = {\n",
        "            'proba': proba,\n",
        "            'indices': direction_windows[direction_name]['indices'],\n",
        "            'labels': direction_windows[direction_name]['labels']\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            avg_conf = np.mean(np.max(proba, axis=1))\n",
        "            print(f\"avg confidence: {avg_conf:.3f}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Combining 7 directions using {combination_method}...\")\n",
        "\n",
        "    # 4. Combine Directional Predictions\n",
        "    combined_proba, position_map = combine_directional_predictions(\n",
        "        direction_results,\n",
        "        method=combination_method\n",
        "    )\n",
        "\n",
        "    # Get ground truth labels in same order as combined predictions\n",
        "    y_test = []\n",
        "    for pos in sorted(position_map.keys()):\n",
        "        # Use label from any direction (they should all be the same for a given position)\n",
        "        for direction_name in direction_names:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                y_test.append(direction_results[direction_name]['labels'][idx])\n",
        "                break\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Applying temporal voting (window={vote_window})...\")\n",
        "\n",
        "    # 5. Apply Confidence-Weighted Temporal Voting\n",
        "    y_pred_voted_encoded = apply_confidence_weighted_voting(combined_proba, vote_window=vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "\n",
        "    # 6. Final Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  âœ“ Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)},\n",
        "        'combination_method': combination_method\n",
        "    }\n",
        "\n",
        "print(\"âœ… Complete extended multi-directional pipeline defined (7 directions + DEEP ATTENTION)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rWma91UjWpA",
        "outputId": "3a478e36-f332-45ea-cfe8-7f7ee6cb340b"
      },
      "outputs": [],
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸŽ¯ SEED OPTIMIZATION EXPERIMENT\n",
        "\n",
        "Testing two theoretically-motivated seed sets:\n",
        "\n",
        "**Set 1 - Diverse Primes:** Seeds based on prime numbers for better RNG independence  \n",
        "**Set 2 - Golden Ratio:** Seeds spaced using golden ratio (Ï† â‰ˆ 1.618) for uniform coverage\n",
        "\n",
        "Each set contains 7 seeds. We'll test both on all 4 folds to find the optimal seed strategy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ðŸŽ¯ DEFINE THE TWO SEED SETS\n",
        "\n",
        "# Set 1: Diverse Primes (based on number theory)\n",
        "# Prime numbers provide better independence in random number generation\n",
        "SEED_SET_1 = [42, 1009, 2503, 4001, 5501, 7507, 9001]\n",
        "\n",
        "# Set 2: Golden Ratio Spacing (based on geometric optimization)\n",
        "# Golden ratio provides quasi-uniform coverage of parameter space\n",
        "SEED_SET_2 = [42, 1000, 2618, 4236, 6854, 8472, 11090]\n",
        "\n",
        "print(\"=\"*80)\n",
        "print(\"SEED OPTIMIZATION EXPERIMENT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nSet 1 (Diverse Primes):\")\n",
        "print(f\"  Seeds: {SEED_SET_1}\")\n",
        "print(f\"  Rationale: Prime numbers for RNG independence\")\n",
        "print(f\"  Spacing: ~1500-2000 apart\")\n",
        "print(f\"  Range: 42 â†’ 9001\")\n",
        "\n",
        "print(f\"\\nSet 2 (Golden Ratio):\")\n",
        "print(f\"  Seeds: {SEED_SET_2}\")\n",
        "print(f\"  Rationale: Golden ratio (Ï† â‰ˆ 1.618) for uniform coverage\")\n",
        "print(f\"  Spacing: Variable (following Ï† pattern)\")\n",
        "print(f\"  Range: 42 â†’ 11090\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT SETUP:\")\n",
        "print(\"=\"*80)\n",
        "print(\"  â€¢ Model: Deep Bidirectional GRU with Attention\")\n",
        "print(\"  â€¢ Directions: 7 (backward_10, centered_10, forward_10, backward_15, forward_15, asymm_past, asymm_future)\")\n",
        "print(\"  â€¢ Ensemble per seed: 1 model (testing individual seeds)\")\n",
        "print(\"  â€¢ Temporal voting: 5-second window\")\n",
        "print(\"  â€¢ Folds: All 4 folds\")\n",
        "print(\"  â€¢ Total runs: 2 sets Ã— 7 seeds Ã— 4 folds = 56 experiments\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA1iijvEjWvC",
        "outputId": "3396649a-3e97-4d48-909f-e701126b0cdf"
      },
      "outputs": [],
      "source": [
        "# ðŸš€ RUN SEED OPTIMIZATION EXPERIMENT\n",
        "\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "seed_sets = {\n",
        "    'Set_1_Primes': SEED_SET_1,\n",
        "    'Set_2_GoldenRatio': SEED_SET_2\n",
        "}\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for set_name, seed_list in seed_sets.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TESTING {set_name}\")\n",
        "    print(f\"Seeds: {seed_list}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    set_results = {}\n",
        "    \n",
        "    for fold_num, (train_df, test_df) in folds.items():\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"FOLD {fold_num} - {set_name}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        fold_results = []\n",
        "        \n",
        "        for seed in seed_list:\n",
        "            print(f\"  Running seed {seed:5d}...\", end=\" \")\n",
        "            result = run_extended_multidirectional_pipeline(\n",
        "                train_df, test_df,\n",
        "                seed=seed,\n",
        "                n_ensemble=1,  # Single model per seed for testing\n",
        "                vote_window=5,\n",
        "                combination_method='confidence_weighted',\n",
        "                verbose=False\n",
        "            )\n",
        "            fold_results.append(result)\n",
        "            print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "        \n",
        "        set_results[fold_num] = fold_results\n",
        "        \n",
        "        # Fold summary\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "        print(f\"\\n  Fold {fold_num} Summary ({set_name}):\")\n",
        "        print(f\"    Mean: {np.mean(macro_f1_scores):.4f} Â± {np.std(macro_f1_scores):.4f}\")\n",
        "        print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "    \n",
        "    all_results[set_name] = set_results\n",
        "    \n",
        "    # Overall summary for this set\n",
        "    all_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        all_f1.extend([r['macro_f1'] for r in set_results[fold_num]])\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"{set_name} OVERALL SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Overall Mean: {np.mean(all_f1):.4f} Â± {np.std(all_f1):.4f}\")\n",
        "    print(f\"Min: {np.min(all_f1):.4f}, Max: {np.max(all_f1):.4f}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL EXPERIMENTS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8QdqLt2jngx",
        "outputId": "c956cd5f-d7d6-43c0-8644-c19eea7167d2"
      },
      "outputs": [],
      "source": [
        "# ðŸ“Š FINAL COMPARISON AND WINNER DECLARATION\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL COMPARISON: SET 1 vs SET 2\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "# Calculate overall statistics for each set\n",
        "set_stats = {}\n",
        "\n",
        "for set_name in ['Set_1_Primes', 'Set_2_GoldenRatio']:\n",
        "    all_f1 = []\n",
        "    fold_means = []\n",
        "    fold_stds = []\n",
        "    \n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_f1s = [r['macro_f1'] for r in all_results[set_name][fold_num]]\n",
        "        all_f1.extend(fold_f1s)\n",
        "        fold_means.append(np.mean(fold_f1s))\n",
        "        fold_stds.append(np.std(fold_f1s))\n",
        "    \n",
        "    set_stats[set_name] = {\n",
        "        'overall_mean': np.mean(all_f1),\n",
        "        'overall_std': np.std(all_f1),\n",
        "        'overall_min': np.min(all_f1),\n",
        "        'overall_max': np.max(all_f1),\n",
        "        'fold_means': fold_means,\n",
        "        'fold_stds': fold_stds,\n",
        "        'avg_fold_std': np.mean(fold_stds)\n",
        "    }\n",
        "\n",
        "# Display comparison table\n",
        "print(\"OVERALL PERFORMANCE:\")\n",
        "print(\"-\"*80)\n",
        "print(f\"{'Metric':<30} {'Set 1 (Primes)':<20} {'Set 2 (Golden Ratio)':<20}\")\n",
        "print(\"-\"*80)\n",
        "\n",
        "for metric in ['overall_mean', 'overall_std', 'overall_min', 'overall_max', 'avg_fold_std']:\n",
        "    metric_name = metric.replace('_', ' ').title()\n",
        "    val1 = set_stats['Set_1_Primes'][metric]\n",
        "    val2 = set_stats['Set_2_GoldenRatio'][metric]\n",
        "    print(f\"{metric_name:<30} {val1:<20.4f} {val2:<20.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PER-FOLD COMPARISON:\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    f1_set1 = [r['macro_f1'] for r in all_results['Set_1_Primes'][fold_num]]\n",
        "    f1_set2 = [r['macro_f1'] for r in all_results['Set_2_GoldenRatio'][fold_num]]\n",
        "    \n",
        "    print(f\"Fold {fold_num}:\")\n",
        "    print(f\"  Set 1: {np.mean(f1_set1):.4f} Â± {np.std(f1_set1):.4f}\")\n",
        "    print(f\"  Set 2: {np.mean(f1_set2):.4f} Â± {np.std(f1_set2):.4f}\")\n",
        "    print()\n",
        "\n",
        "# Determine winner\n",
        "print(\"=\"*80)\n",
        "print(\"WINNER DETERMINATION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "set1_mean = set_stats['Set_1_Primes']['overall_mean']\n",
        "set2_mean = set_stats['Set_2_GoldenRatio']['overall_mean']\n",
        "set1_std = set_stats['Set_1_Primes']['avg_fold_std']\n",
        "set2_std = set_stats['Set_2_GoldenRatio']['avg_fold_std']\n",
        "\n",
        "# Winner based on mean performance\n",
        "if set1_mean > set2_mean:\n",
        "    winner = 'Set_1_Primes'\n",
        "    winner_seeds = SEED_SET_1\n",
        "    gain = set1_mean - set2_mean\n",
        "    winner_std = set1_std\n",
        "else:\n",
        "    winner = 'Set_2_GoldenRatio'\n",
        "    winner_seeds = SEED_SET_2\n",
        "    gain = set2_mean - set1_mean\n",
        "    winner_std = set2_std\n",
        "\n",
        "print(f\"\\nðŸ† WINNER: {winner}\")\n",
        "print(f\"   Advantage: {gain:+.4f} ({gain/min(set1_mean, set2_mean)*100:+.2f}%)\")\n",
        "print(f\"   Overall Mean: {max(set1_mean, set2_mean):.4f} Â± {winner_std:.4f}\")\n",
        "print(f\"   Recommended Seeds: {winner_seeds}\")\n",
        "\n",
        "# Comparison to baseline\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"COMPARISON TO BASELINE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "baseline_mean = 0.4438  # Deep attention with 3 seeds Ã— 5 ensemble\n",
        "baseline_label = \"Deep Attention Baseline (3 seeds Ã— 5 models)\"\n",
        "winner_mean = max(set1_mean, set2_mean)\n",
        "\n",
        "improvement = winner_mean - baseline_mean\n",
        "\n",
        "print(f\"\\n{baseline_label}:\")\n",
        "print(f\"  Overall: {baseline_mean:.4f} Â± 0.0295\")\n",
        "\n",
        "print(f\"\\n{winner} (7 optimized seeds Ã— 1 model):\")\n",
        "print(f\"  Overall: {winner_mean:.4f} Â± {winner_std:.4f}\")\n",
        "\n",
        "print(f\"\\nImprovement: {improvement:+.4f} ({improvement/baseline_mean*100:+.2f}%)\")\n",
        "\n",
        "# Target achievement\n",
        "target = 0.45\n",
        "gap_to_target = target - winner_mean\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TARGET ACHIEVEMENT\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nTarget: {target:.4f}\")\n",
        "print(f\"Current: {winner_mean:.4f}\")\n",
        "print(f\"Gap: {gap_to_target:.4f}\")\n",
        "\n",
        "if winner_mean >= target:\n",
        "    print(\"\\nðŸŽ¯ðŸŽ¯ðŸŽ¯ TARGET ACHIEVED! ðŸŽ¯ðŸŽ¯ðŸŽ¯\")\n",
        "    print(f\"âœ… Exceeded target by {winner_mean - target:+.4f}!\")\n",
        "elif gap_to_target <= 0.005:\n",
        "    print(\"\\nðŸŽ¯ ALMOST THERE!\")\n",
        "    print(f\"   Only {gap_to_target:.4f} away from target!\")\n",
        "    print(f\"   Recommendation: Increase ensemble to 5-7 models per seed\")\n",
        "    print(f\"   Expected with 5 models/seed: ~{winner_mean + 0.005:.4f} to {winner_mean + 0.010:.4f}\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Good progress! {gap_to_target:.4f} away from target\")\n",
        "    print(f\"   Recommendation: Increase ensemble size to 5-7 models per seed\")\n",
        "    print(f\"   Expected gain: +0.005 to +0.015\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atj9yQr1jpGQ",
        "outputId": "d39cbf5b-0dd8-4b83-f4f1-5ee75c079392"
      },
      "outputs": [],
      "source": [
        "# ðŸ’¾ SAVE RESULTS TO FILE\n",
        "\n",
        "with open('seed_optimization_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"SEED OPTIMIZATION EXPERIMENT RESULTS\\n\")\n",
        "    f.write(\"Deep Bidirectional GRU with Attention\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "    \n",
        "    f.write(\"SEED SETS TESTED:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Set 1 (Diverse Primes): {SEED_SET_1}\\n\")\n",
        "    f.write(f\"Set 2 (Golden Ratio):   {SEED_SET_2}\\n\\n\")\n",
        "    \n",
        "    f.write(\"OVERALL RESULTS:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    \n",
        "    for set_name in ['Set_1_Primes', 'Set_2_GoldenRatio']:\n",
        "        stats = set_stats[set_name]\n",
        "        f.write(f\"\\n{set_name}:\\n\")\n",
        "        f.write(f\"  Overall Mean: {stats['overall_mean']:.4f} Â± {stats['overall_std']:.4f}\\n\")\n",
        "        f.write(f\"  Range: {stats['overall_min']:.4f} to {stats['overall_max']:.4f}\\n\")\n",
        "        f.write(f\"  Avg Fold Std: {stats['avg_fold_std']:.4f}\\n\")\n",
        "        \n",
        "        f.write(f\"\\n  Per-Fold Results:\\n\")\n",
        "        for i, (mean, std) in enumerate(zip(stats['fold_means'], stats['fold_stds']), 1):\n",
        "            f.write(f\"    Fold {i}: {mean:.4f} Â± {std:.4f}\\n\")\n",
        "    \n",
        "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    f.write(\"WINNER:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"{winner}\\n\")\n",
        "    f.write(f\"Seeds: {winner_seeds}\\n\")\n",
        "    f.write(f\"Overall Mean: {max(set1_mean, set2_mean):.4f} Â± {winner_std:.4f}\\n\")\n",
        "    f.write(f\"Advantage over other set: {gain:+.4f}\\n\")\n",
        "    \n",
        "    f.write(\"\\n\" + \"=\"*80 + \"\\n\")\n",
        "    f.write(\"DETAILED RESULTS BY FOLD:\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    \n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\nFOLD {fold_num}:\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        \n",
        "        for set_name in ['Set_1_Primes', 'Set_2_GoldenRatio']:\n",
        "            f.write(f\"\\n{set_name}:\\n\")\n",
        "            fold_results = all_results[set_name][fold_num]\n",
        "            \n",
        "            for result in fold_results:\n",
        "                f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
        "            \n",
        "            fold_f1s = [r['macro_f1'] for r in fold_results]\n",
        "            f.write(f\"  Mean: {np.mean(fold_f1s):.4f} Â± {np.std(fold_f1s):.4f}\\n\")\n",
        "\n",
        "print(\"âœ… Results saved to seed_optimization_results.txt\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"EXPERIMENT COMPLETE!\")\n",
        "print(\"=\"*80)\n",
        "print(f\"\\nðŸ† Winning seed set: {winner_seeds}\")\n",
        "print(f\"ðŸ“Š Overall performance: {max(set1_mean, set2_mean):.4f} Â± {winner_std:.4f}\")\n",
        "print(f\"ðŸŽ¯ Gap to target (0.45): {gap_to_target:.4f}\")\n",
        "print(\"\\nNext step: Use winning seeds with n_ensemble=5-7 for final model!\")\n",
        "print(\"=\"*80)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ðŸš€ NEXT STEPS\n",
        "\n",
        "After identifying the winning seed set, you can:\n",
        "\n",
        "1. **Increase ensemble size** with winning seeds:\n",
        "   ```python\n",
        "   n_ensemble = 5  # or 7\n",
        "   # Expected gain: +0.005 to +0.015\n",
        "   ```\n",
        "\n",
        "2. **Fine-tune vote_window** parameter:\n",
        "   ```python\n",
        "   vote_window = 3  # or 7\n",
        "   # Test different temporal smoothing windows\n",
        "   ```\n",
        "\n",
        "3. **Train final production model** on all 4 days combined\n",
        "\n",
        "The winning seed set provides the optimal random initialization strategy for your model!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
