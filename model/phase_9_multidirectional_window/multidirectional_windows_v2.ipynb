{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1kAEk4xjWQR",
        "outputId": "b00dd82e-0f36-453e-dda2-ba50b230ee94"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úì All imports successful\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "to6kTSNJjWTB",
        "outputId": "f287f94a-4c2c-4815-e496-3c46e6f23e6f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All imports successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_filter_fold(i):\n",
        "    train_dir = f'/content/drive/MyDrive/split_data/fold{i}/train.csv'\n",
        "    test_dir = f'/content/drive/MyDrive/split_data/fold{i}/test.csv'\n",
        "    train_df = pd.read_csv(train_dir)\n",
        "    test_df = pd.read_csv(test_dir)\n",
        "\n",
        "    train_labels = list(train_df['room'].unique())\n",
        "    test_labels = list(test_df['room'].unique())\n",
        "    common_labels = list(set(train_labels) & set(test_labels))\n",
        "\n",
        "    train_df = train_df[train_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "    test_df = test_df[test_df['room'].isin(common_labels)].reset_index(drop=True)\n",
        "\n",
        "    return train_df, test_df\n",
        "\n",
        "# Load all 4 folds\n",
        "train_df_1, test_df_1 = load_and_filter_fold(1)\n",
        "train_df_2, test_df_2 = load_and_filter_fold(2)\n",
        "train_df_3, test_df_3 = load_and_filter_fold(3)\n",
        "train_df_4, test_df_4 = load_and_filter_fold(4)\n",
        "\n",
        "print(\"‚úì All folds loaded\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GkOej6oujWVw",
        "outputId": "a93d5410-699b-4de2-fd14-627721ae76af"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì All folds loaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns # Check if we are in 'training' mode\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "def build_bidirectional_gru_model(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Bidirectional GRU Architecture\n",
        "    \"\"\"\n",
        "    model = Sequential([\n",
        "        Masking(mask_value=0.0, input_shape=input_shape),\n",
        "\n",
        "        Bidirectional(GRU(128, return_sequences=True)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Bidirectional(GRU(64, return_sequences=False)),\n",
        "        Dropout(0.3),\n",
        "\n",
        "        Dense(32, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "    return model\n",
        "\n",
        "print(\"‚úÖ Basic functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMw3Pu6ljWYZ",
        "outputId": "6d373c6f-2511-4f8e-bc5b-75aba0420d15"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Basic functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_extended_multidirectional_windows(vector_df):\n",
        "    \"\"\"\n",
        "    EXPERIMENT 2: Create 7 types of sliding windows for extended multi-directional prediction\n",
        "\n",
        "    Directions:\n",
        "    1. backward_10:  [i-9 to i]     - 10s history, predict at i\n",
        "    2. centered_10:  [i-4 to i+5]   - 10s centered, predict at i\n",
        "    3. forward_10:   [i to i+9]     - 10s future, predict at i\n",
        "    4. backward_15:  [i-14 to i]    - 15s history (more context)\n",
        "    5. forward_15:   [i to i+14]    - 15s future (earlier transition detection)\n",
        "    6. asymm_past:   [i-11 to i+3]  - 12s past + 4s future (transition from old room)\n",
        "    7. asymm_future: [i-3 to i+11]  - 4s past + 12s future (entering new room)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with direction names as keys\n",
        "        Each contains: (sequences, labels, valid_indices)\n",
        "    \"\"\"\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    results = {\n",
        "        'backward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'centered_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'backward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_past': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_future': {'sequences': [], 'labels': [], 'indices': []},\n",
        "    }\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        n = len(vectors)\n",
        "\n",
        "        for i in range(n):\n",
        "            # 1. BACKWARD_10: [i-9, ..., i] predict at i\n",
        "            if i >= 9:\n",
        "                window = vectors[i - 9 : i + 1]\n",
        "                results['backward_10']['sequences'].append(window)\n",
        "                results['backward_10']['labels'].append(rooms[i])\n",
        "                results['backward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 2. CENTERED_10: [i-4, ..., i, ..., i+5] predict at i\n",
        "            if i >= 4 and i + 5 < n:\n",
        "                window = vectors[i - 4 : i + 6]\n",
        "                results['centered_10']['sequences'].append(window)\n",
        "                results['centered_10']['labels'].append(rooms[i])\n",
        "                results['centered_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 3. FORWARD_10: [i, ..., i+9] predict at i\n",
        "            if i + 9 < n:\n",
        "                window = vectors[i : i + 10]\n",
        "                results['forward_10']['sequences'].append(window)\n",
        "                results['forward_10']['labels'].append(rooms[i])\n",
        "                results['forward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 4. BACKWARD_15: [i-14, ..., i] predict at i (MORE HISTORY)\n",
        "            if i >= 14:\n",
        "                window = vectors[i - 14 : i + 1]\n",
        "                results['backward_15']['sequences'].append(window)\n",
        "                results['backward_15']['labels'].append(rooms[i])\n",
        "                results['backward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 5. FORWARD_15: [i, ..., i+14] predict at i (EARLIER TRANSITION DETECTION)\n",
        "            if i + 14 < n:\n",
        "                window = vectors[i : i + 15]\n",
        "                results['forward_15']['sequences'].append(window)\n",
        "                results['forward_15']['labels'].append(rooms[i])\n",
        "                results['forward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 6. ASYMM_PAST: [i-11, ..., i, ..., i+3] predict at i (HEAVY PAST BIAS)\n",
        "            # Good for detecting we're leaving a room\n",
        "            if i >= 11 and i + 3 < n:\n",
        "                window = vectors[i - 11 : i + 4]\n",
        "                results['asymm_past']['sequences'].append(window)\n",
        "                results['asymm_past']['labels'].append(rooms[i])\n",
        "                results['asymm_past']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 7. ASYMM_FUTURE: [i-3, ..., i, ..., i+11] predict at i (HEAVY FUTURE BIAS)\n",
        "            # Good for detecting we're entering a room\n",
        "            if i >= 3 and i + 11 < n:\n",
        "                window = vectors[i - 3 : i + 12]\n",
        "                results['asymm_future']['sequences'].append(window)\n",
        "                results['asymm_future']['labels'].append(rooms[i])\n",
        "                results['asymm_future']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"‚úÖ Extended multi-directional window function defined (7 directions)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N4VTHYpRjWbF",
        "outputId": "3fa784fa-fd09-49b7-fa4b-15313d2fa47e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Extended multi-directional window function defined (7 directions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ensemble_models(train_df, n_models=5, base_seed=42, verbose=False):\n",
        "    \"\"\"\n",
        "    Train multiple models with different seeds for ensemble\n",
        "\n",
        "    Returns:\n",
        "        models: List of trained Keras models\n",
        "        label_encoder: Fitted label encoder\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"  Training ensemble of {n_models} models...\")\n",
        "\n",
        "    # Prepare data (same for all models)\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df, max_length=50)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "\n",
        "    # Train multiple models\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000  # 42, 1042, 2042, 3042, 4042\n",
        "        set_seeds(model_seed)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"    Model {i+1}/{n_models} (seed {model_seed})...\", end=\" \")\n",
        "\n",
        "        model = build_bidirectional_gru_model(\n",
        "            input_shape=(50, 23),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "\n",
        "        # Train\n",
        "        model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"‚úì\")\n",
        "\n",
        "    return models, label_encoder\n",
        "\n",
        "print(\"‚úì Ensemble training function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOUm2J7wjWd9",
        "outputId": "9dfa96f4-a12c-4258-fc34-e3e81d4ecf9f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úì Ensemble training function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_direction(models, sequences, max_seq_length=50):\n",
        "    \"\"\"\n",
        "    Get ensemble predictions for a single direction\n",
        "\n",
        "    Returns:\n",
        "        ensemble_proba: (n_samples, n_classes) averaged probability matrix\n",
        "    \"\"\"\n",
        "    # Pad sequences\n",
        "    X_padded = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    # Get predictions from all models\n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        proba = model.predict(X_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "\n",
        "    # Average probabilities across ensemble\n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    return ensemble_proba\n",
        "\n",
        "def combine_directional_predictions(direction_results, method='confidence_weighted'):\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple directions using confidence weighting\n",
        "    Now handles 7 directions instead of 3\n",
        "\n",
        "    Args:\n",
        "        direction_results: Dict with keys for all 7 directions\n",
        "                          Each value is a dict with 'proba' and 'indices'\n",
        "        method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "\n",
        "    Returns:\n",
        "        combined_proba: (n_positions, n_classes) final probability matrix\n",
        "        position_map: mapping from (date, position) to array index\n",
        "    \"\"\"\n",
        "    # Build a mapping of all unique positions\n",
        "    all_positions = set()\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction in direction_names:\n",
        "        all_positions.update(direction_results[direction]['indices'])\n",
        "\n",
        "    # Sort positions for consistent ordering\n",
        "    all_positions = sorted(all_positions)\n",
        "    position_map = {pos: idx for idx, pos in enumerate(all_positions)}\n",
        "\n",
        "    # Get number of classes from first available direction\n",
        "    n_classes = direction_results['backward_10']['proba'].shape[1]\n",
        "    n_positions = len(all_positions)\n",
        "\n",
        "    # Initialize combined predictions\n",
        "    combined_proba = np.zeros((n_positions, n_classes))\n",
        "    position_counts = np.zeros(n_positions)  # Track how many directions contributed\n",
        "\n",
        "    # For each direction, add its weighted contribution\n",
        "    for direction_name in direction_names:\n",
        "        direction_data = direction_results[direction_name]\n",
        "        proba = direction_data['proba']\n",
        "        indices = direction_data['indices']\n",
        "\n",
        "        # Get confidence (max probability) for each prediction\n",
        "        confidences = np.max(proba, axis=1)\n",
        "\n",
        "        # Add weighted contribution to combined predictions\n",
        "        for i, pos in enumerate(indices):\n",
        "            pos_idx = position_map[pos]\n",
        "\n",
        "            if method == 'confidence_weighted':\n",
        "                # Weight by confidence\n",
        "                weight = confidences[i]\n",
        "                combined_proba[pos_idx] += proba[i] * weight\n",
        "            elif method == 'equal':\n",
        "                # Equal weight\n",
        "                combined_proba[pos_idx] += proba[i]\n",
        "            elif method == 'softmax':\n",
        "                # Will apply softmax later\n",
        "                combined_proba[pos_idx] += proba[i] * confidences[i]\n",
        "\n",
        "            position_counts[pos_idx] += 1 if method == 'equal' else confidences[i]\n",
        "\n",
        "    # Normalize by total weight\n",
        "    for i in range(n_positions):\n",
        "        if position_counts[i] > 0:\n",
        "            combined_proba[i] /= position_counts[i]\n",
        "\n",
        "    return combined_proba, position_map\n",
        "\n",
        "print(\"‚úÖ Multi-directional prediction functions defined (handles 7 directions)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsnhVa94jWgj",
        "outputId": "8e00d76c-f5be-4c04-9b39-c6e77f37a950"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Multi-directional prediction functions defined (handles 7 directions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def apply_confidence_weighted_voting(predictions_proba, vote_window=5):\n",
        "    \"\"\"\n",
        "    Confidence-weighted temporal voting\n",
        "\n",
        "    Instead of simple majority voting, weight each prediction by its confidence (max probability).\n",
        "\n",
        "    Args:\n",
        "        predictions_proba: (n_samples, n_classes) probability matrix from ensemble\n",
        "        vote_window: window size for voting\n",
        "\n",
        "    Returns:\n",
        "        voted_predictions: (n_samples,) final class predictions\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Get window boundaries\n",
        "        half_window = vote_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "\n",
        "        # Get probabilities within window\n",
        "        window_proba = predictions_proba[start:end]  # (window_size, n_classes)\n",
        "\n",
        "        # Get confidence (max probability) for each prediction in window\n",
        "        window_confidences = np.max(window_proba, axis=1)  # (window_size,)\n",
        "\n",
        "        # Weight each prediction by its confidence\n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            # Each timestep contributes its probability * its confidence\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "\n",
        "        # Final prediction: class with highest weighted vote\n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "\n",
        "    return voted_predictions\n",
        "\n",
        "print(\"‚úÖ Temporal voting function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7fPkdO7hjWjd",
        "outputId": "272daf16-c12c-492d-d88e-96c37556dd39"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Temporal voting function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_extended_multidirectional_pipeline(train_df, test_df, seed, n_ensemble=5,\n",
        "                                           vote_window=5,\n",
        "                                           combination_method='confidence_weighted',\n",
        "                                           verbose=False):\n",
        "    \"\"\"\n",
        "    EXPERIMENT 2: Extended multi-directional windows (7 directions) with confidence-weighted aggregation\n",
        "\n",
        "    Pipeline:\n",
        "    1. Train ensemble of models (same as baseline)\n",
        "    2. Create 7 directional windows (backward_10, centered_10, forward_10, backward_15, forward_15, asymm_past, asymm_future)\n",
        "    3. Get ensemble predictions for each direction\n",
        "    4. Combine directions using confidence weighting\n",
        "    5. Apply temporal voting\n",
        "\n",
        "    Args:\n",
        "        combination_method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "    \"\"\"\n",
        "    # 0. Clear session and set seeds\n",
        "    tf.keras.backend.clear_session()\n",
        "    set_seeds(seed)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"\\n  Seed {seed}: Training ensemble...\")\n",
        "\n",
        "    # 1. Train Ensemble Models (SAME AS BASELINE)\n",
        "    models, label_encoder = train_ensemble_models(\n",
        "        train_df,\n",
        "        n_models=n_ensemble,\n",
        "        base_seed=seed,\n",
        "        verbose=verbose\n",
        "    )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"  Creating extended multi-directional windows (7 directions)...\")\n",
        "\n",
        "    # 2. Prepare Test Data with Extended Multi-Directional Windows (NEW - 7 DIRECTIONS)\n",
        "    test_vectors = create_beacon_count_vectors(test_df)\n",
        "    direction_windows = create_extended_multidirectional_windows(test_vectors)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"    Backward_10 windows: {len(direction_windows['backward_10']['sequences'])}\")\n",
        "        print(f\"    Centered_10 windows: {len(direction_windows['centered_10']['sequences'])}\")\n",
        "        print(f\"    Forward_10 windows: {len(direction_windows['forward_10']['sequences'])}\")\n",
        "        print(f\"    Backward_15 windows: {len(direction_windows['backward_15']['sequences'])}\")\n",
        "        print(f\"    Forward_15 windows: {len(direction_windows['forward_15']['sequences'])}\")\n",
        "        print(f\"    Asymm_past windows: {len(direction_windows['asymm_past']['sequences'])}\")\n",
        "        print(f\"    Asymm_future windows: {len(direction_windows['asymm_future']['sequences'])}\")\n",
        "        print(\"  Getting directional predictions...\")\n",
        "\n",
        "    # 3. Get Predictions for Each Direction (7 DIRECTIONS)\n",
        "    direction_results = {}\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction_name in direction_names:\n",
        "        if verbose:\n",
        "            print(f\"    Predicting {direction_name}...\", end=\" \")\n",
        "\n",
        "        sequences = direction_windows[direction_name]['sequences']\n",
        "        proba = predict_single_direction(models, sequences, max_seq_length=50)\n",
        "\n",
        "        direction_results[direction_name] = {\n",
        "            'proba': proba,\n",
        "            'indices': direction_windows[direction_name]['indices'],\n",
        "            'labels': direction_windows[direction_name]['labels']\n",
        "        }\n",
        "\n",
        "        if verbose:\n",
        "            avg_conf = np.mean(np.max(proba, axis=1))\n",
        "            print(f\"avg confidence: {avg_conf:.3f}\")\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Combining 7 directions using {combination_method}...\")\n",
        "\n",
        "    # 4. Combine Directional Predictions (7 DIRECTIONS)\n",
        "    combined_proba, position_map = combine_directional_predictions(\n",
        "        direction_results,\n",
        "        method=combination_method\n",
        "    )\n",
        "\n",
        "    # Get ground truth labels in same order as combined predictions\n",
        "    y_test = []\n",
        "    for pos in sorted(position_map.keys()):\n",
        "        # Use label from any direction (they should all be the same for a given position)\n",
        "        for direction_name in direction_names:\n",
        "            if pos in direction_results[direction_name]['indices']:\n",
        "                idx = direction_results[direction_name]['indices'].index(pos)\n",
        "                y_test.append(direction_results[direction_name]['labels'][idx])\n",
        "                break\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Applying temporal voting (window={vote_window})...\")\n",
        "\n",
        "    # 5. Apply Confidence-Weighted Temporal Voting (SAME AS BASELINE)\n",
        "    y_pred_voted_encoded = apply_confidence_weighted_voting(combined_proba, vote_window=vote_window)\n",
        "    y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "\n",
        "    # 6. Final Evaluation\n",
        "    macro_f1 = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
        "    per_class_f1 = f1_score(y_test, y_pred, average=None, labels=label_encoder.classes_, zero_division=0)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  ‚úì Macro F1: {macro_f1:.4f}\")\n",
        "\n",
        "    return {\n",
        "        'seed': seed,\n",
        "        'macro_f1': macro_f1,\n",
        "        'per_class_f1': {label: f1 for label, f1 in zip(label_encoder.classes_, per_class_f1)},\n",
        "        'combination_method': combination_method\n",
        "    }\n",
        "\n",
        "print(\"‚úÖ Complete extended multi-directional pipeline defined (7 directions)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Py5bXt5QjWmB",
        "outputId": "fb851507-2438-49af-fc8a-4e58888bd26a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Complete extended multi-directional pipeline defined (7 directions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check GPU availability\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rWma91UjWpA",
        "outputId": "3a478e36-f332-45ea-cfe8-7f7ee6cb340b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n",
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FULL EXPERIMENT: All 4 folds, 3 seeds each\n",
        "print(\"=\"*80)\n",
        "print(\"FULL 4-FOLD CROSS-VALIDATION - EXPERIMENT 2\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "seeds = [42, 123, 456]\n",
        "folds = {\n",
        "    1: (train_df_1, test_df_1),\n",
        "    2: (train_df_2, test_df_2),\n",
        "    3: (train_df_3, test_df_3),\n",
        "    4: (train_df_4, test_df_4)\n",
        "}\n",
        "\n",
        "all_fold_results = {}\n",
        "\n",
        "for fold_num, (train_df, test_df) in folds.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"PROCESSING FOLD {fold_num}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    fold_results = []\n",
        "\n",
        "    for seed in seeds:\n",
        "        print(f\"  Running seed {seed}...\", end=\" \")\n",
        "        result = run_extended_multidirectional_pipeline(\n",
        "            train_df, test_df,\n",
        "            seed=seed,\n",
        "            n_ensemble=5,\n",
        "            vote_window=5,\n",
        "            combination_method='confidence_weighted',\n",
        "            verbose=False\n",
        "        )\n",
        "        fold_results.append(result)\n",
        "        print(f\"Macro F1: {result['macro_f1']:.4f}\")\n",
        "\n",
        "    all_fold_results[fold_num] = fold_results\n",
        "\n",
        "    # Calculate fold statistics\n",
        "    macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "    print(f\"\\n  Fold {fold_num} Summary:\")\n",
        "    print(f\"    Mean Macro F1: {np.mean(macro_f1_scores):.4f} ¬± {np.std(macro_f1_scores):.4f}\")\n",
        "    print(f\"    Min: {np.min(macro_f1_scores):.4f}, Max: {np.max(macro_f1_scores):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ALL FOLDS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EA1iijvEjWvC",
        "outputId": "3396649a-3e97-4d48-909f-e701126b0cdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "FULL 4-FOLD CROSS-VALIDATION - EXPERIMENT 2\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 1\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.5059\n",
            "  Running seed 123... Macro F1: 0.4935\n",
            "  Running seed 456... Macro F1: 0.4695\n",
            "\n",
            "  Fold 1 Summary:\n",
            "    Mean Macro F1: 0.4896 ¬± 0.0151\n",
            "    Min: 0.4695, Max: 0.5059\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 2\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4187\n",
            "  Running seed 123... Macro F1: 0.4373\n",
            "  Running seed 456... Macro F1: 0.4325\n",
            "\n",
            "  Fold 2 Summary:\n",
            "    Mean Macro F1: 0.4295 ¬± 0.0079\n",
            "    Min: 0.4187, Max: 0.4373\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 3\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4121\n",
            "  Running seed 123... Macro F1: 0.4321\n",
            "  Running seed 456... Macro F1: 0.3898\n",
            "\n",
            "  Fold 3 Summary:\n",
            "    Mean Macro F1: 0.4113 ¬± 0.0173\n",
            "    Min: 0.3898, Max: 0.4321\n",
            "\n",
            "================================================================================\n",
            "PROCESSING FOLD 4\n",
            "================================================================================\n",
            "\n",
            "  Running seed 42... Macro F1: 0.4285\n",
            "  Running seed 123... Macro F1: 0.4117\n",
            "  Running seed 456... Macro F1: 0.4287\n",
            "\n",
            "  Fold 4 Summary:\n",
            "    Mean Macro F1: 0.4230 ¬± 0.0080\n",
            "    Min: 0.4117, Max: 0.4287\n",
            "\n",
            "================================================================================\n",
            "ALL FOLDS COMPLETED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display summary and comparison\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"FINAL SUMMARY - EXPERIMENT 2 (7 DIRECTIONS)\")\n",
        "print(\"=\"*80 + \"\\n\")\n",
        "\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    macro_f1_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "    print(f\"Fold {fold_num}: {np.mean(macro_f1_scores):.4f} ¬± {np.std(macro_f1_scores):.4f}\")\n",
        "\n",
        "all_macro_f1 = []\n",
        "for fold_num in [1, 2, 3, 4]:\n",
        "    all_macro_f1.extend([r['macro_f1'] for r in all_fold_results[fold_num]])\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Overall Mean: {np.mean(all_macro_f1):.4f} ¬± {np.std(all_macro_f1):.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROGRESSION:\")\n",
        "print(\"=\"*80)\n",
        "print(\"Baseline (Approach 24 - single direction):\")\n",
        "print(\"  Overall: 0.4106 ¬± 0.0266\")\n",
        "print(f\"\\nExperiment 1 (3 directions):\")\n",
        "print(f\"  Overall: 0.4273 ¬± 0.0312  (+0.0167 vs baseline)\")\n",
        "print(f\"\\nExperiment 2 (7 directions):\")\n",
        "print(f\"  Overall: {np.mean(all_macro_f1):.4f} ¬± {np.std(all_macro_f1):.4f}  ({np.mean(all_macro_f1) - 0.4106:+.4f} vs baseline, {np.mean(all_macro_f1) - 0.4273:+.4f} vs Exp1)\")\n",
        "\n",
        "total_gain = np.mean(all_macro_f1) - 0.4106\n",
        "target_gap = 0.45 - np.mean(all_macro_f1)\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"Total gain from baseline: {total_gain:+.4f}\")\n",
        "print(f\"Gap to target (0.45): {target_gap:.4f}\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "if np.mean(all_macro_f1) >= 0.45:\n",
        "    print(\"\\nüéØüéØüéØ TARGET ACHIEVED! 0.45 F1 REACHED! üéØüéØüéØ\")\n",
        "elif np.mean(all_macro_f1) > 0.4273:\n",
        "    print(\"\\n‚úÖ Extended directions improved over 3 directions!\")\n",
        "    print(f\"   Recommendation: Try spatial constraints (Viterbi) to close remaining {target_gap:.4f} gap\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  7 directions didn't improve over 3 directions\")\n",
        "    print(\"   Recommendation: Try spatial constraints or hyperparameter tuning instead\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k8QdqLt2jngx",
        "outputId": "c956cd5f-d7d6-43c0-8644-c19eea7167d2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "FINAL SUMMARY - EXPERIMENT 2 (7 DIRECTIONS)\n",
            "================================================================================\n",
            "\n",
            "Fold 1: 0.4896 ¬± 0.0151\n",
            "Fold 2: 0.4295 ¬± 0.0079\n",
            "Fold 3: 0.4113 ¬± 0.0173\n",
            "Fold 4: 0.4230 ¬± 0.0080\n",
            "\n",
            "================================================================================\n",
            "Overall Mean: 0.4384 ¬± 0.0329\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "PROGRESSION:\n",
            "================================================================================\n",
            "Baseline (Approach 24 - single direction):\n",
            "  Overall: 0.4106 ¬± 0.0266\n",
            "\n",
            "Experiment 1 (3 directions):\n",
            "  Overall: 0.4273 ¬± 0.0312  (+0.0167 vs baseline)\n",
            "\n",
            "Experiment 2 (7 directions):\n",
            "  Overall: 0.4384 ¬± 0.0329  (+0.0278 vs baseline, +0.0111 vs Exp1)\n",
            "\n",
            "================================================================================\n",
            "Total gain from baseline: +0.0278\n",
            "Gap to target (0.45): 0.0116\n",
            "================================================================================\n",
            "\n",
            "‚úÖ Extended directions improved over 3 directions!\n",
            "   Recommendation: Try spatial constraints (Viterbi) to close remaining 0.0116 gap\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save results to text file\n",
        "with open('experiment2_results.txt', 'w') as f:\n",
        "    f.write(\"=\"*80 + \"\\n\")\n",
        "    f.write(\"EXPERIMENT 2: EXTENDED MULTI-DIRECTIONAL (7 DIRECTIONS)\\n\")\n",
        "    f.write(\"=\"*80 + \"\\n\\n\")\n",
        "\n",
        "    f.write(\"Configuration:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Directions (7 total):\\n\")\n",
        "    f.write(\"  1. backward_10:  [t-9 to t] - 10s history\\n\")\n",
        "    f.write(\"  2. centered_10:  [t-4 to t+5] - 10s centered\\n\")\n",
        "    f.write(\"  3. forward_10:   [t to t+9] - 10s future\\n\")\n",
        "    f.write(\"  4. backward_15:  [t-14 to t] - 15s history (more context)\\n\")\n",
        "    f.write(\"  5. forward_15:   [t to t+14] - 15s future (earlier transition)\\n\")\n",
        "    f.write(\"  6. asymm_past:   [t-11 to t+3] - heavy past bias\\n\")\n",
        "    f.write(\"  7. asymm_future: [t-3 to t+11] - heavy future bias\\n\")\n",
        "    f.write(\"\\nCombination method: Confidence-weighted\\n\")\n",
        "    f.write(\"Ensemble size: 5 models\\n\")\n",
        "    f.write(\"Temporal voting window: 5 seconds\\n\\n\")\n",
        "\n",
        "    # Overall summary\n",
        "    all_macro_f1 = []\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        fold_scores = [r['macro_f1'] for r in all_fold_results[fold_num]]\n",
        "        all_macro_f1.extend(fold_scores)\n",
        "\n",
        "    f.write(\"OVERALL RESULTS:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(f\"Mean Macro F1: {np.mean(all_macro_f1):.4f} ¬± {np.std(all_macro_f1):.4f}\\n\")\n",
        "    f.write(f\"Min: {np.min(all_macro_f1):.4f}, Max: {np.max(all_macro_f1):.4f}\\n\\n\")\n",
        "\n",
        "    # Comparison\n",
        "    f.write(\"PROGRESSION:\\n\")\n",
        "    f.write(\"-\"*80 + \"\\n\")\n",
        "    f.write(\"Baseline (single direction backward): 0.4106 ¬± 0.0266\\n\")\n",
        "    f.write(\"Experiment 1 (3 directions): 0.4273 ¬± 0.0312\\n\")\n",
        "    f.write(f\"Experiment 2 (7 directions): {np.mean(all_macro_f1):.4f} ¬± {np.std(all_macro_f1):.4f}\\n\\n\")\n",
        "\n",
        "    f.write(f\"Gain vs Baseline: {np.mean(all_macro_f1) - 0.4106:+.4f}\\n\")\n",
        "    f.write(f\"Gain vs Experiment 1: {np.mean(all_macro_f1) - 0.4273:+.4f}\\n\")\n",
        "    f.write(f\"Gap to target (0.45): {0.45 - np.mean(all_macro_f1):.4f}\\n\\n\")\n",
        "\n",
        "    # Per-fold results\n",
        "    for fold_num in [1, 2, 3, 4]:\n",
        "        f.write(f\"\\n{'='*80}\\n\")\n",
        "        f.write(f\"FOLD {fold_num} RESULTS\\n\")\n",
        "        f.write(f\"{'='*80}\\n\\n\")\n",
        "\n",
        "        fold_results = all_fold_results[fold_num]\n",
        "        macro_f1_scores = [r['macro_f1'] for r in fold_results]\n",
        "\n",
        "        f.write(f\"Macro F1 Scores:\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "        for i, result in enumerate(fold_results):\n",
        "            f.write(f\"  Seed {result['seed']:5d}: {result['macro_f1']:.4f}\\n\")\n",
        "\n",
        "        f.write(f\"\\nStatistics:\\n\")\n",
        "        f.write(f\"  Mean: {np.mean(macro_f1_scores):.4f} ¬± {np.std(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Min:  {np.min(macro_f1_scores):.4f}\\n\")\n",
        "        f.write(f\"  Max:  {np.max(macro_f1_scores):.4f}\\n\")\n",
        "\n",
        "        # Per-class F1 (averaged across seeds)\n",
        "        f.write(f\"\\nPer-Class F1 Scores (averaged across seeds):\\n\")\n",
        "        f.write(\"-\"*80 + \"\\n\")\n",
        "\n",
        "        # Collect all class names\n",
        "        all_classes = set()\n",
        "        for result in fold_results:\n",
        "            all_classes.update(result['per_class_f1'].keys())\n",
        "\n",
        "        # Average per-class F1 across seeds\n",
        "        for class_name in sorted(all_classes):\n",
        "            class_f1_scores = [r['per_class_f1'].get(class_name, 0) for r in fold_results]\n",
        "            mean_f1 = np.mean(class_f1_scores)\n",
        "            std_f1 = np.std(class_f1_scores)\n",
        "            f.write(f\"  {class_name:20s}: {mean_f1:.4f} ¬± {std_f1:.4f}\\n\")\n",
        "\n",
        "print(\"‚úÖ Results saved to experiment2_results.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atj9yQr1jpGQ",
        "outputId": "d39cbf5b-0dd8-4b83-f4f1-5ee75c079392"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Results saved to experiment2_results.txt\n"
          ]
        }
      ]
    }
  ]
}