{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6ElytLWfreY",
        "outputId": "2ee4d9b0-f24f-400e-e09b-f84ab9fa18fc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_path = '/content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/labelled_ble_data.csv'\n",
        "test_data_path = '/content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/BLE_Test_predict.csv'"
      ],
      "metadata": {
        "id": "leCDVLeMfycL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup and Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Masking, Input, Multiply, Permute, Reshape, Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, BatchNormalization\n",
        "from tensorflow.keras.layers import Bidirectional, GRU, Layer\n",
        "from tensorflow.keras import backend as K\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.metrics import f1_score\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"âœ“ All imports successful\")\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m_CEGOqCgBwZ",
        "outputId": "75fbebce-2a60-4eab-b614-43170810eaff"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ All imports successful\n",
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionLayer(Layer):\n",
        "    \"\"\"\n",
        "    Custom Attention Layer for Sequence Models\n",
        "\n",
        "    This layer learns which timesteps in the sequence are most important\n",
        "    for the classification task. It computes attention weights and returns\n",
        "    a weighted sum of the input sequence.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super(AttentionLayer, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        # input_shape: (batch_size, timesteps, features)\n",
        "        self.W = self.add_weight(\n",
        "            name='attention_weight',\n",
        "            shape=(input_shape[-1], 1),\n",
        "            initializer='glorot_uniform',\n",
        "            trainable=True\n",
        "        )\n",
        "        self.b = self.add_weight(\n",
        "            name='attention_bias',\n",
        "            shape=(1,),\n",
        "            initializer='zeros',\n",
        "            trainable=True\n",
        "        )\n",
        "        super(AttentionLayer, self).build(input_shape)\n",
        "\n",
        "    def call(self, inputs, mask=None):\n",
        "        # inputs shape: (batch_size, timesteps, features)\n",
        "\n",
        "        # Compute attention scores: (batch_size, timesteps, 1)\n",
        "        attention_scores = K.tanh(K.dot(inputs, self.W) + self.b)\n",
        "\n",
        "        # Apply mask if provided (for padded sequences)\n",
        "        if mask is not None:\n",
        "            # Expand mask to match attention_scores shape\n",
        "            mask = K.cast(mask, K.floatx())\n",
        "            mask = K.expand_dims(mask, axis=-1)\n",
        "            # Set masked positions to very negative value\n",
        "            attention_scores = attention_scores * mask + (1 - mask) * (-1e10)\n",
        "\n",
        "        # Compute attention weights: (batch_size, timesteps, 1)\n",
        "        attention_weights = K.softmax(attention_scores, axis=1)\n",
        "\n",
        "        # Compute weighted sum: (batch_size, features)\n",
        "        context_vector = K.sum(inputs * attention_weights, axis=1)\n",
        "\n",
        "        return context_vector\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        # Output shape: (batch_size, features)\n",
        "        return (input_shape[0], input_shape[-1])\n",
        "\n",
        "    def get_config(self):\n",
        "        return super(AttentionLayer, self).get_config()\n",
        "\n",
        "print(\"âœ… Custom Attention Layer defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ogvGOkCk90E",
        "outputId": "e9598b75-8e58-4635-b62c-e5104e456d5f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Custom Attention Layer defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seeds(seed=42):\n",
        "    np.random.seed(seed)\n",
        "    tf.random.set_seed(seed)\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
        "\n",
        "def create_room_groups(df):\n",
        "    df = df.sort_values('timestamp').reset_index(drop=True)\n",
        "    df['room_group'] = (df['room'] != df['room'].shift()).cumsum()\n",
        "    return df\n",
        "\n",
        "def create_beacon_count_vectors(df):\n",
        "    \"\"\"Aggregates readings into 1s vectors. Handles data with or without 'room_group'.\"\"\"\n",
        "    vectors = []\n",
        "    has_groups = 'room_group' in df.columns  # Check if we are in 'training' mode\n",
        "\n",
        "    for _, group in df.groupby('timestamp'):\n",
        "        beacon_counts = group['mac address'].value_counts()\n",
        "        total_readings = len(group)\n",
        "\n",
        "        vector = [0.0] * 23\n",
        "        for beacon_id, count in beacon_counts.items():\n",
        "            if 1 <= beacon_id <= 23:\n",
        "                vector[int(beacon_id) - 1] = count / total_readings\n",
        "\n",
        "        entry = {\n",
        "            'timestamp': group['timestamp'].iloc[0],\n",
        "            'room': group['room'].iloc[0],\n",
        "            'beacon_vector': vector\n",
        "        }\n",
        "\n",
        "        if has_groups:\n",
        "            entry['room_group'] = group['room_group'].iloc[0]\n",
        "\n",
        "        vectors.append(entry)\n",
        "\n",
        "    return pd.DataFrame(vectors)\n",
        "\n",
        "def create_sequences_from_groups(vector_df, min_length=3, max_length=50):\n",
        "    \"\"\"Used for Training: Creates clean sequences where the room is constant.\"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "\n",
        "    for (room, room_group), group in vector_df.groupby(['room', 'room_group']):\n",
        "        group = group.sort_values('timestamp').reset_index(drop=False)\n",
        "        seq_length = len(group)\n",
        "\n",
        "        if seq_length < min_length:\n",
        "            continue\n",
        "\n",
        "        if seq_length > max_length:\n",
        "            group = group.tail(max_length)\n",
        "\n",
        "        sequence = [row['beacon_vector'] for _, row in group.iterrows()]\n",
        "        sequences.append(sequence)\n",
        "        labels.append(room)\n",
        "\n",
        "    return sequences, labels\n",
        "\n",
        "print(\"âœ… Basic functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JOktqrbBk_59",
        "outputId": "7239362d-60e7-4251-c4d4-51126a254c9e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Basic functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_bidirectional_gru_model_with_deep_attention(input_shape, num_classes):\n",
        "    \"\"\"\n",
        "    Deep Bidirectional GRU Architecture with Attention Mechanism\n",
        "\n",
        "    Architecture:\n",
        "    1. Masking layer (handle variable-length sequences)\n",
        "    2. First Bi-GRU (128 units) with return_sequences=True\n",
        "    3. Dropout (0.3)\n",
        "    4. Second Bi-GRU (64 units) with return_sequences=True\n",
        "    5. Dropout (0.2)\n",
        "    6. ATTENTION LAYER - learns which timesteps matter most\n",
        "    7. Dense layer (64 units)\n",
        "    8. Dropout (0.3)\n",
        "    9. Dense layer (32 units)\n",
        "    10. Dropout (0.2)\n",
        "    11. Output layer (softmax)\n",
        "    \"\"\"\n",
        "    inputs = Input(shape=input_shape, name='input_layer')\n",
        "\n",
        "    # Masking for padded sequences\n",
        "    masked = Masking(mask_value=0.0, name='masking')(inputs)\n",
        "\n",
        "    # First Bi-GRU layer - extracts sequential features\n",
        "    gru1 = Bidirectional(\n",
        "        GRU(128, return_sequences=True, name='gru_layer_1'),\n",
        "        name='bidirectional_gru_1'\n",
        "    )(masked)\n",
        "    gru1 = Dropout(0.3, name='dropout_1')(gru1)\n",
        "\n",
        "    # Second Bi-GRU layer - deeper feature extraction\n",
        "    gru2 = Bidirectional(\n",
        "        GRU(64, return_sequences=True, name='gru_layer_2'),\n",
        "        name='bidirectional_gru_2'\n",
        "    )(gru1)\n",
        "    gru2 = Dropout(0.2, name='dropout_2')(gru2)\n",
        "\n",
        "    # ATTENTION MECHANISM\n",
        "    # Learns which timesteps are most important after deep feature extraction\n",
        "    attention_output = AttentionLayer(name='attention_layer')(gru2)\n",
        "\n",
        "    # Dense layers for classification\n",
        "    dense1 = Dense(64, activation='relu', name='dense_1')(attention_output)\n",
        "    dense1 = Dropout(0.3, name='dropout_3')(dense1)\n",
        "\n",
        "    dense2 = Dense(32, activation='relu', name='dense_2')(dense1)\n",
        "    dense1 = Dropout(0.2, name='dropout_4')(dense2)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = Dense(num_classes, activation='softmax', name='output_layer')(dense1)\n",
        "\n",
        "    # Create model\n",
        "    model = Model(inputs=inputs, outputs=outputs, name='Deep_BiGRU_with_Attention')\n",
        "\n",
        "    model.compile(\n",
        "        optimizer='adam',\n",
        "        loss='sparse_categorical_crossentropy',\n",
        "        metrics=['accuracy']\n",
        "    )\n",
        "\n",
        "    return model\n",
        "\n",
        "print(\"âœ… Deep Attention-based Bi-GRU model architecture defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zB7_v84ulCTC",
        "outputId": "c562a0b6-9b84-4c47-b925-2fe5aadf2438"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Deep Attention-based Bi-GRU model architecture defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def create_extended_multidirectional_windows(vector_df):\n",
        "    \"\"\"\n",
        "    Create 7 types of sliding windows for extended multi-directional prediction\n",
        "\n",
        "    Directions:\n",
        "    1. backward_10:  [i-9 to i]     - 10s history, predict at i\n",
        "    2. centered_10:  [i-4 to i+5]   - 10s centered, predict at i\n",
        "    3. forward_10:   [i to i+9]     - 10s future, predict at i\n",
        "    4. backward_15:  [i-14 to i]    - 15s history (more context)\n",
        "    5. forward_15:   [i to i+14]    - 15s future (earlier transition detection)\n",
        "    6. asymm_past:   [i-11 to i+3]  - 12s past + 4s future (transition from old room)\n",
        "    7. asymm_future: [i-3 to i+11]  - 4s past + 12s future (entering new room)\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with direction names as keys\n",
        "        Each contains: (sequences, labels, valid_indices)\n",
        "    \"\"\"\n",
        "    # Ensure chronological order and group by day\n",
        "    vector_df['dt'] = pd.to_datetime(vector_df['timestamp'])\n",
        "    vector_df['date'] = vector_df['dt'].dt.date\n",
        "\n",
        "    results = {\n",
        "        'backward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'centered_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_10': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'backward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'forward_15': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_past': {'sequences': [], 'labels': [], 'indices': []},\n",
        "        'asymm_future': {'sequences': [], 'labels': [], 'indices': []},\n",
        "    }\n",
        "\n",
        "    for _, day_group in vector_df.groupby('date'):\n",
        "        day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "        vectors = list(day_group['beacon_vector'])\n",
        "        rooms = list(day_group['room'])\n",
        "        n = len(vectors)\n",
        "\n",
        "        for i in range(n):\n",
        "            # 1. BACKWARD_10: [i-9, ..., i] predict at i\n",
        "            if i >= 9:\n",
        "                window = vectors[i - 9 : i + 1]\n",
        "                results['backward_10']['sequences'].append(window)\n",
        "                results['backward_10']['labels'].append(rooms[i])\n",
        "                results['backward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 2. CENTERED_10: [i-4, ..., i, ..., i+5] predict at i\n",
        "            if i >= 4 and i + 5 < n:\n",
        "                window = vectors[i - 4 : i + 6]\n",
        "                results['centered_10']['sequences'].append(window)\n",
        "                results['centered_10']['labels'].append(rooms[i])\n",
        "                results['centered_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 3. FORWARD_10: [i, ..., i+9] predict at i\n",
        "            if i + 9 < n:\n",
        "                window = vectors[i : i + 10]\n",
        "                results['forward_10']['sequences'].append(window)\n",
        "                results['forward_10']['labels'].append(rooms[i])\n",
        "                results['forward_10']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 4. BACKWARD_15: [i-14, ..., i] predict at i (MORE HISTORY)\n",
        "            if i >= 14:\n",
        "                window = vectors[i - 14 : i + 1]\n",
        "                results['backward_15']['sequences'].append(window)\n",
        "                results['backward_15']['labels'].append(rooms[i])\n",
        "                results['backward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 5. FORWARD_15: [i, ..., i+14] predict at i (EARLIER TRANSITION DETECTION)\n",
        "            if i + 14 < n:\n",
        "                window = vectors[i : i + 15]\n",
        "                results['forward_15']['sequences'].append(window)\n",
        "                results['forward_15']['labels'].append(rooms[i])\n",
        "                results['forward_15']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 6. ASYMM_PAST: [i-11, ..., i, ..., i+3] predict at i (HEAVY PAST BIAS)\n",
        "            # Good for detecting we're leaving a room\n",
        "            if i >= 11 and i + 3 < n:\n",
        "                window = vectors[i - 11 : i + 4]\n",
        "                results['asymm_past']['sequences'].append(window)\n",
        "                results['asymm_past']['labels'].append(rooms[i])\n",
        "                results['asymm_past']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "            # 7. ASYMM_FUTURE: [i-3, ..., i, ..., i+11] predict at i (HEAVY FUTURE BIAS)\n",
        "            # Good for detecting we're entering a room\n",
        "            if i >= 3 and i + 11 < n:\n",
        "                window = vectors[i - 3 : i + 12]\n",
        "                results['asymm_future']['sequences'].append(window)\n",
        "                results['asymm_future']['labels'].append(rooms[i])\n",
        "                results['asymm_future']['indices'].append((day_group['date'].iloc[0], i))\n",
        "\n",
        "    return results\n",
        "\n",
        "print(\"âœ… Extended multi-directional window function defined (7 directions)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wchu0PBOlD9c",
        "outputId": "a1962c59-88ca-4fcf-a690-db32972516ec"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Extended multi-directional window function defined (7 directions)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_ensemble_models(train_df, n_models=5, base_seed=42, verbose=True):\n",
        "    \"\"\"\n",
        "    Train multiple models with ATTENTION mechanism for deployment\n",
        "\n",
        "    Returns:\n",
        "        models: List of trained Keras models (with attention)\n",
        "        label_encoder: Fitted label encoder\n",
        "    \"\"\"\n",
        "    if verbose:\n",
        "        print(f\"\\nğŸš€ Training ensemble of {n_models} models with Deep Attention...\")\n",
        "\n",
        "    # Prepare data (same for all models)\n",
        "    train_df_grouped = create_room_groups(train_df)\n",
        "    train_vector_df = create_beacon_count_vectors(train_df_grouped)\n",
        "    X_train_seq, y_train_labels = create_sequences_from_groups(train_vector_df, max_length=50)\n",
        "\n",
        "    # Encode labels\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train_labels)\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"  Number of classes: {len(label_encoder.classes_)}\")\n",
        "        print(f\"  Classes: {list(label_encoder.classes_)}\")\n",
        "        print(f\"  Number of training sequences: {len(X_train_seq)}\")\n",
        "\n",
        "    # Pad sequences\n",
        "    X_train_padded = pad_sequences(X_train_seq, maxlen=50, padding='post', dtype='float32', value=0.0)\n",
        "\n",
        "    # Compute class weights\n",
        "    class_weights_array = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
        "    class_weights = {i: weight for i, weight in enumerate(class_weights_array)}\n",
        "\n",
        "    # Train multiple models\n",
        "    models = []\n",
        "    for i in range(n_models):\n",
        "        model_seed = base_seed + i * 1000  # 42, 1042, 2042, 3042, 4042\n",
        "        set_seeds(model_seed)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"\\n  Training Model {i+1}/{n_models} (seed {model_seed})...\")\n",
        "\n",
        "        # BUILD DEEP ATTENTION MODEL\n",
        "        model = build_bidirectional_gru_model_with_deep_attention(\n",
        "            input_shape=(50, 23),\n",
        "            num_classes=len(label_encoder.classes_)\n",
        "        )\n",
        "\n",
        "        # Callbacks\n",
        "        early_stop = EarlyStopping(monitor='loss', patience=5, restore_best_weights=True, verbose=0)\n",
        "        reduce_lr = ReduceLROnPlateau(monitor='loss', factor=0.5, patience=3, verbose=0, min_lr=1e-6)\n",
        "\n",
        "        # Train\n",
        "        history = model.fit(\n",
        "            X_train_padded, y_train,\n",
        "            epochs=30,\n",
        "            batch_size=32,\n",
        "            class_weight=class_weights,\n",
        "            callbacks=[early_stop, reduce_lr],\n",
        "            verbose=1 if verbose else 0\n",
        "        )\n",
        "\n",
        "        models.append(model)\n",
        "\n",
        "        if verbose:\n",
        "            print(f\"  âœ“ Model {i+1} training completed\")\n",
        "\n",
        "    return models, label_encoder\n",
        "\n",
        "print(\"âœ… Ensemble training function defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsSF_P3blFsg",
        "outputId": "fbf81e06-e6aa-41f5-b0eb-24a7e6f31022"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Ensemble training function defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_single_direction(models, sequences, max_seq_length=50):\n",
        "    \"\"\"\n",
        "    Get ensemble predictions for a single direction\n",
        "\n",
        "    Returns:\n",
        "        ensemble_proba: (n_samples, n_classes) averaged probability matrix\n",
        "    \"\"\"\n",
        "    # Pad sequences\n",
        "    X_padded = pad_sequences(sequences, maxlen=max_seq_length, dtype='float32', padding='post', value=0.0)\n",
        "\n",
        "    # Get predictions from all models\n",
        "    all_predictions = []\n",
        "    for model in models:\n",
        "        proba = model.predict(X_padded, verbose=0)\n",
        "        all_predictions.append(proba)\n",
        "\n",
        "    # Average probabilities across ensemble\n",
        "    ensemble_proba = np.mean(all_predictions, axis=0)\n",
        "\n",
        "    return ensemble_proba\n",
        "\n",
        "def combine_directional_predictions(direction_results, method='confidence_weighted'):\n",
        "    \"\"\"\n",
        "    Combine predictions from multiple directions using confidence weighting\n",
        "    Now handles 7 directions\n",
        "\n",
        "    Args:\n",
        "        direction_results: Dict with keys for all 7 directions\n",
        "                          Each value is a dict with 'proba' and 'indices'\n",
        "        method: 'confidence_weighted', 'equal', or 'softmax'\n",
        "\n",
        "    Returns:\n",
        "        combined_proba: (n_positions, n_classes) final probability matrix\n",
        "        position_map: mapping from (date, position) to array index\n",
        "    \"\"\"\n",
        "    # Build a mapping of all unique positions\n",
        "    all_positions = set()\n",
        "    direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                      'backward_15', 'forward_15',\n",
        "                      'asymm_past', 'asymm_future']\n",
        "\n",
        "    for direction in direction_names:\n",
        "        all_positions.update(direction_results[direction]['indices'])\n",
        "\n",
        "    # Sort positions for consistent ordering\n",
        "    all_positions = sorted(all_positions)\n",
        "    position_map = {pos: idx for idx, pos in enumerate(all_positions)}\n",
        "\n",
        "    # Get number of classes from first available direction\n",
        "    n_classes = direction_results['backward_10']['proba'].shape[1]\n",
        "    n_positions = len(all_positions)\n",
        "\n",
        "    # Initialize combined predictions\n",
        "    combined_proba = np.zeros((n_positions, n_classes))\n",
        "    position_counts = np.zeros(n_positions)  # Track how many directions contributed\n",
        "\n",
        "    # For each direction, add its weighted contribution\n",
        "    for direction_name in direction_names:\n",
        "        direction_data = direction_results[direction_name]\n",
        "        proba = direction_data['proba']\n",
        "        indices = direction_data['indices']\n",
        "\n",
        "        # Get confidence (max probability) for each prediction\n",
        "        confidences = np.max(proba, axis=1)\n",
        "\n",
        "        # Add weighted contribution to combined predictions\n",
        "        for i, pos in enumerate(indices):\n",
        "            pos_idx = position_map[pos]\n",
        "\n",
        "            if method == 'confidence_weighted':\n",
        "                # Weight by confidence\n",
        "                weight = confidences[i]\n",
        "                combined_proba[pos_idx] += proba[i] * weight\n",
        "            elif method == 'equal':\n",
        "                # Equal weight\n",
        "                combined_proba[pos_idx] += proba[i]\n",
        "            elif method == 'softmax':\n",
        "                # Will apply softmax later\n",
        "                combined_proba[pos_idx] += proba[i] * confidences[i]\n",
        "\n",
        "            position_counts[pos_idx] += 1 if method == 'equal' else confidences[i]\n",
        "\n",
        "    # Normalize by total weight\n",
        "    for i in range(n_positions):\n",
        "        if position_counts[i] > 0:\n",
        "            combined_proba[i] /= position_counts[i]\n",
        "\n",
        "    return combined_proba, position_map\n",
        "\n",
        "def apply_confidence_weighted_voting(predictions_proba, vote_window=5):\n",
        "    \"\"\"\n",
        "    Confidence-weighted temporal voting\n",
        "\n",
        "    Instead of simple majority voting, weight each prediction by its confidence (max probability).\n",
        "\n",
        "    Args:\n",
        "        predictions_proba: (n_samples, n_classes) probability matrix from ensemble\n",
        "        vote_window: window size for voting\n",
        "\n",
        "    Returns:\n",
        "        voted_predictions: (n_samples,) final class predictions\n",
        "    \"\"\"\n",
        "    n_samples, n_classes = predictions_proba.shape\n",
        "    voted_predictions = np.zeros(n_samples, dtype=int)\n",
        "\n",
        "    for i in range(n_samples):\n",
        "        # Get window boundaries\n",
        "        half_window = vote_window // 2\n",
        "        start = max(0, i - half_window)\n",
        "        end = min(n_samples, i + half_window + 1)\n",
        "\n",
        "        # Get probabilities within window\n",
        "        window_proba = predictions_proba[start:end]  # (window_size, n_classes)\n",
        "\n",
        "        # Get confidence (max probability) for each prediction in window\n",
        "        window_confidences = np.max(window_proba, axis=1)  # (window_size,)\n",
        "\n",
        "        # Weight each prediction by its confidence\n",
        "        weighted_votes = np.zeros(n_classes)\n",
        "        for j in range(len(window_proba)):\n",
        "            # Each timestep contributes its probability * its confidence\n",
        "            weighted_votes += window_proba[j] * window_confidences[j]\n",
        "\n",
        "        # Final prediction: class with highest weighted vote\n",
        "        voted_predictions[i] = np.argmax(weighted_votes)\n",
        "\n",
        "    return voted_predictions\n",
        "\n",
        "print(\"âœ… Prediction and combination functions defined\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yowyxl9NlJnK",
        "outputId": "3fe14ead-1776-4781-ee49-4ea7a9b654eb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Prediction and combination functions defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*80)\n",
        "print(\"LOADING FULL TRAINING DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load training data\n",
        "train_df = pd.read_csv(train_data_path)\n",
        "\n",
        "# Handle timezone in timestamp\n",
        "train_df['timestamp'] = pd.to_datetime(train_df['timestamp'], utc=True)\n",
        "# Convert to timezone-naive for consistency\n",
        "train_df['timestamp'] = train_df['timestamp'].dt.tz_localize(None)\n",
        "\n",
        "print(f\"\\nâœ“ Training data loaded\")\n",
        "print(f\"  Shape: {train_df.shape}\")\n",
        "print(f\"  Columns: {list(train_df.columns)}\")\n",
        "print(f\"  Number of unique rooms: {train_df['room'].nunique()}\")\n",
        "print(f\"  Unique rooms: {sorted(train_df['room'].unique())}\")\n",
        "print(f\"  MAC addresses range: {train_df['mac address'].min()} to {train_df['mac address'].max()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(train_df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0E8aF7IqlMii",
        "outputId": "fb059f6c-06d5-4e70-aec8-8b466d284097"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "LOADING FULL TRAINING DATA\n",
            "================================================================================\n",
            "\n",
            "âœ“ Training data loaded\n",
            "  Shape: (1099957, 4)\n",
            "  Columns: ['timestamp', 'mac address', 'RSSI', 'room']\n",
            "  Number of unique rooms: 22\n",
            "  Unique rooms: ['501', '502', '503', '505', '506', '508', '510', '511', '512', '513', '515', '516', '517', '518', '520', '522', '523', 'cafeteria', 'cleaning', 'hallway', 'kitchen', 'nurse station']\n",
            "  MAC addresses range: 1 to 23\n",
            "\n",
            "First few rows:\n",
            "            timestamp  mac address  RSSI     room\n",
            "0 2023-04-10 05:21:46            6   -93  kitchen\n",
            "1 2023-04-10 05:21:46            6   -93  kitchen\n",
            "2 2023-04-10 05:21:46            6   -93  kitchen\n",
            "3 2023-04-10 05:21:46            6   -93  kitchen\n",
            "4 2023-04-10 05:21:46            6   -93  kitchen\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"TRAINING ENSEMBLE MODELS ON FULL DATASET\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Clear any previous sessions\n",
        "tf.keras.backend.clear_session()\n",
        "\n",
        "# Train ensemble with seeds: 42, 1042, 2042, 3042, 4042\n",
        "models, label_encoder = train_ensemble_models(\n",
        "    train_df,\n",
        "    n_models=5,\n",
        "    base_seed=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ALL MODELS TRAINED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YAKmwZM8lUSs",
        "outputId": "62ea69d5-908f-4811-f036-cbe55d0afda3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "TRAINING ENSEMBLE MODELS ON FULL DATASET\n",
            "================================================================================\n",
            "\n",
            "ğŸš€ Training ensemble of 5 models with Deep Attention...\n",
            "  Number of classes: 22\n",
            "  Classes: [np.str_('501'), np.str_('502'), np.str_('503'), np.str_('505'), np.str_('506'), np.str_('508'), np.str_('510'), np.str_('511'), np.str_('512'), np.str_('513'), np.str_('515'), np.str_('516'), np.str_('517'), np.str_('518'), np.str_('520'), np.str_('522'), np.str_('523'), np.str_('cafeteria'), np.str_('cleaning'), np.str_('hallway'), np.str_('kitchen'), np.str_('nurse station')]\n",
            "  Number of training sequences: 309\n",
            "\n",
            "  Training Model 1/5 (seed 42)...\n",
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 26ms/step - accuracy: 0.0385 - loss: 3.2153 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1283 - loss: 3.1567 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.1757 - loss: 3.0604 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.1728 - loss: 2.9269 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2752 - loss: 2.8536 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3028 - loss: 2.6478 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4052 - loss: 2.4543 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4015 - loss: 2.3607 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3862 - loss: 2.1746 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3057 - loss: 2.4016 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4213 - loss: 1.9764 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4363 - loss: 2.1525 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4369 - loss: 2.1639 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4569 - loss: 1.8878 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4918 - loss: 1.6659 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5232 - loss: 1.5404 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4467 - loss: 1.6568 - learning_rate: 0.0010\n",
            "Epoch 18/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4507 - loss: 1.6968 - learning_rate: 0.0010\n",
            "Epoch 19/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4786 - loss: 1.5605 - learning_rate: 0.0010\n",
            "Epoch 20/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4875 - loss: 1.4673 - learning_rate: 0.0010\n",
            "Epoch 21/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4526 - loss: 1.6278 - learning_rate: 0.0010\n",
            "Epoch 22/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.4338 - loss: 1.6045 - learning_rate: 0.0010\n",
            "Epoch 23/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5120 - loss: 1.4778 - learning_rate: 0.0010\n",
            "Epoch 24/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5016 - loss: 1.3716 - learning_rate: 5.0000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step - accuracy: 0.5074 - loss: 1.3751 - learning_rate: 5.0000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5487 - loss: 1.1686 - learning_rate: 5.0000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4830 - loss: 1.2817 - learning_rate: 5.0000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4845 - loss: 1.2672 - learning_rate: 5.0000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5418 - loss: 1.3402 - learning_rate: 5.0000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5391 - loss: 1.1036 - learning_rate: 2.5000e-04\n",
            "  âœ“ Model 1 training completed\n",
            "\n",
            "  Training Model 2/5 (seed 1042)...\n",
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 37ms/step - accuracy: 0.1225 - loss: 3.2751 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.1925 - loss: 3.1825 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1949 - loss: 3.0824 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2369 - loss: 2.9178 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2612 - loss: 2.6995 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2588 - loss: 2.6519 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3333 - loss: 2.5171 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3191 - loss: 2.3552 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3624 - loss: 2.3080 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3533 - loss: 2.1786 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.3756 - loss: 2.0485 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3879 - loss: 1.9200 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4156 - loss: 1.8376 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.3790 - loss: 1.9687 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3994 - loss: 1.8105 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4497 - loss: 1.5942 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4510 - loss: 1.6661 - learning_rate: 0.0010\n",
            "Epoch 18/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4321 - loss: 1.5887 - learning_rate: 0.0010\n",
            "Epoch 19/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4889 - loss: 1.5072 - learning_rate: 0.0010\n",
            "Epoch 20/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5101 - loss: 1.3089 - learning_rate: 0.0010\n",
            "Epoch 21/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4773 - loss: 1.3607 - learning_rate: 0.0010\n",
            "Epoch 22/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4527 - loss: 1.5235 - learning_rate: 0.0010\n",
            "Epoch 23/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4513 - loss: 1.3602 - learning_rate: 0.0010\n",
            "Epoch 24/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5084 - loss: 1.3598 - learning_rate: 5.0000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4422 - loss: 1.3560 - learning_rate: 5.0000e-04\n",
            "  âœ“ Model 2 training completed\n",
            "\n",
            "  Training Model 3/5 (seed 2042)...\n",
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.0999 - loss: 3.2772 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.2016 - loss: 3.2125 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.2895 - loss: 3.1513 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.2845 - loss: 3.0004 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.3209 - loss: 2.9347 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.3292 - loss: 2.7579 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3265 - loss: 2.5898 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - accuracy: 0.3467 - loss: 2.3236 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step - accuracy: 0.3581 - loss: 2.5937 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step - accuracy: 0.3347 - loss: 2.2084 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - accuracy: 0.3957 - loss: 2.0961 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3987 - loss: 2.0351 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4230 - loss: 1.9473 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4853 - loss: 1.6433 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4427 - loss: 1.8490 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4750 - loss: 1.6720 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4876 - loss: 1.8009 - learning_rate: 0.0010\n",
            "Epoch 18/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4755 - loss: 1.5318 - learning_rate: 0.0010\n",
            "Epoch 19/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4655 - loss: 1.7481 - learning_rate: 0.0010\n",
            "Epoch 20/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4529 - loss: 1.6675 - learning_rate: 0.0010\n",
            "Epoch 21/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4359 - loss: 1.7474 - learning_rate: 0.0010\n",
            "Epoch 22/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5185 - loss: 1.4730 - learning_rate: 5.0000e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5032 - loss: 1.3069 - learning_rate: 5.0000e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4828 - loss: 1.3992 - learning_rate: 5.0000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4935 - loss: 1.2678 - learning_rate: 5.0000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5310 - loss: 1.2876 - learning_rate: 5.0000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5136 - loss: 1.2824 - learning_rate: 2.5000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5136 - loss: 1.3568 - learning_rate: 2.5000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5021 - loss: 1.1796 - learning_rate: 2.5000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4788 - loss: 1.2244 - learning_rate: 2.5000e-04\n",
            "  âœ“ Model 3 training completed\n",
            "\n",
            "  Training Model 4/5 (seed 3042)...\n",
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 26ms/step - accuracy: 0.0836 - loss: 2.9263 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2235 - loss: 2.8707 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2361 - loss: 2.7856 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.2464 - loss: 2.6296 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.2584 - loss: 2.4559 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.3127 - loss: 2.2934 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.3510 - loss: 2.1590 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.3154 - loss: 2.1024 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.3439 - loss: 2.1768 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 92ms/step - accuracy: 0.3524 - loss: 1.9030 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3540 - loss: 2.0479 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 81ms/step - accuracy: 0.3939 - loss: 1.8773 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 61ms/step - accuracy: 0.4292 - loss: 1.7239 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.4813 - loss: 1.6276 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4224 - loss: 1.6821 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4549 - loss: 1.7572 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4827 - loss: 1.3851 - learning_rate: 0.0010\n",
            "Epoch 18/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4387 - loss: 1.4841 - learning_rate: 0.0010\n",
            "Epoch 19/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.4916 - loss: 1.5194 - learning_rate: 0.0010\n",
            "Epoch 20/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4876 - loss: 1.2746 - learning_rate: 0.0010\n",
            "Epoch 21/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4541 - loss: 1.3833 - learning_rate: 0.0010\n",
            "Epoch 22/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.4849 - loss: 1.5931 - learning_rate: 0.0010\n",
            "Epoch 23/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4625 - loss: 1.3767 - learning_rate: 0.0010\n",
            "Epoch 24/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5076 - loss: 1.4554 - learning_rate: 5.0000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5080 - loss: 1.2327 - learning_rate: 5.0000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5370 - loss: 1.2994 - learning_rate: 5.0000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5371 - loss: 1.1681 - learning_rate: 5.0000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5015 - loss: 1.1547 - learning_rate: 5.0000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5116 - loss: 1.3174 - learning_rate: 5.0000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5097 - loss: 1.2756 - learning_rate: 5.0000e-04\n",
            "  âœ“ Model 4 training completed\n",
            "\n",
            "  Training Model 5/5 (seed 4042)...\n",
            "Epoch 1/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 27ms/step - accuracy: 0.0599 - loss: 3.6040 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.1660 - loss: 3.5650 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.2204 - loss: 3.4607 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.2255 - loss: 3.3294 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.1787 - loss: 3.1038 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.2682 - loss: 3.0932 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.2853 - loss: 2.9523 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - accuracy: 0.3369 - loss: 2.6691 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3529 - loss: 2.4431 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - accuracy: 0.3282 - loss: 2.2656 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step - accuracy: 0.4160 - loss: 2.4048 - learning_rate: 0.0010\n",
            "Epoch 12/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4574 - loss: 2.1643 - learning_rate: 0.0010\n",
            "Epoch 13/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step - accuracy: 0.4592 - loss: 1.9971 - learning_rate: 0.0010\n",
            "Epoch 14/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - accuracy: 0.4962 - loss: 2.1022 - learning_rate: 0.0010\n",
            "Epoch 15/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.4496 - loss: 1.7204 - learning_rate: 0.0010\n",
            "Epoch 16/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.5035 - loss: 1.9905 - learning_rate: 0.0010\n",
            "Epoch 17/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4785 - loss: 1.8815 - learning_rate: 0.0010\n",
            "Epoch 18/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4913 - loss: 1.8146 - learning_rate: 0.0010\n",
            "Epoch 19/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.4948 - loss: 1.7578 - learning_rate: 5.0000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 0.5229 - loss: 1.7037 - learning_rate: 5.0000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.4996 - loss: 1.7849 - learning_rate: 5.0000e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5589 - loss: 1.3193 - learning_rate: 5.0000e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5300 - loss: 1.3866 - learning_rate: 5.0000e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5250 - loss: 1.2061 - learning_rate: 5.0000e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5328 - loss: 1.3692 - learning_rate: 5.0000e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - accuracy: 0.5247 - loss: 1.5811 - learning_rate: 5.0000e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - accuracy: 0.5578 - loss: 1.1840 - learning_rate: 5.0000e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5869 - loss: 1.1049 - learning_rate: 2.5000e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - accuracy: 0.5495 - loss: 1.1750 - learning_rate: 2.5000e-04\n",
            "Epoch 30/30\n",
            "\u001b[1m10/10\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - accuracy: 0.5697 - loss: 1.2143 - learning_rate: 2.5000e-04\n",
            "  âœ“ Model 5 training completed\n",
            "\n",
            "================================================================================\n",
            "âœ… ALL MODELS TRAINED SUCCESSFULLY!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING MODELS AND ENCODER\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create directory for models\n",
        "model_dir = '/content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save each model\n",
        "for i, model in enumerate(models):\n",
        "    seed = 42 + i * 1000\n",
        "    model_path = os.path.join(model_dir, f'model_seed_{seed}.keras')\n",
        "    model.save(model_path)\n",
        "    print(f\"  âœ“ Model {i+1} (seed {seed}) saved to: {model_path}\")\n",
        "\n",
        "# Save label encoder\n",
        "encoder_path = os.path.join(model_dir, 'label_encoder.pkl')\n",
        "with open(encoder_path, 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)\n",
        "print(f\"\\n  âœ“ Label encoder saved to: {encoder_path}\")\n",
        "\n",
        "# Save class names for reference\n",
        "classes_path = os.path.join(model_dir, 'class_names.txt')\n",
        "with open(classes_path, 'w') as f:\n",
        "    f.write(\"Room Classes:\\n\")\n",
        "    f.write(\"=============\\n\")\n",
        "    for i, class_name in enumerate(label_encoder.classes_):\n",
        "        f.write(f\"{i}: {class_name}\\n\")\n",
        "print(f\"  âœ“ Class names saved to: {classes_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… ALL MODELS AND ENCODER SAVED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6UX74tklWW6",
        "outputId": "ea8b1d65-af29-4780-e194-cc5aeb5c3f35"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SAVING MODELS AND ENCODER\n",
            "================================================================================\n",
            "  âœ“ Model 1 (seed 42) saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/model_seed_42.keras\n",
            "  âœ“ Model 2 (seed 1042) saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/model_seed_1042.keras\n",
            "  âœ“ Model 3 (seed 2042) saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/model_seed_2042.keras\n",
            "  âœ“ Model 4 (seed 3042) saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/model_seed_3042.keras\n",
            "  âœ“ Model 5 (seed 4042) saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/model_seed_4042.keras\n",
            "\n",
            "  âœ“ Label encoder saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/label_encoder.pkl\n",
            "  âœ“ Class names saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/trained_models/class_names.txt\n",
            "\n",
            "================================================================================\n",
            "âœ… ALL MODELS AND ENCODER SAVED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"LOADING REAL TEST DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Load test data\n",
        "test_df_original = pd.read_csv(test_data_path)\n",
        "\n",
        "# Handle timestamp format (no timezone in test data)\n",
        "test_df_original['timestamp'] = pd.to_datetime(test_df_original['timestamp'])\n",
        "\n",
        "print(f\"\\nâœ“ Test data loaded\")\n",
        "print(f\"  Shape: {test_df_original.shape}\")\n",
        "print(f\"  Columns: {list(test_df_original.columns)}\")\n",
        "print(f\"  Unique timestamps: {test_df_original['timestamp'].nunique()}\")\n",
        "print(f\"  MAC addresses range: {test_df_original['mac address'].min()} to {test_df_original['mac address'].max()}\")\n",
        "print(f\"\\nFirst few rows:\")\n",
        "print(test_df_original.head())\n",
        "\n",
        "# Keep a copy with original index for later\n",
        "test_df_original['original_index'] = test_df_original.index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T-d6OHclYUL",
        "outputId": "15bf7ab7-8bc8-4274-bfd7-a4aa054219e9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "LOADING REAL TEST DATA\n",
            "================================================================================\n",
            "\n",
            "âœ“ Test data loaded\n",
            "  Shape: (62222, 6)\n",
            "  Columns: ['Unnamed: 0', 'user_id', 'timestamp', 'mac address', 'RSSI', 'power']\n",
            "  Unique timestamps: 5721\n",
            "  MAC addresses range: 1 to 23\n",
            "\n",
            "First few rows:\n",
            "   Unnamed: 0  user_id           timestamp  mac address  RSSI       power\n",
            "0         766       90 2023-04-14 10:01:40            7   -97 -2147483648\n",
            "1         764       90 2023-04-14 10:01:40            7   -97 -2147483648\n",
            "2         765       90 2023-04-14 10:01:40            7   -97 -2147483648\n",
            "3         762       90 2023-04-14 10:01:40            7   -97 -2147483648\n",
            "4         761       90 2023-04-14 10:01:40            7   -97 -2147483648\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"MAKING PREDICTIONS ON TEST DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# For prediction, we need a 'room' column (can be dummy since we don't have labels)\n",
        "# We'll use a placeholder value\n",
        "test_df = test_df_original.copy()\n",
        "test_df['room'] = 'unknown'  # Placeholder\n",
        "\n",
        "# Create beacon vectors from test data\n",
        "print(\"\\n  Creating beacon count vectors...\")\n",
        "test_vectors = create_beacon_count_vectors(test_df)\n",
        "print(f\"  âœ“ Created {len(test_vectors)} 1-second windows\")\n",
        "\n",
        "# Create multi-directional windows\n",
        "print(\"\\n  Creating 7-directional windows...\")\n",
        "direction_windows = create_extended_multidirectional_windows(test_vectors)\n",
        "\n",
        "for direction_name in ['backward_10', 'centered_10', 'forward_10', 'backward_15', 'forward_15', 'asymm_past', 'asymm_future']:\n",
        "    n_windows = len(direction_windows[direction_name]['sequences'])\n",
        "    print(f\"    {direction_name:15s}: {n_windows:6d} windows\")\n",
        "\n",
        "# Get predictions for each direction\n",
        "print(\"\\n  Getting directional predictions...\")\n",
        "direction_results = {}\n",
        "direction_names = ['backward_10', 'centered_10', 'forward_10',\n",
        "                  'backward_15', 'forward_15',\n",
        "                  'asymm_past', 'asymm_future']\n",
        "\n",
        "for direction_name in direction_names:\n",
        "    print(f\"    Predicting {direction_name}...\", end=\" \")\n",
        "\n",
        "    sequences = direction_windows[direction_name]['sequences']\n",
        "    proba = predict_single_direction(models, sequences, max_seq_length=50)\n",
        "\n",
        "    direction_results[direction_name] = {\n",
        "        'proba': proba,\n",
        "        'indices': direction_windows[direction_name]['indices'],\n",
        "        'labels': direction_windows[direction_name]['labels']\n",
        "    }\n",
        "\n",
        "    avg_conf = np.mean(np.max(proba, axis=1))\n",
        "    print(f\"avg confidence: {avg_conf:.3f}\")\n",
        "\n",
        "# Combine directional predictions\n",
        "print(\"\\n  Combining 7 directions using confidence weighting...\")\n",
        "combined_proba, position_map = combine_directional_predictions(\n",
        "    direction_results,\n",
        "    method='confidence_weighted'\n",
        ")\n",
        "print(f\"  âœ“ Combined predictions for {len(position_map)} unique positions\")\n",
        "\n",
        "# Apply temporal voting\n",
        "print(\"\\n  Applying temporal voting (window=5)...\")\n",
        "y_pred_voted_encoded = apply_confidence_weighted_voting(combined_proba, vote_window=5)\n",
        "print(f\"  âœ“ Voting completed\")\n",
        "\n",
        "# Decode predictions\n",
        "print(\"\\n  Decoding predictions to room names...\")\n",
        "y_pred = label_encoder.inverse_transform(y_pred_voted_encoded)\n",
        "print(f\"  âœ“ Predictions decoded\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PREDICTIONS COMPLETED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "azbriIoyliRG",
        "outputId": "876671c9-4cff-4944-f020-3595493b82d8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "MAKING PREDICTIONS ON TEST DATA\n",
            "================================================================================\n",
            "\n",
            "  Creating beacon count vectors...\n",
            "  âœ“ Created 5721 1-second windows\n",
            "\n",
            "  Creating 7-directional windows...\n",
            "    backward_10    :   5712 windows\n",
            "    centered_10    :   5712 windows\n",
            "    forward_10     :   5712 windows\n",
            "    backward_15    :   5707 windows\n",
            "    forward_15     :   5707 windows\n",
            "    asymm_past     :   5707 windows\n",
            "    asymm_future   :   5707 windows\n",
            "\n",
            "  Getting directional predictions...\n",
            "    Predicting backward_10... avg confidence: 0.531\n",
            "    Predicting centered_10... avg confidence: 0.531\n",
            "    Predicting forward_10... avg confidence: 0.531\n",
            "    Predicting backward_15... avg confidence: 0.572\n",
            "    Predicting forward_15... avg confidence: 0.572\n",
            "    Predicting asymm_past... avg confidence: 0.572\n",
            "    Predicting asymm_future... avg confidence: 0.572\n",
            "\n",
            "  Combining 7 directions using confidence weighting...\n",
            "  âœ“ Combined predictions for 5721 unique positions\n",
            "\n",
            "  Applying temporal voting (window=5)...\n",
            "  âœ“ Voting completed\n",
            "\n",
            "  Decoding predictions to room names...\n",
            "  âœ“ Predictions decoded\n",
            "\n",
            "================================================================================\n",
            "âœ… PREDICTIONS COMPLETED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"PROPAGATING PREDICTIONS TO ORIGINAL DATA\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Create a mapping from (date, position) to predicted room\n",
        "prediction_map = {}\n",
        "for pos, idx in position_map.items():\n",
        "    prediction_map[pos] = y_pred[idx]\n",
        "\n",
        "print(f\"\\n  Created prediction map for {len(prediction_map)} positions\")\n",
        "\n",
        "# Now we need to map back to original timestamps\n",
        "# First, recreate the position mapping for test_vectors\n",
        "test_vectors_with_date = test_vectors.copy()\n",
        "test_vectors_with_date['dt'] = pd.to_datetime(test_vectors_with_date['timestamp'])\n",
        "test_vectors_with_date['date'] = test_vectors_with_date['dt'].dt.date\n",
        "\n",
        "# Create position index for each timestamp within each day\n",
        "predictions_by_timestamp = {}\n",
        "\n",
        "for date, day_group in test_vectors_with_date.groupby('date'):\n",
        "    day_group = day_group.sort_values('timestamp').reset_index(drop=True)\n",
        "    for i, row in day_group.iterrows():\n",
        "        pos_key = (date, i)\n",
        "        if pos_key in prediction_map:\n",
        "            timestamp = row['timestamp']\n",
        "            predicted_room = prediction_map[pos_key]\n",
        "            predictions_by_timestamp[timestamp] = predicted_room\n",
        "\n",
        "print(f\"  Mapped predictions to {len(predictions_by_timestamp)} unique timestamps\")\n",
        "\n",
        "# Assign predictions to original test data\n",
        "print(\"\\n  Assigning predictions to original rows...\")\n",
        "test_df_final = test_df_original.copy()\n",
        "test_df_final['Location'] = test_df_final['timestamp'].map(predictions_by_timestamp)\n",
        "\n",
        "# Check for any missing predictions\n",
        "missing_predictions = test_df_final['Location'].isna().sum()\n",
        "if missing_predictions > 0:\n",
        "    print(f\"  âš ï¸  Warning: {missing_predictions} rows have no predictions (edge cases)\")\n",
        "    print(f\"     These will be forward-filled with nearest prediction\")\n",
        "    # Forward fill missing values\n",
        "    test_df_final['Location'] = test_df_final['Location'].fillna(method='ffill')\n",
        "    # If still any NaN (at the beginning), backward fill\n",
        "    test_df_final['Location'] = test_df_final['Location'].fillna(method='bfill')\n",
        "\n",
        "print(f\"\\n  âœ“ All {len(test_df_final)} rows have predictions\")\n",
        "\n",
        "# Show prediction distribution\n",
        "print(\"\\n  Prediction distribution:\")\n",
        "print(test_df_final['Location'].value_counts().sort_index())\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"âœ… PROPAGATION COMPLETED!\")\n",
        "print(\"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BQmaFF_vlkE-",
        "outputId": "31ea203a-9d84-4be2-f364-934953ffb486"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "PROPAGATING PREDICTIONS TO ORIGINAL DATA\n",
            "================================================================================\n",
            "\n",
            "  Created prediction map for 5721 positions\n",
            "  Mapped predictions to 5721 unique timestamps\n",
            "\n",
            "  Assigning predictions to original rows...\n",
            "\n",
            "  âœ“ All 62222 rows have predictions\n",
            "\n",
            "  Prediction distribution:\n",
            "Location\n",
            "501                585\n",
            "502                276\n",
            "503                990\n",
            "506               1828\n",
            "508                546\n",
            "510                 43\n",
            "511                143\n",
            "513                340\n",
            "516                264\n",
            "517                144\n",
            "518                798\n",
            "520                746\n",
            "522                687\n",
            "523                935\n",
            "cafeteria        10330\n",
            "cleaning          5505\n",
            "hallway            174\n",
            "kitchen          18687\n",
            "nurse station    19201\n",
            "Name: count, dtype: int64\n",
            "\n",
            "================================================================================\n",
            "âœ… PROPAGATION COMPLETED!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"SAVING FINAL PREDICTIONS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Drop the helper columns and keep original columns + Location\n",
        "output_df = test_df_final.drop(columns=['original_index'], errors='ignore')\n",
        "\n",
        "# Save to the specified filename\n",
        "output_path = '/content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/predicted_BLE_Test_Data.csv'\n",
        "output_df.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"\\n  âœ“ Predictions saved to: {output_path}\")\n",
        "print(f\"  Shape: {output_df.shape}\")\n",
        "print(f\"  Columns: {list(output_df.columns)}\")\n",
        "\n",
        "print(\"\\n  Preview of saved data:\")\n",
        "print(output_df.head(10))\n",
        "\n",
        "print(\"\\n  Final statistics:\")\n",
        "print(f\"    Total rows: {len(output_df)}\")\n",
        "print(f\"    Unique timestamps: {output_df['timestamp'].nunique()}\")\n",
        "print(f\"    Unique locations predicted: {output_df['Location'].nunique()}\")\n",
        "print(f\"    Locations: {sorted(output_df['Location'].unique())}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ‰ DEPLOYMENT PIPELINE COMPLETED SUCCESSFULLY!\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nSummary:\")\n",
        "print(\"  âœ… Trained 5 ensemble models on full training data\")\n",
        "print(\"  âœ… Saved all models and label encoder\")\n",
        "print(\"  âœ… Made predictions on real test data\")\n",
        "print(\"  âœ… Propagated window-level predictions to original rows\")\n",
        "print(\"  âœ… Saved final predictions to predicted_BLE_Test_Data.csv\")\n",
        "print(\"\\n\" + \"=\"*80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k2x5eG6alyEf",
        "outputId": "8c300f33-45d3-4ee6-b7b5-3c11d7812475"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "================================================================================\n",
            "SAVING FINAL PREDICTIONS\n",
            "================================================================================\n",
            "\n",
            "  âœ“ Predictions saved to: /content/drive/MyDrive/ABC_Full_Train_Data_And_Real_Test_Data/predicted_BLE_Test_Data.csv\n",
            "  Shape: (62222, 7)\n",
            "  Columns: ['Unnamed: 0', 'user_id', 'timestamp', 'mac address', 'RSSI', 'power', 'Location']\n",
            "\n",
            "  Preview of saved data:\n",
            "   Unnamed: 0  user_id           timestamp  mac address  RSSI       power  \\\n",
            "0         766       90 2023-04-14 10:01:40            7   -97 -2147483648   \n",
            "1         764       90 2023-04-14 10:01:40            7   -97 -2147483648   \n",
            "2         765       90 2023-04-14 10:01:40            7   -97 -2147483648   \n",
            "3         762       90 2023-04-14 10:01:40            7   -97 -2147483648   \n",
            "4         761       90 2023-04-14 10:01:40            7   -97 -2147483648   \n",
            "5         763       90 2023-04-14 10:01:40            7   -97 -2147483648   \n",
            "6         777       90 2023-04-14 10:01:42            7   -91 -2147483648   \n",
            "7         783       90 2023-04-14 10:01:42            7   -91 -2147483648   \n",
            "8         782       90 2023-04-14 10:01:42           22   -98 -2147483648   \n",
            "9         781       90 2023-04-14 10:01:42           22   -98 -2147483648   \n",
            "\n",
            "        Location  \n",
            "0  nurse station  \n",
            "1  nurse station  \n",
            "2  nurse station  \n",
            "3  nurse station  \n",
            "4  nurse station  \n",
            "5  nurse station  \n",
            "6  nurse station  \n",
            "7  nurse station  \n",
            "8  nurse station  \n",
            "9  nurse station  \n",
            "\n",
            "  Final statistics:\n",
            "    Total rows: 62222\n",
            "    Unique timestamps: 5721\n",
            "    Unique locations predicted: 19\n",
            "    Locations: [np.str_('501'), np.str_('502'), np.str_('503'), np.str_('506'), np.str_('508'), np.str_('510'), np.str_('511'), np.str_('513'), np.str_('516'), np.str_('517'), np.str_('518'), np.str_('520'), np.str_('522'), np.str_('523'), np.str_('cafeteria'), np.str_('cleaning'), np.str_('hallway'), np.str_('kitchen'), np.str_('nurse station')]\n",
            "\n",
            "================================================================================\n",
            "ğŸ‰ DEPLOYMENT PIPELINE COMPLETED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            "Summary:\n",
            "  âœ… Trained 5 ensemble models on full training data\n",
            "  âœ… Saved all models and label encoder\n",
            "  âœ… Made predictions on real test data\n",
            "  âœ… Propagated window-level predictions to original rows\n",
            "  âœ… Saved final predictions to predicted_BLE_Test_Data.csv\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}